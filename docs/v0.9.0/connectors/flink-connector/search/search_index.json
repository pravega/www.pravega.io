{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Apache Flink Connectors for Pravega This documentation describes the connectors API and it's usage to read and write Pravega streams with Apache Flink stream processing framework. Build end-to-end stream processing pipelines that use Pravega as the stream storage and message bus, and Apache Flink for computation over the streams. See the Pravega Concepts page for more information. Table of Contents Getting Started Quick Start Features Streaming Batch Table API/SQL Metrics Configurations Serialization","title":"Overview"},{"location":"#apache-flink-connectors-for-pravega","text":"This documentation describes the connectors API and it's usage to read and write Pravega streams with Apache Flink stream processing framework. Build end-to-end stream processing pipelines that use Pravega as the stream storage and message bus, and Apache Flink for computation over the streams. See the Pravega Concepts page for more information.","title":"Apache Flink Connectors for Pravega"},{"location":"#table-of-contents","text":"Getting Started Quick Start Features Streaming Batch Table API/SQL Metrics Configurations Serialization","title":"Table of Contents"},{"location":"batch/","text":"Batch Connector The Flink Connector library for Pravega makes it possible to use a Pravega Stream as a data source and data sink in a batch program. See the below sections for details. Table of Contents FlinkPravegaInputFormat Parameters Input Stream(s) StreamCuts Parallelism FlinkPravegaOutputFormat Parameters Output Stream Parallelism Event Routing Serialization FlinkPravegaInputFormat A Pravega Stream may be used as a data source within a Flink batch program using an instance of io.pravega.connectors.flink.FlinkPravegaInputFormat . The input format reads events of a stream as a DataSet (the basic abstraction of the Flink Batch API). This input format opens the stream for batch reading, which processes stream segments in parallel and does not follow routing key order. Use the ExecutionEnvironment::createInput method to open a Pravega Stream as a DataSet . Example // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event deserializer DeserializationSchema < EventType > deserializer = ... // Define the input format based on a Pravega stream FlinkPravegaInputFormat < EventType > inputFormat = FlinkPravegaInputFormat .< EventType > builder () . forStream (...) . withPravegaConfig ( config ) . withDeserializationSchema ( deserializer ) . build (); ExecutionEnvironment env = ExecutionEnvironment . getExecutionEnvironment (); DataSource < EventType > dataSet = env . createInput ( inputFormat , TypeInformation . of ( EventType . class )). setParallelism ( 2 ); Parameters A builder API is provided to construct an instance of FlinkPravegaInputFormat . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. withDeserializationSchema The deserialization schema which describes how to turn byte messages into events. Input Stream(s) Each Pravega stream exists within a scope. A scope defines a namespace for streams such that names are unique. Across scopes, streams can have the same name. For example, if we have scopes A and B , then we can have a stream called myStream in each one of them. We cannot have a stream with the same name in the same scope. The builder API accepts both qualified and unqualified stream names. In qualified stream names, the scope is explicitly specified, e.g. my-scope/my-stream . In unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . See the configurations page for more information on default scope. A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") . Multiple streams can be passed as parameter option (using the builder API). The BatchClient implementation is capable of reading from numerous streams in parallel, even across scopes. StreamCuts A StreamCut represents a specific position in a Pravega Stream, which may be obtained from various API interactions with the Pravega client. The BatchClient accepts a StreamCut as the start and/or end position of a given stream. For further reading on StreamCuts, please refer to documentation on StreamCut and sample code . If stream cuts are not provided then the default start position requested is assumed to be the earliest available data in the stream and the default end position is assumed to be all available data in that stream as of when the job execution begins. Parallelism FlinkPravegaInputFormat supports parallelization. Use the setParallelism method of DataSet to configure the number of parallel instances to execute. The parallel instances consume the stream in a coordinated manner, each consuming one or more stream segments. FlinkPravegaOutputFormat A Pravega Stream may be used as a data sink within a Flink batch program using an instance of io.pravega.connectors.flink.FlinkPravegaOutputFormat . The FlinkPravegaOutputFormat can be supplied as a sink to the DataSet (the basic abstraction of the Flink Batch API). Example // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event serializer SerializationSchema < EventType > serializer = ... // Define the event router for selecting the Routing Key PravegaEventRouter < EventType > router = ... // Define the input format based on a Pravega Stream FlinkPravegaOutputFormat < EventType > outputFormat = FlinkPravegaOutputFormat .< EventType > builder () . forStream (...) . withPravegaConfig ( config ) . withSerializationSchema ( serializer ) . withEventRouter ( router ) . build (); ExecutionEnvironment env = ExecutionEnvironment . getExecutionEnvironment (); Collection < EventType > inputData = Arrays . asList (...); env . fromCollection ( inputData ) . output ( outputFormat ); env . execute ( \"...\" ); Parameter A builder API is provided to construct an instance of FlinkPravegaOutputFormat . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withSerializationSchema The serialization schema which describes how to turn events into byte messages. withEventRouter The router function which determines the Routing Key for a given event. Output Stream Each stream in Pravega is contained by a scope. A scope acts as a namespace for one or more streams. The builder API accepts both qualified and unqualified stream names. In qualified, the scope is explicitly specified, e.g. my-scope/my-stream . In Unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") . Parallelism FlinkPravegaWriter supports parallelization. Use the setParallelism method to configure the number of parallel instances to execute. Event Routing Every event written to a Pravega Stream has an associated Routing Key. The Routing Key is the basis for event ordering. See the Pravega Concepts for details. To establish the routing key for each event, provide an implementation of io.pravega.connectors.flink.PravegaEventRouter when constructing the writer. Serialization Please, see the serialization page for more information on how to use the serializer and deserializer .","title":"Batch"},{"location":"batch/#batch-connector","text":"The Flink Connector library for Pravega makes it possible to use a Pravega Stream as a data source and data sink in a batch program. See the below sections for details.","title":"Batch Connector"},{"location":"batch/#table-of-contents","text":"FlinkPravegaInputFormat Parameters Input Stream(s) StreamCuts Parallelism FlinkPravegaOutputFormat Parameters Output Stream Parallelism Event Routing Serialization","title":"Table of Contents"},{"location":"batch/#flinkpravegainputformat","text":"A Pravega Stream may be used as a data source within a Flink batch program using an instance of io.pravega.connectors.flink.FlinkPravegaInputFormat . The input format reads events of a stream as a DataSet (the basic abstraction of the Flink Batch API). This input format opens the stream for batch reading, which processes stream segments in parallel and does not follow routing key order. Use the ExecutionEnvironment::createInput method to open a Pravega Stream as a DataSet .","title":"FlinkPravegaInputFormat"},{"location":"batch/#example","text":"// Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event deserializer DeserializationSchema < EventType > deserializer = ... // Define the input format based on a Pravega stream FlinkPravegaInputFormat < EventType > inputFormat = FlinkPravegaInputFormat .< EventType > builder () . forStream (...) . withPravegaConfig ( config ) . withDeserializationSchema ( deserializer ) . build (); ExecutionEnvironment env = ExecutionEnvironment . getExecutionEnvironment (); DataSource < EventType > dataSet = env . createInput ( inputFormat , TypeInformation . of ( EventType . class )). setParallelism ( 2 );","title":"Example"},{"location":"batch/#parameters","text":"A builder API is provided to construct an instance of FlinkPravegaInputFormat . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. withDeserializationSchema The deserialization schema which describes how to turn byte messages into events.","title":"Parameters"},{"location":"batch/#input-streams","text":"Each Pravega stream exists within a scope. A scope defines a namespace for streams such that names are unique. Across scopes, streams can have the same name. For example, if we have scopes A and B , then we can have a stream called myStream in each one of them. We cannot have a stream with the same name in the same scope. The builder API accepts both qualified and unqualified stream names. In qualified stream names, the scope is explicitly specified, e.g. my-scope/my-stream . In unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . See the configurations page for more information on default scope. A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") . Multiple streams can be passed as parameter option (using the builder API). The BatchClient implementation is capable of reading from numerous streams in parallel, even across scopes.","title":"Input Stream(s)"},{"location":"batch/#streamcuts","text":"A StreamCut represents a specific position in a Pravega Stream, which may be obtained from various API interactions with the Pravega client. The BatchClient accepts a StreamCut as the start and/or end position of a given stream. For further reading on StreamCuts, please refer to documentation on StreamCut and sample code . If stream cuts are not provided then the default start position requested is assumed to be the earliest available data in the stream and the default end position is assumed to be all available data in that stream as of when the job execution begins.","title":"StreamCuts"},{"location":"batch/#parallelism","text":"FlinkPravegaInputFormat supports parallelization. Use the setParallelism method of DataSet to configure the number of parallel instances to execute. The parallel instances consume the stream in a coordinated manner, each consuming one or more stream segments.","title":"Parallelism"},{"location":"batch/#flinkpravegaoutputformat","text":"A Pravega Stream may be used as a data sink within a Flink batch program using an instance of io.pravega.connectors.flink.FlinkPravegaOutputFormat . The FlinkPravegaOutputFormat can be supplied as a sink to the DataSet (the basic abstraction of the Flink Batch API).","title":"FlinkPravegaOutputFormat"},{"location":"batch/#example_1","text":"// Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event serializer SerializationSchema < EventType > serializer = ... // Define the event router for selecting the Routing Key PravegaEventRouter < EventType > router = ... // Define the input format based on a Pravega Stream FlinkPravegaOutputFormat < EventType > outputFormat = FlinkPravegaOutputFormat .< EventType > builder () . forStream (...) . withPravegaConfig ( config ) . withSerializationSchema ( serializer ) . withEventRouter ( router ) . build (); ExecutionEnvironment env = ExecutionEnvironment . getExecutionEnvironment (); Collection < EventType > inputData = Arrays . asList (...); env . fromCollection ( inputData ) . output ( outputFormat ); env . execute ( \"...\" );","title":"Example"},{"location":"batch/#parameter","text":"A builder API is provided to construct an instance of FlinkPravegaOutputFormat . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withSerializationSchema The serialization schema which describes how to turn events into byte messages. withEventRouter The router function which determines the Routing Key for a given event.","title":"Parameter"},{"location":"batch/#output-stream","text":"Each stream in Pravega is contained by a scope. A scope acts as a namespace for one or more streams. The builder API accepts both qualified and unqualified stream names. In qualified, the scope is explicitly specified, e.g. my-scope/my-stream . In Unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") .","title":"Output Stream"},{"location":"batch/#parallelism_1","text":"FlinkPravegaWriter supports parallelization. Use the setParallelism method to configure the number of parallel instances to execute.","title":"Parallelism"},{"location":"batch/#event-routing","text":"Every event written to a Pravega Stream has an associated Routing Key. The Routing Key is the basis for event ordering. See the Pravega Concepts for details. To establish the routing key for each event, provide an implementation of io.pravega.connectors.flink.PravegaEventRouter when constructing the writer.","title":"Event Routing"},{"location":"batch/#serialization","text":"Please, see the serialization page for more information on how to use the serializer and deserializer .","title":"Serialization"},{"location":"configurations/","text":"Configurations The Flink connector library for Pravega supports the Flink Streaming API , Table API and Batch API , using a common configuration class. Table of Contents Common Configuration PravegaConfig Class Creating PravegaConfig Using PravegaConfig Understanding the Default Scope Common Configuration PravegaConfig Class A top-level config object, PravegaConfig , is provided to establish a Pravega context for the Flink connector. The config object automatically configures itself from environment variables , system properties and program arguments . PravegaConfig information sources is given below: Setting Environment Variable / System Property / Program Argument Default Value Controller URI PRAVEGA_CONTROLLER_URI pravega.controller.uri --controller tcp://localhost:9090 Default Scope PRAVEGA_SCOPE pravega.scope --scope - Credentials - - Hostname Validation - true Creating PravegaConfig The recommended way to create an instance of PravegaConfig is to pass an instance of ParameterTool to fromParams : ParameterTool params = ParameterTool . fromArgs ( args ); PravegaConfig config = PravegaConfig . fromParams ( params ); If your application doesn't use the ParameterTool class that is provided by Flink, create the PravegaConfig using fromDefaults : PravegaConfig config = PravegaConfig . fromDefaults (); The PravegaConfig class provides a builder-style API to override the default configuration settings: PravegaConfig config = PravegaConfig . fromDefaults () . withControllerURI ( \"tcp://...\" ) . withDefaultScope ( \"SCOPE-NAME\" ) . withCredentials ( credentials ) . withHostnameValidation ( false ); Using PravegaConfig All of the various source and sink classes provided with the connector library have a builder-style API which accepts a PravegaConfig for common configuration. Pass a PravegaConfig object to the respective builder via withPravegaConfig . For example, see below code: PravegaConfig config = ...; FlinkPravegaReader < MyClass > pravegaSource = FlinkPravegaReader .< MyClass > builder () . forStream (...) . withPravegaConfig ( config ) . build (); Understanding the Default Scope Pravega organizes streams into scopes for the purposes of manageability. The PravegaConfig establishes a default scope name that is used in two scenarios: For resolving unqualified stream names when constructing a source or sink. The sources and sinks accept stream names that may be qualified (e.g. my-scope/my-stream ) or unqualified (e.g. my-stream ). For establishing the scope name for the coordination stream underlying a Pravega Reader Group. It is important to note that, the FlinkPravegaReader and the FlinkPravegaTableSource use the default scope configured on PravegaConfig as their Reader Group scope, and provide withReaderGroupScope as an override. The scope name of input stream(s) doesn't influence the Reader Group scope.","title":"Configurations"},{"location":"configurations/#configurations","text":"The Flink connector library for Pravega supports the Flink Streaming API , Table API and Batch API , using a common configuration class.","title":"Configurations"},{"location":"configurations/#table-of-contents","text":"Common Configuration PravegaConfig Class Creating PravegaConfig Using PravegaConfig Understanding the Default Scope","title":"Table of Contents"},{"location":"configurations/#common-configuration","text":"","title":"Common Configuration"},{"location":"configurations/#pravegaconfig-class","text":"A top-level config object, PravegaConfig , is provided to establish a Pravega context for the Flink connector. The config object automatically configures itself from environment variables , system properties and program arguments . PravegaConfig information sources is given below: Setting Environment Variable / System Property / Program Argument Default Value Controller URI PRAVEGA_CONTROLLER_URI pravega.controller.uri --controller tcp://localhost:9090 Default Scope PRAVEGA_SCOPE pravega.scope --scope - Credentials - - Hostname Validation - true","title":"PravegaConfig Class"},{"location":"configurations/#creating-pravegaconfig","text":"The recommended way to create an instance of PravegaConfig is to pass an instance of ParameterTool to fromParams : ParameterTool params = ParameterTool . fromArgs ( args ); PravegaConfig config = PravegaConfig . fromParams ( params ); If your application doesn't use the ParameterTool class that is provided by Flink, create the PravegaConfig using fromDefaults : PravegaConfig config = PravegaConfig . fromDefaults (); The PravegaConfig class provides a builder-style API to override the default configuration settings: PravegaConfig config = PravegaConfig . fromDefaults () . withControllerURI ( \"tcp://...\" ) . withDefaultScope ( \"SCOPE-NAME\" ) . withCredentials ( credentials ) . withHostnameValidation ( false );","title":"Creating PravegaConfig"},{"location":"configurations/#using-pravegaconfig","text":"All of the various source and sink classes provided with the connector library have a builder-style API which accepts a PravegaConfig for common configuration. Pass a PravegaConfig object to the respective builder via withPravegaConfig . For example, see below code: PravegaConfig config = ...; FlinkPravegaReader < MyClass > pravegaSource = FlinkPravegaReader .< MyClass > builder () . forStream (...) . withPravegaConfig ( config ) . build ();","title":"Using PravegaConfig"},{"location":"configurations/#understanding-the-default-scope","text":"Pravega organizes streams into scopes for the purposes of manageability. The PravegaConfig establishes a default scope name that is used in two scenarios: For resolving unqualified stream names when constructing a source or sink. The sources and sinks accept stream names that may be qualified (e.g. my-scope/my-stream ) or unqualified (e.g. my-stream ). For establishing the scope name for the coordination stream underlying a Pravega Reader Group. It is important to note that, the FlinkPravegaReader and the FlinkPravegaTableSource use the default scope configured on PravegaConfig as their Reader Group scope, and provide withReaderGroupScope as an override. The scope name of input stream(s) doesn't influence the Reader Group scope.","title":"Understanding the Default Scope"},{"location":"getting-started/","text":"Pravega Flink Connectors This repository implements connectors to read and write Pravega Streams with Apache Flink stream processing framework. The connectors can be used to build end-to-end stream processing pipelines (see Samples ) that use Pravega as the stream storage and message bus, and Apache Flink for computation over the streams. Features & Highlights Exactly-once processing guarantees for both Reader and Writer, supporting end-to-end exactly-once processing pipelines Seamless integration with Flink's checkpoints and savepoints. Parallel Readers and Writers supporting high throughput and low latency processing. Table API support to access Pravega Streams for both Batch and Streaming use case. Building Connectors Building the connectors from the source is only necessary when we want to use or contribute to the latest ( unreleased ) version of the Pravega Flink connectors. The connector project is linked to a specific version of Pravega, based on a git submodule pointing to a commit-id. By default the sub-module option is disabled and the build step will make use of the Pravega version defined in the gradle.properties file. You could override this option by enabling usePravegaVersionSubModule flag in gradle.properties to true . Checkout the source code repository by following below steps: git clone --recursive https://github.com/pravega/flink-connectors.git After cloning the repository, the project can be built by running the below command in the project root directory flink-connectors . ./gradlew clean build To install the artifacts in the local maven repository cache ~/.m2/repository , run the following command: ./gradlew clean install Customizing the Build Building against a custom Flink version We can check and change the Flink version that Pravega builds against via the flinkVersion variable in the gradle.properties file. Note : Only Flink versions that are compatible with the latest connector code can be chosen. Building against another Scala version This section is only relevant if you use Scala in the stream processing application with Flink and Pravega. Parts of the Apache Flink use the language or depend on libraries written in Scala. Because Scala is not strictly compatible across versions, there exist different versions of Flink compiled for different Scala versions. If we use Scala code in the same application where we use the Apache Flink or the Flink connectors, we typically have to make sure we use a version of Flink that uses the same Scala version as our application. Each version of Flink has a preferred Scala version as determined by the official Flink docker image. We use the preferred version by default. To depend on released Flink artifacts for a different Scala version, you need to edit the build.gradle file and change all entries for the Flink dependencies to have a different Scala version suffix. For example, flink-streaming-java_2.11 would be replaced by flink-streaming-java_2.12 for Scala 2.12 . In order to build a new version of Flink for a different Scala version, please refer to the Flink documentation . Setting up your IDE Connector project uses Project Lombok , so we should ensure that we have our IDE setup with the required plugins. ( IntelliJ is recommended ). To import the source into IntelliJ: Import the project directory into IntelliJ IDE. It will automatically detect the gradle project and import things correctly. Enable Annotation Processing by going to Build, Execution, Deployment -> Compiler > Annotation Processors and checking Enable annotation processing . Install the Lombok Plugin . This can be found in Preferences -> Plugins . Restart your IDE. Connectors project compiles properly after applying the above steps. For eclipse, we can generate eclipse project files by running ./gradlew eclipse . Releases The latest releases can be found on the Github Release project page. Support Don\u2019t hesitate to ask! Contact the developers and community on the Slack if you need any help. Open an issue if you found a bug on Github Issues . Samples Follow the Pravega Samples repository to learn more about how to build and use the Flink Connector library.","title":"Getting Started"},{"location":"getting-started/#pravega-flink-connectors","text":"This repository implements connectors to read and write Pravega Streams with Apache Flink stream processing framework. The connectors can be used to build end-to-end stream processing pipelines (see Samples ) that use Pravega as the stream storage and message bus, and Apache Flink for computation over the streams.","title":"Pravega Flink Connectors"},{"location":"getting-started/#features-highlights","text":"Exactly-once processing guarantees for both Reader and Writer, supporting end-to-end exactly-once processing pipelines Seamless integration with Flink's checkpoints and savepoints. Parallel Readers and Writers supporting high throughput and low latency processing. Table API support to access Pravega Streams for both Batch and Streaming use case.","title":"Features &amp; Highlights"},{"location":"getting-started/#building-connectors","text":"Building the connectors from the source is only necessary when we want to use or contribute to the latest ( unreleased ) version of the Pravega Flink connectors. The connector project is linked to a specific version of Pravega, based on a git submodule pointing to a commit-id. By default the sub-module option is disabled and the build step will make use of the Pravega version defined in the gradle.properties file. You could override this option by enabling usePravegaVersionSubModule flag in gradle.properties to true . Checkout the source code repository by following below steps: git clone --recursive https://github.com/pravega/flink-connectors.git After cloning the repository, the project can be built by running the below command in the project root directory flink-connectors . ./gradlew clean build To install the artifacts in the local maven repository cache ~/.m2/repository , run the following command: ./gradlew clean install","title":"Building Connectors"},{"location":"getting-started/#customizing-the-build","text":"","title":"Customizing the Build"},{"location":"getting-started/#building-against-a-custom-flink-version","text":"We can check and change the Flink version that Pravega builds against via the flinkVersion variable in the gradle.properties file. Note : Only Flink versions that are compatible with the latest connector code can be chosen.","title":"Building against a custom Flink version"},{"location":"getting-started/#building-against-another-scala-version","text":"This section is only relevant if you use Scala in the stream processing application with Flink and Pravega. Parts of the Apache Flink use the language or depend on libraries written in Scala. Because Scala is not strictly compatible across versions, there exist different versions of Flink compiled for different Scala versions. If we use Scala code in the same application where we use the Apache Flink or the Flink connectors, we typically have to make sure we use a version of Flink that uses the same Scala version as our application. Each version of Flink has a preferred Scala version as determined by the official Flink docker image. We use the preferred version by default. To depend on released Flink artifacts for a different Scala version, you need to edit the build.gradle file and change all entries for the Flink dependencies to have a different Scala version suffix. For example, flink-streaming-java_2.11 would be replaced by flink-streaming-java_2.12 for Scala 2.12 . In order to build a new version of Flink for a different Scala version, please refer to the Flink documentation .","title":"Building against another Scala version"},{"location":"getting-started/#setting-up-your-ide","text":"Connector project uses Project Lombok , so we should ensure that we have our IDE setup with the required plugins. ( IntelliJ is recommended ). To import the source into IntelliJ: Import the project directory into IntelliJ IDE. It will automatically detect the gradle project and import things correctly. Enable Annotation Processing by going to Build, Execution, Deployment -> Compiler > Annotation Processors and checking Enable annotation processing . Install the Lombok Plugin . This can be found in Preferences -> Plugins . Restart your IDE. Connectors project compiles properly after applying the above steps. For eclipse, we can generate eclipse project files by running ./gradlew eclipse .","title":"Setting up your IDE"},{"location":"getting-started/#releases","text":"The latest releases can be found on the Github Release project page.","title":"Releases"},{"location":"getting-started/#support","text":"Don\u2019t hesitate to ask! Contact the developers and community on the Slack if you need any help. Open an issue if you found a bug on Github Issues .","title":"Support"},{"location":"getting-started/#samples","text":"Follow the Pravega Samples repository to learn more about how to build and use the Flink Connector library.","title":"Samples"},{"location":"metrics/","text":"Metrics Pravega metrics are collected and exposed via Flink metrics framework when using FlinkPravegaReader or FlinkPravegaWriter . Reader Metrics The following metrics are exposed for FlinkPravegaReader related operations: Name Description readerGroupName The name of the Reader Group. scope The scope name of the Reader Group. streams The fully qualified name (i.e., scope/stream ) of the streams that are part of the Reader Group. onlineReaders The readers that are currently online/available. segmentPositions The StreamCut information that indicates where the readers have read so far. unreadBytes The total number of bytes that have not been read yet. Writer Metrics For FlinkPravegaWriter related operations, only the stream name is exposed: Name Description streams The fully qualified name of the stream i.e., scope/stream Querying Metrics The metrics can be viewed either from Flink UI or using the Flink REST API (like below): curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . readerGroupName curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . scope curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . streams curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . onlineReaders curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . stream . test . segmentPositions curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . unreadBytes","title":"Metrics"},{"location":"metrics/#metrics","text":"Pravega metrics are collected and exposed via Flink metrics framework when using FlinkPravegaReader or FlinkPravegaWriter .","title":"Metrics"},{"location":"metrics/#reader-metrics","text":"The following metrics are exposed for FlinkPravegaReader related operations: Name Description readerGroupName The name of the Reader Group. scope The scope name of the Reader Group. streams The fully qualified name (i.e., scope/stream ) of the streams that are part of the Reader Group. onlineReaders The readers that are currently online/available. segmentPositions The StreamCut information that indicates where the readers have read so far. unreadBytes The total number of bytes that have not been read yet.","title":"Reader Metrics"},{"location":"metrics/#writer-metrics","text":"For FlinkPravegaWriter related operations, only the stream name is exposed: Name Description streams The fully qualified name of the stream i.e., scope/stream","title":"Writer Metrics"},{"location":"metrics/#querying-metrics","text":"The metrics can be viewed either from Flink UI or using the Flink REST API (like below): curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . readerGroupName curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . scope curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . streams curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . onlineReaders curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . stream . test . segmentPositions curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . unreadBytes","title":"Querying Metrics"},{"location":"quickstart/","text":"Getting Started Creating a Flink Stream Processing Project Note : You can skip this step if you have a streaming project set up already. Please use the following project templates and setup guidelines, to set up a stream processing project with Apache Flink using Connectors Project template for Java Project template for Scala Once after the set up, please follow the below instructions to add the Flink Pravega connectors to the project. Add the Connector Dependencies To add the Pravega connector dependencies to your project, add the following entry to your project file: (For example, pom.xml for Maven) <!-- Before Pravega 0.6 --> <dependency> <groupId> io.pravega </groupId> <artifactId> pravega-connectors-flink_2.12 </artifactId> <version> 0.5.1 </version> </dependency> <!-- Pravega 0.6 and After --> <dependency> <groupId> io.pravega </groupId> <artifactId> pravega-connectors-flink-1.9_2.12 </artifactId> <version> 0.6.0 </version> </dependency> Use appropriate version as necessary. 1.9 is the Flink Major-Minor version. 2.12 is the Scala version. 0.6.0 is the Pravega version. The snapshot versions are published to jcenter repository and the release artifacts are available in Maven Central repository. Alternatively, we could build and publish the connector artifacts to local maven repository by executing the following command and make use of that version as your application dependency. ./gradlew clean install Running / Deploying the Application From Flink's perspective, the connector to Pravega is part of the streaming application (not part of Flink's core runtime), so the connector code must be part of the application's code artifact (JAR file). Typically, a Flink application is bundled as a fat-jar (also known as an uber-jar ) , such that all its dependencies are embedded. The project set up should have been a success, if you have used the above linked templates/guides . If you set up a application's project and dependencies manually, you need to make sure that it builds a jar with dependencies , to include both the application and the connector classes. The Flink connector has embedded and shaded the pravega-client dependency of the same version. Please DO NOT include both pravega-client and pravega-flink-connector dependency into the jar file, otherwise there will be dependency conflict issues. If user application uses Table API and SQL, please DO NOT compile flink-table-planner or flink-table-planner-blink into the jar file as they are provided by the Flink cluster.","title":"Quick Start"},{"location":"quickstart/#getting-started","text":"","title":"Getting Started"},{"location":"quickstart/#creating-a-flink-stream-processing-project","text":"Note : You can skip this step if you have a streaming project set up already. Please use the following project templates and setup guidelines, to set up a stream processing project with Apache Flink using Connectors Project template for Java Project template for Scala Once after the set up, please follow the below instructions to add the Flink Pravega connectors to the project.","title":"Creating a Flink Stream Processing Project"},{"location":"quickstart/#add-the-connector-dependencies","text":"To add the Pravega connector dependencies to your project, add the following entry to your project file: (For example, pom.xml for Maven) <!-- Before Pravega 0.6 --> <dependency> <groupId> io.pravega </groupId> <artifactId> pravega-connectors-flink_2.12 </artifactId> <version> 0.5.1 </version> </dependency> <!-- Pravega 0.6 and After --> <dependency> <groupId> io.pravega </groupId> <artifactId> pravega-connectors-flink-1.9_2.12 </artifactId> <version> 0.6.0 </version> </dependency> Use appropriate version as necessary. 1.9 is the Flink Major-Minor version. 2.12 is the Scala version. 0.6.0 is the Pravega version. The snapshot versions are published to jcenter repository and the release artifacts are available in Maven Central repository. Alternatively, we could build and publish the connector artifacts to local maven repository by executing the following command and make use of that version as your application dependency. ./gradlew clean install","title":"Add the Connector Dependencies"},{"location":"quickstart/#running-deploying-the-application","text":"From Flink's perspective, the connector to Pravega is part of the streaming application (not part of Flink's core runtime), so the connector code must be part of the application's code artifact (JAR file). Typically, a Flink application is bundled as a fat-jar (also known as an uber-jar ) , such that all its dependencies are embedded. The project set up should have been a success, if you have used the above linked templates/guides . If you set up a application's project and dependencies manually, you need to make sure that it builds a jar with dependencies , to include both the application and the connector classes. The Flink connector has embedded and shaded the pravega-client dependency of the same version. Please DO NOT include both pravega-client and pravega-flink-connector dependency into the jar file, otherwise there will be dependency conflict issues. If user application uses Table API and SQL, please DO NOT compile flink-table-planner or flink-table-planner-blink into the jar file as they are provided by the Flink cluster.","title":"Running / Deploying the Application"},{"location":"serialization/","text":"Serialization Serialization refers to converting a data element in your Flink program to/from a message in a Pravega stream. Flink defines a standard interface for data serialization to/from byte messages delivered by various connectors. The core interfaces are: - org.apache.flink.streaming.util.serialization.SerializationSchema - org.apache.flink.streaming.util.serialization.DeserializationSchema Built-in serializers include: - org.apache.flink.streaming.util.serialization.SimpleStringSchema - org.apache.flink.streaming.util.serialization.TypeInformationSerializationSchema The Pravega connector is designed to use Flink's serialization interfaces. For example, to read each stream event as a UTF-8 string: DeserializationSchema < String > schema = new SimpleStringSchema (); FlinkPravegaReader < String > reader = new FlinkPravegaReader <>(..., schema ); DataStream < MyEvent > stream = env . addSource ( reader ); Interoperability with Other Applications A common scenario is using Flink to process Pravega stream data produced by a non-Flink application. The Pravega client library used by such applications defines the io.pravega.client.stream.Serializer interface for working with event data. The implementations of Serializer directly in a Flink program via built-in adapters can be used: - io.pravega.connectors.flink.serialization.PravegaSerializationSchema - io.pravega.connectors.flink.serialization.PravegaDeserializationSchema Below is an example, to pass an instance of the appropriate Pravega de/serializer class to the adapter's constructor: import io.pravega.client.stream.impl.JavaSerializer ; ... DeserializationSchema < MyEvent > adapter = new PravegaDeserializationSchema <>( MyEvent . class , new JavaSerializer < MyEvent >()); FlinkPravegaReader < MyEvent > reader = new FlinkPravegaReader <>(..., adapter ); DataStream < MyEvent > stream = env . addSource ( reader ); Note that the Pravega serializer must implement java.io.Serializable to be usable in a Flink program. Deserialize with metadata Pravega reader client wraps the event with the metadata in an EventRead data structure. Some Flink jobs might care about the stream position of the event data which is in EventRead , e.g. for indexing purposes. PravegaDeserializationSchema offers a method to extract event with the metadata public T extractEvent ( EventRead < T > eventRead ) { return eventRead . getEvent (); } The default implementation can be overwritten to involve in metadata structure like EventPointer into the event by a custom extended PravegaDeserializationSchema . For example: private static class MyJsonDeserializationSchema extends PravegaDeserializationSchema < JsonNode > { private boolean includeMetadata ; public MyJsonDeserializationSchema ( boolean includeMetadata ) { super ( JsonNode . class , new JSONSerializer ()); this . includeMetadata = includeMetadata ; } @Override public JsonNode extractEvent ( EventRead < JsonNode > eventRead ) { JsonNode node = eventRead . getEvent (); if ( includeMetadata ) { return (( ObjectNode ) node ). put ( \"eventpointer\" , eventRead . getEventPointer (). toBytes (). array ()); } return node ; } }","title":"Serialization"},{"location":"serialization/#serialization","text":"Serialization refers to converting a data element in your Flink program to/from a message in a Pravega stream. Flink defines a standard interface for data serialization to/from byte messages delivered by various connectors. The core interfaces are: - org.apache.flink.streaming.util.serialization.SerializationSchema - org.apache.flink.streaming.util.serialization.DeserializationSchema Built-in serializers include: - org.apache.flink.streaming.util.serialization.SimpleStringSchema - org.apache.flink.streaming.util.serialization.TypeInformationSerializationSchema The Pravega connector is designed to use Flink's serialization interfaces. For example, to read each stream event as a UTF-8 string: DeserializationSchema < String > schema = new SimpleStringSchema (); FlinkPravegaReader < String > reader = new FlinkPravegaReader <>(..., schema ); DataStream < MyEvent > stream = env . addSource ( reader );","title":"Serialization"},{"location":"serialization/#interoperability-with-other-applications","text":"A common scenario is using Flink to process Pravega stream data produced by a non-Flink application. The Pravega client library used by such applications defines the io.pravega.client.stream.Serializer interface for working with event data. The implementations of Serializer directly in a Flink program via built-in adapters can be used: - io.pravega.connectors.flink.serialization.PravegaSerializationSchema - io.pravega.connectors.flink.serialization.PravegaDeserializationSchema Below is an example, to pass an instance of the appropriate Pravega de/serializer class to the adapter's constructor: import io.pravega.client.stream.impl.JavaSerializer ; ... DeserializationSchema < MyEvent > adapter = new PravegaDeserializationSchema <>( MyEvent . class , new JavaSerializer < MyEvent >()); FlinkPravegaReader < MyEvent > reader = new FlinkPravegaReader <>(..., adapter ); DataStream < MyEvent > stream = env . addSource ( reader ); Note that the Pravega serializer must implement java.io.Serializable to be usable in a Flink program.","title":"Interoperability with Other Applications"},{"location":"serialization/#deserialize-with-metadata","text":"Pravega reader client wraps the event with the metadata in an EventRead data structure. Some Flink jobs might care about the stream position of the event data which is in EventRead , e.g. for indexing purposes. PravegaDeserializationSchema offers a method to extract event with the metadata public T extractEvent ( EventRead < T > eventRead ) { return eventRead . getEvent (); } The default implementation can be overwritten to involve in metadata structure like EventPointer into the event by a custom extended PravegaDeserializationSchema . For example: private static class MyJsonDeserializationSchema extends PravegaDeserializationSchema < JsonNode > { private boolean includeMetadata ; public MyJsonDeserializationSchema ( boolean includeMetadata ) { super ( JsonNode . class , new JSONSerializer ()); this . includeMetadata = includeMetadata ; } @Override public JsonNode extractEvent ( EventRead < JsonNode > eventRead ) { JsonNode node = eventRead . getEvent (); if ( includeMetadata ) { return (( ObjectNode ) node ). put ( \"eventpointer\" , eventRead . getEventPointer (). toBytes (). array ()); } return node ; } }","title":"Deserialize with metadata"},{"location":"streaming/","text":"Streaming Connector The Flink Connector library for Pravega provides a data source and data sink for use with the Flink Streaming API. See the below sections for details. Table of Contents FlinkPravegaReader Parameters Input Stream(s) Reader Parallelism Checkpointing Timestamp Extraction (Watermark Emission) Stream Cuts Historical Stream Processing FlinkPravegaWriter Parameters Writer Parallelism Event Routing Event Time Ordering Watermark Writer Modes Metrics Data Serialization FlinkPravegaReader A Pravega Stream may be used as a data source within a Flink streaming program using an instance of io.pravega.connectors.flink.FlinkPravegaReader . The reader reads a given Pravega Stream (or multiple streams) as a DataStream (the basic abstraction of the Flink Streaming API). Open a Pravega Stream as a DataStream using the method StreamExecutionEnvironment::addSource . Example StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event deserializer DeserializationSchema < MyClass > deserializer = ... // Define the data stream FlinkPravegaReader < MyClass > pravegaSource = FlinkPravegaReader .< MyClass > builder () . forStream (...) . withPravegaConfig ( config ) . withDeserializationSchema ( deserializer ) . build (); DataStream < MyClass > stream = env . addSource ( pravegaSource ); Parameters A builder API is provided to construct an instance of FlinkPravegaReader . See the table below for a summary of builder properties. Note that, the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. uid The uid to identify the checkpoint state of this source. withReaderGroupScope The scope to store the Reader Group synchronization stream into. withReaderGroupName The Reader Group name for display purposes. withReaderGroupRefreshTime The interval for synchronizing the Reader Group state across parallel source instances. withCheckpointInitiateTimeout The timeout for executing a checkpoint of the Reader Group state. withDeserializationSchema The deserialization schema which describes how to turn byte messages into events. withTimestampAssigner The AssignerWithTimeWindows implementation which describes the event timestamp and Pravega watermark strategy in event time semantics. enableMetrics true or false to enable/disable reporting Pravega metrics. Metrics is enabled by default. Input Stream(s) Each stream in Pravega is contained by a scope. A scope acts as a namespace for one or more streams. The FlinkPravegaReader is able to read from numerous streams in parallel, even across scopes. The builder API accepts both qualified and unqualified stream names. In qualified, the scope is explicitly specified, e.g. my-scope/my-stream . In Unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") . Reader Parallelism The FlinkPravegaReader supports parallelization. Use the setParallelism method to of Datastream to configure the number of parallel instances to execute. The parallel instances consume the stream in a coordinated manner, each consuming one or more stream segments. Note: Coordination is achieved with the use of a Pravega Reader Group, which is based on a State Synchronizer . The Synchronizer creates a backing stream that may be manually deleted after the completion of the job. Checkpointing In order to make state fault tolerant, Flink needs to checkpoint the state. Checkpoints allow Flink to recover state and positions in the streams to give the application the same semantics as a failure-free execution. The reader is compatible with Flink checkpoints and savepoints. The reader automatically recovers from failure by rewinding to the checkpointed position in the stream. A savepoint is self-contained; it contains all information needed to resume from the correct position. The checkpoint mechanism works as a two-step process: The master hook handler from the job manager initiates the triggerCheckpoint request to the ReaderCheckpointHook that was registered with the Job Manager during FlinkPravegaReader source initialization. The ReaderCheckpointHook handler notifies Pravega to checkpoint the current reader state. This is a non-blocking call which returns a future once Pravega readers are done with the checkpointing. A CheckPoint event will be sent by Pravega as part of the data stream flow and on receiving the event, the FlinkPravegaReader will initiate triggerCheckpoint request to effectively let Flink continue and complete the checkpoint process. Timestamp Extraction (Watermark Emission) Flink requires the events\u2019 timestamps (each element in the stream needs to have its event timestamp assigned). This is achieved by accessing/extracting the timestamp from some field in the element. These are used to tell the system about progress in event time. Since Pravega 0.6, Pravega has proposed a new watermarking API to enable the writer to provide time information. On the reader side, a new concept TimeWindow is proposed to represent a time window for the events which are currently being read by a reader. It is possible to use event time semantics with either pravega watermark (after 0.6) or normal watermark. To use Pravega watermark, an interface called AssignerWithTimeWindows should be implemented in the application via an application-specific timestamp assigner and a watermark generator with TimeWindow . Different applications can choose to be more or less conservative with the given TimeWindow . LowerBoundAssigner is provided as a default implementation of the most conservative watermark. LowerBoundAssigner periodically emits the watermark which equals the lower bound of TimeWindow. You can set the period of watermark emission like below. StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); env . getConfig (). setAutoWatermarkInterval ( AUTO_WATERMARK_INTERVAL_MS ); To use normal watermark, you can follow Flink documentation . Simply, specify an AssignerWithPeriodicWatermarks or AssignerWithPunctuatedWatermarks on the DataStream as normal. Each parallel instance of the source processes one or more stream segments in parallel. Each watermark generator instance will receive events multiplexed from numerous segments. Be aware that segments are processed in parallel, and that no effort is made to order the events across segments in terms of their event time. Also, a given segment may be reassigned to another parallel instance at any time, preserving exactly-once behavior but causing further spread in observed event times. StreamCuts A StreamCut represents a specific position in a Pravega Stream, which may be obtained from various API interactions with the Pravega client. The FlinkPravegaReader accepts a StreamCut as the start and/or end position of a given stream. For further reading on StreamCuts, please refer to documentation on StreamCut and sample code . Many Readers will be reading Events from the tail of the Stream. Tail reads corresponding to recently written Events are immediately delivered to Readers. Here is an example for a Flink application to perform tail-read. StreamManager streamManager = StreamManager . create ( pravegaConfig . getClientConfig ()); StreamCut tailStreamCut = streamManager . getStreamInfo ( \"scope\" , \"stream\" ). getTailStreamCut (); FlinkPravegaReader < MyClass > pravegaSource = FlinkPravegaReader .< MyClass > builder () . forStream ( streamName , tailStreamCut ) . withPravegaConfig ( config ) . withDeserializationSchema ( deserializer ) . build (); DataStream < MyClass > stream = env . addSource ( pravegaSource ); Historical Stream Processing Historical processing refers to processing stream data from a specific position in the stream rather than from the stream's tail. The builder API provides an overloaded method forStream that accepts a StreamCut parameter for this purpose. One such example is re-processing a stream, where we may have to process the data from the beginning (or from a certain point in the stream) to re-derive the output. For instance, in situations where the computation logic has been changed to address new additional criteria, or we fixed a bug or doing a typical A/B testing etc., where the ability to consume historical data as a stream is critical. FlinkPravegaWriter A Pravega Stream may be used as a data sink within a Flink program using an instance of io.pravega.connectors.flink.FlinkPravegaWriter . Add an instance of the writer to the dataflow program using the method DataStream::addSink . Example StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event serializer SerializationSchema < MyClass > serializer = ... // Define the event router for selecting the Routing Key PravegaEventRouter < MyClass > router = ... // Define the sink function FlinkPravegaWriter < MyClass > pravegaSink = FlinkPravegaWriter .< MyClass > builder () . forStream (...) . withPravegaConfig ( config ) . withSerializationSchema ( serializer ) . withEventRouter ( router ) . withWriterMode ( PravegaWriterMode . EXACTLY_ONCE ) . build (); DataStream < MyClass > stream = ... stream . addSink ( pravegaSink ); Parameters A builder API is provided to construct an instance of FlinkPravegaWriter . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withWriterMode The writer mode to provide Best-effort , At-least-once , or Exactly-once guarantees. withTxnLeaseRenewalPeriod The Transaction lease renewal period that supports the Exactly-once writer mode. withSerializationSchema The serialization schema which describes how to turn events into byte messages. withEventRouter The router function which determines the Routing Key for a given event. enableWatermark true or false to enable/disable emitting Flink watermark in event-time semantics to Pravega streams. enableMetrics true or false to enable/disable reporting Pravega metrics. Metrics is enabled by default. Writer Parallelism FlinkPravegaWriter supports parallelization. Use the setParallelism method to configure the number of parallel instances to execute. Event Routing Every event written to a Pravega Stream has an associated Routing Key. The Routing Key is the basis for event ordering. See the Pravega Concepts for details. When constructing the FlinkPravegaWriter , please provide an implementation of io.pravega.connectors.flink.PravegaEventRouter which will guarantee the event ordering. In Pravega, events are guaranteed to be ordered at the segment level. For example, to guarantee write order specific to sensor id, you could provide a router implementation like below. private static class SensorEventRouter < SensorEvent > implements PravegaEventRouter < SensorEvent > { @Override public String getRoutingKey ( SensorEvent event ) { return event . getId (); } } Event Time Ordering For programs that use Flink's event time semantics, the connector library supports writing events in event time order. In combination with a Routing Key, this establishes a well-understood ordering for each key in the output stream. Use the method FlinkPravegaUtils::writeToPravegaInEventTimeOrder to write a given DataStream to a Pravega Stream such that events are automatically ordered by event time (on a per-key basis). Refer here for sample code. Watermark Flink applications in event time semantics are carrying watermarks within each operator. Both Pravega transactional and non-transactional writers provide watermark API to indicate the event-time watermark for a stream. With enableWatermark(true) , each watermark in Flink will be emitted into a Pravega stream. Writer Modes Writer modes relate to guarantees about the persistence of events emitted by the sink to a Pravega Stream. The writer supports three writer modes: Best-effort - Any write failures will be ignored hence there could be data loss. At-least-once - All events are persisted in Pravega. Duplicate events are possible, due to retries or in case of failure and subsequent recovery. Exactly-once - All events are persisted in Pravega using a transactional approach integrated with the Flink checkpointing feature. By default, the At-least-once option is enabled and use .withWriterMode(...) option to override the value. See the Pravega documentation for details on transactional behavior. Metrics Metrics are reported by default unless it is explicitly disabled using enableMetrics(false) option. See Metrics page for more details on type of metrics that are reported. Serialization See the serialization page for more information on how to use the serializer and deserializer .","title":"Streaming"},{"location":"streaming/#streaming-connector","text":"The Flink Connector library for Pravega provides a data source and data sink for use with the Flink Streaming API. See the below sections for details.","title":"Streaming Connector"},{"location":"streaming/#table-of-contents","text":"FlinkPravegaReader Parameters Input Stream(s) Reader Parallelism Checkpointing Timestamp Extraction (Watermark Emission) Stream Cuts Historical Stream Processing FlinkPravegaWriter Parameters Writer Parallelism Event Routing Event Time Ordering Watermark Writer Modes Metrics Data Serialization","title":"Table of Contents"},{"location":"streaming/#flinkpravegareader","text":"A Pravega Stream may be used as a data source within a Flink streaming program using an instance of io.pravega.connectors.flink.FlinkPravegaReader . The reader reads a given Pravega Stream (or multiple streams) as a DataStream (the basic abstraction of the Flink Streaming API). Open a Pravega Stream as a DataStream using the method StreamExecutionEnvironment::addSource .","title":"FlinkPravegaReader"},{"location":"streaming/#example","text":"StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event deserializer DeserializationSchema < MyClass > deserializer = ... // Define the data stream FlinkPravegaReader < MyClass > pravegaSource = FlinkPravegaReader .< MyClass > builder () . forStream (...) . withPravegaConfig ( config ) . withDeserializationSchema ( deserializer ) . build (); DataStream < MyClass > stream = env . addSource ( pravegaSource );","title":"Example"},{"location":"streaming/#parameters","text":"A builder API is provided to construct an instance of FlinkPravegaReader . See the table below for a summary of builder properties. Note that, the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. uid The uid to identify the checkpoint state of this source. withReaderGroupScope The scope to store the Reader Group synchronization stream into. withReaderGroupName The Reader Group name for display purposes. withReaderGroupRefreshTime The interval for synchronizing the Reader Group state across parallel source instances. withCheckpointInitiateTimeout The timeout for executing a checkpoint of the Reader Group state. withDeserializationSchema The deserialization schema which describes how to turn byte messages into events. withTimestampAssigner The AssignerWithTimeWindows implementation which describes the event timestamp and Pravega watermark strategy in event time semantics. enableMetrics true or false to enable/disable reporting Pravega metrics. Metrics is enabled by default.","title":"Parameters"},{"location":"streaming/#input-streams","text":"Each stream in Pravega is contained by a scope. A scope acts as a namespace for one or more streams. The FlinkPravegaReader is able to read from numerous streams in parallel, even across scopes. The builder API accepts both qualified and unqualified stream names. In qualified, the scope is explicitly specified, e.g. my-scope/my-stream . In Unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") .","title":"Input Stream(s)"},{"location":"streaming/#reader-parallelism","text":"The FlinkPravegaReader supports parallelization. Use the setParallelism method to of Datastream to configure the number of parallel instances to execute. The parallel instances consume the stream in a coordinated manner, each consuming one or more stream segments. Note: Coordination is achieved with the use of a Pravega Reader Group, which is based on a State Synchronizer . The Synchronizer creates a backing stream that may be manually deleted after the completion of the job.","title":"Reader Parallelism"},{"location":"streaming/#checkpointing","text":"In order to make state fault tolerant, Flink needs to checkpoint the state. Checkpoints allow Flink to recover state and positions in the streams to give the application the same semantics as a failure-free execution. The reader is compatible with Flink checkpoints and savepoints. The reader automatically recovers from failure by rewinding to the checkpointed position in the stream. A savepoint is self-contained; it contains all information needed to resume from the correct position. The checkpoint mechanism works as a two-step process: The master hook handler from the job manager initiates the triggerCheckpoint request to the ReaderCheckpointHook that was registered with the Job Manager during FlinkPravegaReader source initialization. The ReaderCheckpointHook handler notifies Pravega to checkpoint the current reader state. This is a non-blocking call which returns a future once Pravega readers are done with the checkpointing. A CheckPoint event will be sent by Pravega as part of the data stream flow and on receiving the event, the FlinkPravegaReader will initiate triggerCheckpoint request to effectively let Flink continue and complete the checkpoint process.","title":"Checkpointing"},{"location":"streaming/#timestamp-extraction-watermark-emission","text":"Flink requires the events\u2019 timestamps (each element in the stream needs to have its event timestamp assigned). This is achieved by accessing/extracting the timestamp from some field in the element. These are used to tell the system about progress in event time. Since Pravega 0.6, Pravega has proposed a new watermarking API to enable the writer to provide time information. On the reader side, a new concept TimeWindow is proposed to represent a time window for the events which are currently being read by a reader. It is possible to use event time semantics with either pravega watermark (after 0.6) or normal watermark. To use Pravega watermark, an interface called AssignerWithTimeWindows should be implemented in the application via an application-specific timestamp assigner and a watermark generator with TimeWindow . Different applications can choose to be more or less conservative with the given TimeWindow . LowerBoundAssigner is provided as a default implementation of the most conservative watermark. LowerBoundAssigner periodically emits the watermark which equals the lower bound of TimeWindow. You can set the period of watermark emission like below. StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); env . getConfig (). setAutoWatermarkInterval ( AUTO_WATERMARK_INTERVAL_MS ); To use normal watermark, you can follow Flink documentation . Simply, specify an AssignerWithPeriodicWatermarks or AssignerWithPunctuatedWatermarks on the DataStream as normal. Each parallel instance of the source processes one or more stream segments in parallel. Each watermark generator instance will receive events multiplexed from numerous segments. Be aware that segments are processed in parallel, and that no effort is made to order the events across segments in terms of their event time. Also, a given segment may be reassigned to another parallel instance at any time, preserving exactly-once behavior but causing further spread in observed event times.","title":"Timestamp Extraction (Watermark Emission)"},{"location":"streaming/#streamcuts","text":"A StreamCut represents a specific position in a Pravega Stream, which may be obtained from various API interactions with the Pravega client. The FlinkPravegaReader accepts a StreamCut as the start and/or end position of a given stream. For further reading on StreamCuts, please refer to documentation on StreamCut and sample code . Many Readers will be reading Events from the tail of the Stream. Tail reads corresponding to recently written Events are immediately delivered to Readers. Here is an example for a Flink application to perform tail-read. StreamManager streamManager = StreamManager . create ( pravegaConfig . getClientConfig ()); StreamCut tailStreamCut = streamManager . getStreamInfo ( \"scope\" , \"stream\" ). getTailStreamCut (); FlinkPravegaReader < MyClass > pravegaSource = FlinkPravegaReader .< MyClass > builder () . forStream ( streamName , tailStreamCut ) . withPravegaConfig ( config ) . withDeserializationSchema ( deserializer ) . build (); DataStream < MyClass > stream = env . addSource ( pravegaSource );","title":"StreamCuts"},{"location":"streaming/#historical-stream-processing","text":"Historical processing refers to processing stream data from a specific position in the stream rather than from the stream's tail. The builder API provides an overloaded method forStream that accepts a StreamCut parameter for this purpose. One such example is re-processing a stream, where we may have to process the data from the beginning (or from a certain point in the stream) to re-derive the output. For instance, in situations where the computation logic has been changed to address new additional criteria, or we fixed a bug or doing a typical A/B testing etc., where the ability to consume historical data as a stream is critical.","title":"Historical Stream Processing"},{"location":"streaming/#flinkpravegawriter","text":"A Pravega Stream may be used as a data sink within a Flink program using an instance of io.pravega.connectors.flink.FlinkPravegaWriter . Add an instance of the writer to the dataflow program using the method DataStream::addSink .","title":"FlinkPravegaWriter"},{"location":"streaming/#example_1","text":"StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event serializer SerializationSchema < MyClass > serializer = ... // Define the event router for selecting the Routing Key PravegaEventRouter < MyClass > router = ... // Define the sink function FlinkPravegaWriter < MyClass > pravegaSink = FlinkPravegaWriter .< MyClass > builder () . forStream (...) . withPravegaConfig ( config ) . withSerializationSchema ( serializer ) . withEventRouter ( router ) . withWriterMode ( PravegaWriterMode . EXACTLY_ONCE ) . build (); DataStream < MyClass > stream = ... stream . addSink ( pravegaSink );","title":"Example"},{"location":"streaming/#parameters_1","text":"A builder API is provided to construct an instance of FlinkPravegaWriter . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withWriterMode The writer mode to provide Best-effort , At-least-once , or Exactly-once guarantees. withTxnLeaseRenewalPeriod The Transaction lease renewal period that supports the Exactly-once writer mode. withSerializationSchema The serialization schema which describes how to turn events into byte messages. withEventRouter The router function which determines the Routing Key for a given event. enableWatermark true or false to enable/disable emitting Flink watermark in event-time semantics to Pravega streams. enableMetrics true or false to enable/disable reporting Pravega metrics. Metrics is enabled by default.","title":"Parameters"},{"location":"streaming/#writer-parallelism","text":"FlinkPravegaWriter supports parallelization. Use the setParallelism method to configure the number of parallel instances to execute.","title":"Writer Parallelism"},{"location":"streaming/#event-routing","text":"Every event written to a Pravega Stream has an associated Routing Key. The Routing Key is the basis for event ordering. See the Pravega Concepts for details. When constructing the FlinkPravegaWriter , please provide an implementation of io.pravega.connectors.flink.PravegaEventRouter which will guarantee the event ordering. In Pravega, events are guaranteed to be ordered at the segment level. For example, to guarantee write order specific to sensor id, you could provide a router implementation like below. private static class SensorEventRouter < SensorEvent > implements PravegaEventRouter < SensorEvent > { @Override public String getRoutingKey ( SensorEvent event ) { return event . getId (); } }","title":"Event Routing"},{"location":"streaming/#event-time-ordering","text":"For programs that use Flink's event time semantics, the connector library supports writing events in event time order. In combination with a Routing Key, this establishes a well-understood ordering for each key in the output stream. Use the method FlinkPravegaUtils::writeToPravegaInEventTimeOrder to write a given DataStream to a Pravega Stream such that events are automatically ordered by event time (on a per-key basis). Refer here for sample code.","title":"Event Time Ordering"},{"location":"streaming/#watermark","text":"Flink applications in event time semantics are carrying watermarks within each operator. Both Pravega transactional and non-transactional writers provide watermark API to indicate the event-time watermark for a stream. With enableWatermark(true) , each watermark in Flink will be emitted into a Pravega stream.","title":"Watermark"},{"location":"streaming/#writer-modes","text":"Writer modes relate to guarantees about the persistence of events emitted by the sink to a Pravega Stream. The writer supports three writer modes: Best-effort - Any write failures will be ignored hence there could be data loss. At-least-once - All events are persisted in Pravega. Duplicate events are possible, due to retries or in case of failure and subsequent recovery. Exactly-once - All events are persisted in Pravega using a transactional approach integrated with the Flink checkpointing feature. By default, the At-least-once option is enabled and use .withWriterMode(...) option to override the value. See the Pravega documentation for details on transactional behavior.","title":"Writer Modes"},{"location":"streaming/#metrics","text":"Metrics are reported by default unless it is explicitly disabled using enableMetrics(false) option. See Metrics page for more details on type of metrics that are reported.","title":"Metrics"},{"location":"streaming/#serialization","text":"See the serialization page for more information on how to use the serializer and deserializer .","title":"Serialization"},{"location":"table-api/","text":"Table Connector The Flink connector library for Pravega provides a table source and table sink for use with the Flink Table API. The Table API provides a unified table source API for both the Flink streaming and batch environment, and also sink for the Flink streaming environment. It is possible to treat the Pravega streams as tables with the help of Flink. See the below sections for details. Table of Contents Introduction How to create a table Connector options Features Batch and Streaming read Specify start and end streamcut Changelog Source Routing key by column Consistency guarantees Useful Flink links Introduction Before Flink 1.10 connector, the connector has implemented Flink legacy TableFactory interface to support table mapping, and provided FlinkPravegaTableSource and FlinkPravegaTableSink to read and write Pravega as Flink tables via a Pravega descriptor. Since Flink 1.11 connector, as Flink introduces a new Table API with FLIP-95 , we integrate Flink Factory interface and provided FlinkPravegaDynamicTableSource and FlinkPravegaDynamicTableSink to simplify the application coding. Note that the legacy table API is deprecated and will be removed in the future releases, we strongly suggest users to switch to the new table API. We will focus on the new table API introduction in the document below, please refer to the documentation of older versions if you want to check the legacy table API. Pravega table source supports both the Flink streaming and batch environments. Pravega table sink is an append-only table sink, it does NOT support upsert/retract output. How to create a table Pravega Stream can be used as a table source/sink within a Flink table program. The example below shows how to create a table connecting a Pravega stream as both source and sink: create table pravega ( user_id STRING , item_id BIGINT , category_id BIGINT , behavior STRING , log_ts TIMESTAMP ( 3 ), ts as log_ts + INTERVAL '1' SECOND , watermark for ts as ts ) with ( 'connector' = 'pravega' 'controller-uri' = 'tcp://localhost:9090' , 'scope' = 'scope' , 'scan.execution.type' = 'streaming' , 'scan.reader-group.name' = 'group1' , 'scan.streams' = 'stream' , 'sink.stream' = 'stream' , 'sink.routing-key.field.name' = 'user_id' , 'format' = 'json' ) Connector options Option Required Default Type Description connector required (none) String Specify what connector to use, here should be 'pravega' controller-uri required (none) String Pravega controller URI security.auth-type optional (none) String Static authentication/authorization type for security security.auth-token optional (none) String Static authentication/authorization token for security security.validate-hostname optional (none) Boolean If host name validation should be enabled when TLS is enabled security.trust-store optional (none) String Trust Store for Pravega client scan.execution.type optional streaming String Execution type for scan source. Valid values are 'streaming', 'batch'. scan.reader-group.name required for streaming source (none) String Pravega reader group name scan.streams required for source (none) List Semicolon-separated list of stream names from which the table is read. scan.start-streamcuts optional (none) List Semicolon-separated list of base64 encoded strings for start streamcuts, begin of the stream if not specified scan.end-streamcuts optional (none) List Semicolon-separated list of base64 encoded strings for end streamcuts, unbounded end if not specified scan.reader-group.max-outstanding-checkpoint-request optional 3 Integer Maximum outstanding checkpoint requests to Pravega scan.reader-group.refresh.interval optional 3 s Duration Refresh interval for reader group scan.event-read.timeout.interval optional 1 s Duration Timeout for the call to read events from Pravega scan.reader-group.checkpoint-initiate-timeout.interval optional 5 s Duration Timeout for call that initiates the Pravega checkpoint sink.stream required for sink (none) String Stream name to which the table is written sink.semantic optional at-least-once String Semantic when commit. Valid values are 'at-least-once', 'exactly-once', 'best-effort' sink.txn-lease-renewal.interval optional 30 s Duration Transaction lease renewal period, valid for exactly-once semantic. sink.enable.watermark-propagation optional false Boolean If watermark propagation should be enabled from Flink table to Pravega stream sink.routing-key.field.name optional (none) String Field name to use as a Pravega event routing key, field type must be STRING, random routing if not specified. Features Batch and Streaming read scan.execution.type can be specified as user's choice to perform batch read or streaming read. In the streaming environment, the table source uses a FlinkPravegaReader connector. In the batch environment, the table source uses a FlinkPravegaInputFormat connector. Please see the documentation of Streaming Connector and Batch Connector to have a better understanding on the below mentioned parameter list. Specify start and end streamcut A StreamCut represents a consistent position in the stream, and can be fetched from other applications uses Pravega client through checkpoints or custom defined index. scan.start-streamcuts and scan.end-streamcuts can be specified to perform bounded read and \"start-at-some-point\" read for Pravega streams. Pravega source supports read from multiple streams, and if read from multiple streams, please make sure the order of the streamcuts keeps the same as the order of the streams. Changelog Source If messages in Pravega stream is change event captured from other databases using CDC tools, then you can use a CDC format to interpret messages as INSERT/UPDATE/DELETE messages into Flink SQL system. Flink provides two CDC formats debezium-json and canal-json to interpret change events captured by Debezium and Canal. The changelog source is a very useful feature in many cases, such as synchronizing incremental data from databases to other systems, auditing logs, materialized views on databases, temporal join changing history of a database table and so on. See more about how to use the CDC formats in debezium-json and canal-json Routing key by column Pravega writers can use domain specific meaningful Routing Keys (like customer ID, Timestamp, Machine ID, etc.) to group similar together and make such parallelism with segment scaling. Pravega makes ordering guarantees in terms of routing keys. Pravega sink supports event routing according to a certain event field by specifying sink.routing-key.field.name . This field type must be STRING , and it will be random routing if not specified. Consistency guarantees By default, a Pravega sink ingests data with at-least-once guarantees if the query is executed with checkpointing enabled. sink.semantic: exactly-once can be specified to turn on the transactional writes with exactly-once guarantees. Useful Flink links Users can try with Pravega table APIs quickly though Flink SQL client. Here is some tutorial to setup the environment. https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/sqlClient.html The usage and definition of time attribute and WATERMARK schema can be referred in: https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/streaming/time_attributes.html","title":"Table API"},{"location":"table-api/#table-connector","text":"The Flink connector library for Pravega provides a table source and table sink for use with the Flink Table API. The Table API provides a unified table source API for both the Flink streaming and batch environment, and also sink for the Flink streaming environment. It is possible to treat the Pravega streams as tables with the help of Flink. See the below sections for details.","title":"Table Connector"},{"location":"table-api/#table-of-contents","text":"Introduction How to create a table Connector options Features Batch and Streaming read Specify start and end streamcut Changelog Source Routing key by column Consistency guarantees Useful Flink links","title":"Table of Contents"},{"location":"table-api/#introduction","text":"Before Flink 1.10 connector, the connector has implemented Flink legacy TableFactory interface to support table mapping, and provided FlinkPravegaTableSource and FlinkPravegaTableSink to read and write Pravega as Flink tables via a Pravega descriptor. Since Flink 1.11 connector, as Flink introduces a new Table API with FLIP-95 , we integrate Flink Factory interface and provided FlinkPravegaDynamicTableSource and FlinkPravegaDynamicTableSink to simplify the application coding. Note that the legacy table API is deprecated and will be removed in the future releases, we strongly suggest users to switch to the new table API. We will focus on the new table API introduction in the document below, please refer to the documentation of older versions if you want to check the legacy table API. Pravega table source supports both the Flink streaming and batch environments. Pravega table sink is an append-only table sink, it does NOT support upsert/retract output.","title":"Introduction"},{"location":"table-api/#how-to-create-a-table","text":"Pravega Stream can be used as a table source/sink within a Flink table program. The example below shows how to create a table connecting a Pravega stream as both source and sink: create table pravega ( user_id STRING , item_id BIGINT , category_id BIGINT , behavior STRING , log_ts TIMESTAMP ( 3 ), ts as log_ts + INTERVAL '1' SECOND , watermark for ts as ts ) with ( 'connector' = 'pravega' 'controller-uri' = 'tcp://localhost:9090' , 'scope' = 'scope' , 'scan.execution.type' = 'streaming' , 'scan.reader-group.name' = 'group1' , 'scan.streams' = 'stream' , 'sink.stream' = 'stream' , 'sink.routing-key.field.name' = 'user_id' , 'format' = 'json' )","title":"How to create a table"},{"location":"table-api/#connector-options","text":"Option Required Default Type Description connector required (none) String Specify what connector to use, here should be 'pravega' controller-uri required (none) String Pravega controller URI security.auth-type optional (none) String Static authentication/authorization type for security security.auth-token optional (none) String Static authentication/authorization token for security security.validate-hostname optional (none) Boolean If host name validation should be enabled when TLS is enabled security.trust-store optional (none) String Trust Store for Pravega client scan.execution.type optional streaming String Execution type for scan source. Valid values are 'streaming', 'batch'. scan.reader-group.name required for streaming source (none) String Pravega reader group name scan.streams required for source (none) List Semicolon-separated list of stream names from which the table is read. scan.start-streamcuts optional (none) List Semicolon-separated list of base64 encoded strings for start streamcuts, begin of the stream if not specified scan.end-streamcuts optional (none) List Semicolon-separated list of base64 encoded strings for end streamcuts, unbounded end if not specified scan.reader-group.max-outstanding-checkpoint-request optional 3 Integer Maximum outstanding checkpoint requests to Pravega scan.reader-group.refresh.interval optional 3 s Duration Refresh interval for reader group scan.event-read.timeout.interval optional 1 s Duration Timeout for the call to read events from Pravega scan.reader-group.checkpoint-initiate-timeout.interval optional 5 s Duration Timeout for call that initiates the Pravega checkpoint sink.stream required for sink (none) String Stream name to which the table is written sink.semantic optional at-least-once String Semantic when commit. Valid values are 'at-least-once', 'exactly-once', 'best-effort' sink.txn-lease-renewal.interval optional 30 s Duration Transaction lease renewal period, valid for exactly-once semantic. sink.enable.watermark-propagation optional false Boolean If watermark propagation should be enabled from Flink table to Pravega stream sink.routing-key.field.name optional (none) String Field name to use as a Pravega event routing key, field type must be STRING, random routing if not specified.","title":"Connector options"},{"location":"table-api/#features","text":"","title":"Features"},{"location":"table-api/#batch-and-streaming-read","text":"scan.execution.type can be specified as user's choice to perform batch read or streaming read. In the streaming environment, the table source uses a FlinkPravegaReader connector. In the batch environment, the table source uses a FlinkPravegaInputFormat connector. Please see the documentation of Streaming Connector and Batch Connector to have a better understanding on the below mentioned parameter list.","title":"Batch and Streaming read"},{"location":"table-api/#specify-start-and-end-streamcut","text":"A StreamCut represents a consistent position in the stream, and can be fetched from other applications uses Pravega client through checkpoints or custom defined index. scan.start-streamcuts and scan.end-streamcuts can be specified to perform bounded read and \"start-at-some-point\" read for Pravega streams. Pravega source supports read from multiple streams, and if read from multiple streams, please make sure the order of the streamcuts keeps the same as the order of the streams.","title":"Specify start and end streamcut"},{"location":"table-api/#changelog-source","text":"If messages in Pravega stream is change event captured from other databases using CDC tools, then you can use a CDC format to interpret messages as INSERT/UPDATE/DELETE messages into Flink SQL system. Flink provides two CDC formats debezium-json and canal-json to interpret change events captured by Debezium and Canal. The changelog source is a very useful feature in many cases, such as synchronizing incremental data from databases to other systems, auditing logs, materialized views on databases, temporal join changing history of a database table and so on. See more about how to use the CDC formats in debezium-json and canal-json","title":"Changelog Source"},{"location":"table-api/#routing-key-by-column","text":"Pravega writers can use domain specific meaningful Routing Keys (like customer ID, Timestamp, Machine ID, etc.) to group similar together and make such parallelism with segment scaling. Pravega makes ordering guarantees in terms of routing keys. Pravega sink supports event routing according to a certain event field by specifying sink.routing-key.field.name . This field type must be STRING , and it will be random routing if not specified.","title":"Routing key by column"},{"location":"table-api/#consistency-guarantees","text":"By default, a Pravega sink ingests data with at-least-once guarantees if the query is executed with checkpointing enabled. sink.semantic: exactly-once can be specified to turn on the transactional writes with exactly-once guarantees.","title":"Consistency guarantees"},{"location":"table-api/#useful-flink-links","text":"Users can try with Pravega table APIs quickly though Flink SQL client. Here is some tutorial to setup the environment. https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/sqlClient.html The usage and definition of time attribute and WATERMARK schema can be referred in: https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/streaming/time_attributes.html","title":"Useful Flink links"}]}