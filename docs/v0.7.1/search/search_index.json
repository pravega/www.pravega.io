{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Pravega Overview \u00b6 Pravega is a storage system that exposes Stream as the main primitive for continuous and unbounded data. A Pravega stream is a durable, elastic, append-only, unbounded sequence of bytes having good performance and strong consistency. Read Pravega Concepts for more details. Key Features \u00b6 Exactly-Once Semantics: Pravega ensures that each event is delivered and processed exactly once, with exact ordering guarantees, despite failures in clients, servers or the network. Auto Scaling: Pravega automatically changes the parallelism of the individual data streams to accommodate fluctuations in data ingestion rate. Distributed Computing Primitive: Pravega is not only a great storage service for data streams, it can also be used as a durable and consistent messaging service across processes. Pravega provides unique abstractions, which aids the users to build advanced services, like distributed consensus and leader election. Write Performance: Pravega shrinks write latency to milliseconds, and seamlessly scales to handle high throughput reads and writes from thousands of concurrent clients, making it ideal for IoT and other time sensitive applications. Unlimited Retention: Pravega decouples brokering of events from the actual data storage. This allows Pravega to transparently move data events from low-latency, durable storage tier to a cloud storage service (e.g., HDFS, Amazon S3, or DellEMC Isilon/ECS), while clients are agnostic to the actual location of data. Storage Efficiency: Pravega is used to build data processing pipelines that may combine batch and real-time applications without duplicating data for every step of the pipeline. This is possible because Pravega unifies stream (ordered) and batch (parallel) access to data events for data processing engines. Durability: Pravega persists and protects data events once the write operation is acknowledged to the client. Transaction Support: A Pravega Transaction ensures that a set of events are written to a stream atomically. This is a key feature for distributed streaming applications requiring exactly-once guarantees on their output. Security and pluggable role-based access control: Pravega can be deployed by the administrators securely by enabling TLS for communications and can deploy their own implementation of role-based access control plugin. Releases \u00b6 The latest Pravega releases can be found on the Github Release project page. Quick Start \u00b6 Read Getting Started page for more information, and also visit Pravega samples repository for more applications. Frequently Asked Questions \u00b6 You can find a list of frequently asked questions here . Running Pravega \u00b6 Pravega can be installed locally or in a distributed environment. The installation and deployment of Pravega is covered in the Running Pravega guide. Pravega Security, Role-based access control and TLS \u00b6 Pravega supports encryption of all communication channels and pluggable role-based access control. For more information please refer to the following: TLS Authorization, Authentication and RBAC Support \u00b6 Don\u2019t hesitate to ask! Contact the developers and community on the Slack or email at security@pravega.io if you need any help. Please open an issue in Github Issues if you find a bug. Contributing \u00b6 Become one of the contributors! We thrive to build a welcoming and open community for anyone who wants to use the system or contribute to it. Please check the Contributions Guidelines to quickly understand on how to contribute to Pravega? You can see the Roadmap document for more information. About \u00b6 Pravega is 100% open source and community-driven. All components are available under Apache 2 License on GitHub.","title":"Overview"},{"location":"#pravega-overview","text":"Pravega is a storage system that exposes Stream as the main primitive for continuous and unbounded data. A Pravega stream is a durable, elastic, append-only, unbounded sequence of bytes having good performance and strong consistency. Read Pravega Concepts for more details.","title":"Pravega Overview"},{"location":"#key-features","text":"Exactly-Once Semantics: Pravega ensures that each event is delivered and processed exactly once, with exact ordering guarantees, despite failures in clients, servers or the network. Auto Scaling: Pravega automatically changes the parallelism of the individual data streams to accommodate fluctuations in data ingestion rate. Distributed Computing Primitive: Pravega is not only a great storage service for data streams, it can also be used as a durable and consistent messaging service across processes. Pravega provides unique abstractions, which aids the users to build advanced services, like distributed consensus and leader election. Write Performance: Pravega shrinks write latency to milliseconds, and seamlessly scales to handle high throughput reads and writes from thousands of concurrent clients, making it ideal for IoT and other time sensitive applications. Unlimited Retention: Pravega decouples brokering of events from the actual data storage. This allows Pravega to transparently move data events from low-latency, durable storage tier to a cloud storage service (e.g., HDFS, Amazon S3, or DellEMC Isilon/ECS), while clients are agnostic to the actual location of data. Storage Efficiency: Pravega is used to build data processing pipelines that may combine batch and real-time applications without duplicating data for every step of the pipeline. This is possible because Pravega unifies stream (ordered) and batch (parallel) access to data events for data processing engines. Durability: Pravega persists and protects data events once the write operation is acknowledged to the client. Transaction Support: A Pravega Transaction ensures that a set of events are written to a stream atomically. This is a key feature for distributed streaming applications requiring exactly-once guarantees on their output. Security and pluggable role-based access control: Pravega can be deployed by the administrators securely by enabling TLS for communications and can deploy their own implementation of role-based access control plugin.","title":"Key Features"},{"location":"#releases","text":"The latest Pravega releases can be found on the Github Release project page.","title":"Releases"},{"location":"#quick-start","text":"Read Getting Started page for more information, and also visit Pravega samples repository for more applications.","title":"Quick Start"},{"location":"#frequently-asked-questions","text":"You can find a list of frequently asked questions here .","title":"Frequently Asked Questions"},{"location":"#running-pravega","text":"Pravega can be installed locally or in a distributed environment. The installation and deployment of Pravega is covered in the Running Pravega guide.","title":"Running Pravega"},{"location":"#pravega-security-role-based-access-control-and-tls","text":"Pravega supports encryption of all communication channels and pluggable role-based access control. For more information please refer to the following: TLS Authorization, Authentication and RBAC","title":"Pravega Security, Role-based access control and TLS"},{"location":"#support","text":"Don\u2019t hesitate to ask! Contact the developers and community on the Slack or email at security@pravega.io if you need any help. Please open an issue in Github Issues if you find a bug.","title":"Support"},{"location":"#contributing","text":"Become one of the contributors! We thrive to build a welcoming and open community for anyone who wants to use the system or contribute to it. Please check the Contributions Guidelines to quickly understand on how to contribute to Pravega? You can see the Roadmap document for more information.","title":"Contributing"},{"location":"#about","text":"Pravega is 100% open source and community-driven. All components are available under Apache 2 License on GitHub.","title":"About"},{"location":"basic-reader-and-writer/","text":"Working with Pravega: Basic Reader and Writer \u00b6 Lets examine how to build Pravega applications. The simplest kind of Pravega application uses a Reader to read from a Stream or a Writer that writes to a Stream. A simple sample application of both can be found in the Pravega samples repository ( HelloWorldReader and HelloWorldWriter ) applications. These samples give a sense on how a Java application could use the Pravega's Java Client Library to access Pravega functionality. Instructions for running the sample applications can be found in the Pravega Samples . Get familiar with the Pravega Concepts before executing the sample applications. HelloWorldWriter \u00b6 The HelloWorldWriter application demonstrates the usage of EventStreamWriter to write an Event to Pravega. The key part of HelloWorldWriter is in the run() method. The purpose of the run() method is to create a Stream and output the given Event to that Stream. public void run ( String routingKey , String message ) { StreamManager streamManager = StreamManager . create ( controllerURI ); final boolean scopeCreation = streamManager . createScope ( scope ); StreamConfiguration streamConfig = StreamConfiguration . builder () . scalingPolicy ( ScalingPolicy . fixed ( 1 )) . build (); final boolean streamCreation = streamManager . createStream ( scope , streamName , streamConfig ); try ( ClientFactory clientFactory = ClientFactory . withScope ( scope , controllerURI ); EventStreamWriter < String > writer = clientFactory . createEventWriter ( streamName , new JavaSerializer < String >(), EventWriterConfig . builder (). build ())) { System . out . format ( \"Writing message: '%s' with routing-key: '%s' to stream '%s / %s'%n\" , message , routingKey , scope , streamName ); final CompletableFuture < Void > writeFuture = writer . writeEvent ( routingKey , message ); } } Creating a Stream and the StreamManager Interface \u00b6 Scopes and Streams are created and manipulated via the StreamManager interface to the Pravega Controller. An URI to any of the Pravega Controller instance(s) in your cluster is required to create a StreamManager object. In the setup for the HelloWorld sample applications, the controllerURI is configured as a command line parameter when the sample application is launched. Note: For the \"standalone\" deployment of Pravega, the Controller is listening on localhost, port 9090. The StreamManager provides access to various control plane functions in Pravega related to Scopes and Streams: Method Parameters Discussion (static) create ( URI controller ) Given a URI to one of the Pravega Controller instances in the Pravega Cluster, create a Stream Manager object. createScope (String scopeName ) Creates a Scope with the given name. Returns True if the Scope is created, returns False if the Scope already exists. This method can be called even if the Stream is already existing. deleteScope (String scopeName ) Deletes a Scope with the given name. Returns True if the scope was deleted, returns False otherwise. If the Scope contains Streams, the deleteScope operation will fail with an exception. If we delete a non-existent Scope, the method will succeed and return False . createStream (String scopeName , String streamName , StreamConfiguration config ) Create a Stream within a given Scope. Both Scope name and Stream name are limited using the following characters: Letters (a-z A-Z), numbers (0-9) and delimiters: \".\" and \"-\" are allowed. The Scope must exist, an exception is thrown if we create a Stream in a non-existent Scope. A Stream Configuration is built using a builder pattern. Returns True if the Stream is created, returns False if the Stream already exists. This method can be called even if the Stream is already existing. updateStream (String scopeName , String streamName , StreamConfiguration config ) Swap out the Stream's configuration. The Stream must already exist, an exception is thrown if we update a non-existent Stream. Returns True if the Stream was changed. sealStream (String scopeName , String streamName ) Prevent any further writes to a Stream. The Stream must already exist, an exception is thrown if you seal a non-existent Stream. Returns True if the Stream is successfully sealed. deleteStream (String scopeName , String streamName ) Remove the Stream from Pravega and recover any resources used by that Stream. Returns False if the Stream is non-existent. Returns True if the Stream was deleted. The execution of API createScope(scope) establishes that the Scope exists. Then we can create the Stream using the API createStream(scope, streamName, streamConfig) . The StreamManager requires three parameters to create a Stream: Scope Name. Stream Name. Stream Configuration. The most interesting task is to create the Stream Configuration ( streamConfig ). Like many objects in Pravega, a Stream takes a configuration object that allows a developer to control various behaviors of the Stream. All configuration object instantiated via builder pattern. That allows a developer to control various aspects of a Stream's behavior in terms of policies ; Retention Policy and Scaling Policy are the most important ones related to Streams. For the sake of simplicity, in our sample application we instantiate a Stream with a single segment ( ScalingPolicy.fixed(1) ) and using the default (infinite) retention policy. Once the Stream Configuration ( streamConfig ) object is built, creating the Stream is straight forward using createStream() . After the Stream is created, we are all set to start writing Event(s) to the Stream. Writing an Event using EventWriter \u00b6 Applications use an EventStreamWriter object to write Events to a Stream. The EventStreamWriter is created using the ClientFactory object. The ClientFactory is used to create Readers, Writers and other types of Pravega Client objects such as the State Synchronizer (see Working with Pravega: State Synchronizer ). A ClientFactory is created in the context of a Scope, since all Readers, Writers and other Clients created by the ClientFactory are created in the context of that Scope. The ClientFactory also needs a URI to one of the Pravega Controllers ( ClientFactory.withScope(scope, controllerURI) ) , just like StreamManager . As the ClientFactory and the objects it creates consume resources from Pravega and implement AutoCloseable , it is a good practice to create these objects using a try-with-resources. By doing this, we make sure that, regardless of how the application ends, the Pravega resources will be properly closed in the right order. Once the ClientFactory is instantiated, we can use it to create a Writer. There are several things a developer needs to know before creating a Writer: What is the name of the Stream to write to? (The Scope has already been determined when the ClientFactory was created.) What Type of Event objects will be written to the Stream? What serializer will be used to convert an Event object to bytes? (Recall that Pravega only knows about sequences of bytes, it is unaware about Java objects.) Does the Writer need to be configured with any special behavior? EventStreamWriter < String > writer = clientFactory . createEventWriter ( streamName , new JavaSerializer < String >(), EventWriterConfig . builder (). build ())) The EventStreamWriter writes to the Stream specified in the configuration of the HelloWorldWriter sample application (by default the stream is named \"helloStream\" in the \"examples\" Scope). The Writer processes Java String objects as Events and uses the built in Java serializer for Strings. Note: Pravega allows users to write their own serializer. For more information and example, please refer to Pravega Serializer . The EventWriterConfig allows the developer to specify things like the number of attempts to retry a request before giving up and associated exponential back-off settings. Pravega takes care to retry requests in the presence of connection failures or Pravega component outages, which may temporarily prevent a request from succeeding, so application logic doesn't need to be complicated by dealing with intermittent cluster failures. In the sample application, EventWriterConfig was considered as the default settings. EventStreamWriter provides a writeEvent() operation that writes the given non-null Event object to the Stream using a given Routing key to determine which Stream Segment it should written to. Many operations in Pravega, such as writeEvent() , are asynchronous and return some sort of Future object. If the application needed to make sure the Event was durably written to Pravega and available for Readers, it could wait on the Future before proceeding. In the case of Pravega's HelloWorld example, it does wait on the Future . EventStreamWriter can also be used to begin a Transaction. We cover Transactions in more detail in Working with Pravega: Transactions . HelloWorldReader \u00b6 The HelloWorldReader is a simple demonstration of using the EventStreamReader . The application reads Events from the given Stream and prints a string representation of those Events onto the console. Just like the HelloWorldWriter example, the key part of the HelloWorldReader application is in the run() method: public void run () { StreamManager streamManager = StreamManager . create ( controllerURI ); final boolean scopeIsNew = streamManager . createScope ( scope ); StreamConfiguration streamConfig = StreamConfiguration . builder () . scalingPolicy ( ScalingPolicy . fixed ( 1 )) . build (); final boolean streamIsNew = streamManager . createStream ( scope , streamName , streamConfig ); final String readerGroup = UUID . randomUUID (). toString (). replace ( \"-\" , \"\" ); final ReaderGroupConfig readerGroupConfig = ReaderGroupConfig . builder () . stream ( Stream . of ( scope , streamName )) . build (); try ( ReaderGroupManager readerGroupManager = ReaderGroupManager . withScope ( scope , controllerURI )) { readerGroupManager . createReaderGroup ( readerGroup , readerGroupConfig ); } try ( ClientFactory clientFactory = ClientFactory . withScope ( scope , controllerURI ); EventStreamReader < String > reader = clientFactory . createReader ( \"reader\" , readerGroup , new JavaSerializer < String >(), ReaderConfig . builder (). build ())) { System . out . format ( \"Reading all the events from %s/%s%n\" , scope , streamName ); EventRead < String > event = null ; do { try { event = reader . readNextEvent ( READER_TIMEOUT_MS ); if ( event . getEvent () != null ) { System . out . format ( \"Read event '%s'%n\" , event . getEvent ()); } } catch ( ReinitializationRequiredException e ) { //There are certain circumstances where the reader needs to be reinitialized e . printStackTrace (); } } while ( event . getEvent () != null ); System . out . format ( \"No more events from %s/%s%n\" , scope , streamName ); } The API streamManager.createScope() and streamManager.createStream() set up the Scope and Stream just like in the HelloWorldWriter application. The API ReaderGroupConfig set up the Reader Group as the prerequisite to creating the EventStreamReader and using it to read Events from the Stream ( createReader() , reader.readNextEvent() ). Reader Group Basics \u00b6 Any Reader in Pravega belongs to some ReaderGroup . A ReaderGroup is a grouping of one or more Readers that consume from a Stream in parallel. Before we create a Reader, we need to either create a ReaderGroup (or be aware of the name of an existing ReaderGroup ). This application only uses the basics from Reader Group. ReaderGroup objects are created from a ReaderGroupManager object. The ReaderGroupManager object, in turn, is created on a given Scope with a URI to one of the Pravega Controllers, very much like a ClientFactory is created. Note that, the createReaderGroup is also in a try-with-resources statement to make sure that the ReaderGroupManager is properly cleaned up. The ReaderGroupManager allows a developer to create, delete and retrieve ReaderGroup objects using the name. To create a ReaderGroup , the developer needs a name for the Reader Group and a configuration with a set of one or more Streams to read from. The Reader Group's name (alphanumeric) might be meaningful to the application, like \"WebClickStreamReaders\". In cases where we require multiple Readers reading in parallel and each Reader in a separate process, it is helpful to have a human readable name for the Reader Group. In this example, we have one Reader, reading in isolation, so a UUID is a safe way to name the Reader Group. The Reader Group is created via the ReaderGroupManager and since the ReaderGroupManager is created within the context of a Scope, we can safely conclude that Reader Group names are namespaced by that Scope. The developer specifies the Stream which should be the part of the Reader Group and its lower and upper bounds. In the sample application, we start at the beginning of the Stream as follows: final ReaderGroupConfig readerGroupConfig = ReaderGroupConfig . builder () . stream ( Stream . of ( scope , streamName )) . build (); Other configuration items, such as Checkpointing are options that will be available through the ReaderGroupConfig . The Reader Group can be configured to read from multiple Streams. For example, imagine a situation where there is a collection of Stream of sensor data coming from a factory floor, each machine has its own Stream of sensor data. We can build applications that uses a Reader Group per Stream so that the application reasons about data from exactly one machine. We can build other applications that use a Reader Group configured to read from all of the Streams. To keep it simple, in the sample application the Reader Group only reads from one Stream. We can call createReaderGroup with the same parameters multiple times and the same Reader Group will be returned each time after it is initially created (idempotent operation). Note that in other cases, if the developer knows the name of the Reader Group to use and knows it has already been created, they can use getReaderGroup() on ReaderGroupManager to retrieve the ReaderGroup object by name. At this point, we have the Scope and Stream is set up and the ReaderGroup object created. Next, we need to create a Reader and start reading Events. Reading Event using an EventStreamReader \u00b6 First, we create a ClientFactory object, the same way we did it in the HelloWorldWriter application. Then we use the ClientFactory to create an EventStreamReader object. The following are the four parameters to create a Reader: Name for the Reader. Reader Group it should be part of. The type of object expected on the Stream. Serializer to convert from the bytes stored in Pravega into the Event objects and a ReaderConfig . EventStreamReader < String > reader = clientFactory . createReader ( \"reader\" , readerGroup , new JavaSerializer < String >(), ReaderConfig . builder (). build ())) The name of the Reader can be any valid Pravega naming convention (numbers and letters). Note that the name of the Reader is namespaced within the Scope. EventStreamWriter and EventStreamReader uses Java generic types to allow a developer to specify a type safe Reader. In the sample application, we read Strings from the stream and use the standard Java String Serializer to convert the bytes read from the stream into String objects. Note: Pravega allows users to write their own serializer. For more information and example, please refer to Pravega Serializer . Finally, we use a ReaderConfig object with default values. Note that you cannot create the same Reader multiple times. That is, an application may call createReader() to add new Readers to the Reader Group. But if the Reader Group already contains a Reader with that name, an exception is thrown. After creating an EventStreamReader , we can use it to read Events from the Stream. The readNextEvent() operation returns the next Event available on the Stream, or if there is no such Event, blocks for a specified time. After the expiry of the timeout period, if no Event is available for reading, then Null is returned. The null check ( EventRead<String> event = null ) is used to avoid printing out a spurious Null event message to the console and also used to terminate the loop. Note that the Event itself is wrapped in an EventRead object. It is worth noting that readNextEvent() may throw an exception ReinitializationRequiredException and the object is reinitialized. This exception would be handled in cases where the Readers in the Reader Group need to reset to a Checkpoint or the Reader Group itself has been altered and the set of Streams being read has been therefore changed. TruncatedDataException is thrown when we try to read the deleted data. It is however possible to recover from the later by calling readNextEvent() again (it will just skip forward). Thus, the simple HelloWorldReader loops, reading Events from a Stream until there are no more Events, and then the application terminates. Experimental Batch Reader \u00b6 BatchClient is used for applications that require parallel, unordered reads of historical stream data. Using the Batch Reader all the segments in a Stream can be listed and read from. Hence, the Events for a given Routing Key which can reside on multiple segments are not read in order. Obviously this API is not for every application, the main advantage is that it allows for low level integration with batch processing frameworks such as MapReduce . Example \u00b6 To iterate over all the segments in the stream: //Passing null to fromStreamCut and toStreamCut will result in using the current start of stream and the current end of stream respectively. Iterator < SegmentRange > segments = client . listSegments ( stream , null , null ). getIterator (); SegmentRange segmentInfo = segments . next (); To read the events from a segment: SegmentIterator < T > events = client . readSegment ( segmentInfo , deserializer ); while ( events . hasNext ()) { processEvent ( events . next ()); }","title":"Working with Reader and Writer"},{"location":"basic-reader-and-writer/#working-with-pravega-basic-reader-and-writer","text":"Lets examine how to build Pravega applications. The simplest kind of Pravega application uses a Reader to read from a Stream or a Writer that writes to a Stream. A simple sample application of both can be found in the Pravega samples repository ( HelloWorldReader and HelloWorldWriter ) applications. These samples give a sense on how a Java application could use the Pravega's Java Client Library to access Pravega functionality. Instructions for running the sample applications can be found in the Pravega Samples . Get familiar with the Pravega Concepts before executing the sample applications.","title":"Working with Pravega: Basic Reader and Writer"},{"location":"basic-reader-and-writer/#helloworldwriter","text":"The HelloWorldWriter application demonstrates the usage of EventStreamWriter to write an Event to Pravega. The key part of HelloWorldWriter is in the run() method. The purpose of the run() method is to create a Stream and output the given Event to that Stream. public void run ( String routingKey , String message ) { StreamManager streamManager = StreamManager . create ( controllerURI ); final boolean scopeCreation = streamManager . createScope ( scope ); StreamConfiguration streamConfig = StreamConfiguration . builder () . scalingPolicy ( ScalingPolicy . fixed ( 1 )) . build (); final boolean streamCreation = streamManager . createStream ( scope , streamName , streamConfig ); try ( ClientFactory clientFactory = ClientFactory . withScope ( scope , controllerURI ); EventStreamWriter < String > writer = clientFactory . createEventWriter ( streamName , new JavaSerializer < String >(), EventWriterConfig . builder (). build ())) { System . out . format ( \"Writing message: '%s' with routing-key: '%s' to stream '%s / %s'%n\" , message , routingKey , scope , streamName ); final CompletableFuture < Void > writeFuture = writer . writeEvent ( routingKey , message ); } }","title":"HelloWorldWriter"},{"location":"basic-reader-and-writer/#creating-a-stream-and-the-streammanager-interface","text":"Scopes and Streams are created and manipulated via the StreamManager interface to the Pravega Controller. An URI to any of the Pravega Controller instance(s) in your cluster is required to create a StreamManager object. In the setup for the HelloWorld sample applications, the controllerURI is configured as a command line parameter when the sample application is launched. Note: For the \"standalone\" deployment of Pravega, the Controller is listening on localhost, port 9090. The StreamManager provides access to various control plane functions in Pravega related to Scopes and Streams: Method Parameters Discussion (static) create ( URI controller ) Given a URI to one of the Pravega Controller instances in the Pravega Cluster, create a Stream Manager object. createScope (String scopeName ) Creates a Scope with the given name. Returns True if the Scope is created, returns False if the Scope already exists. This method can be called even if the Stream is already existing. deleteScope (String scopeName ) Deletes a Scope with the given name. Returns True if the scope was deleted, returns False otherwise. If the Scope contains Streams, the deleteScope operation will fail with an exception. If we delete a non-existent Scope, the method will succeed and return False . createStream (String scopeName , String streamName , StreamConfiguration config ) Create a Stream within a given Scope. Both Scope name and Stream name are limited using the following characters: Letters (a-z A-Z), numbers (0-9) and delimiters: \".\" and \"-\" are allowed. The Scope must exist, an exception is thrown if we create a Stream in a non-existent Scope. A Stream Configuration is built using a builder pattern. Returns True if the Stream is created, returns False if the Stream already exists. This method can be called even if the Stream is already existing. updateStream (String scopeName , String streamName , StreamConfiguration config ) Swap out the Stream's configuration. The Stream must already exist, an exception is thrown if we update a non-existent Stream. Returns True if the Stream was changed. sealStream (String scopeName , String streamName ) Prevent any further writes to a Stream. The Stream must already exist, an exception is thrown if you seal a non-existent Stream. Returns True if the Stream is successfully sealed. deleteStream (String scopeName , String streamName ) Remove the Stream from Pravega and recover any resources used by that Stream. Returns False if the Stream is non-existent. Returns True if the Stream was deleted. The execution of API createScope(scope) establishes that the Scope exists. Then we can create the Stream using the API createStream(scope, streamName, streamConfig) . The StreamManager requires three parameters to create a Stream: Scope Name. Stream Name. Stream Configuration. The most interesting task is to create the Stream Configuration ( streamConfig ). Like many objects in Pravega, a Stream takes a configuration object that allows a developer to control various behaviors of the Stream. All configuration object instantiated via builder pattern. That allows a developer to control various aspects of a Stream's behavior in terms of policies ; Retention Policy and Scaling Policy are the most important ones related to Streams. For the sake of simplicity, in our sample application we instantiate a Stream with a single segment ( ScalingPolicy.fixed(1) ) and using the default (infinite) retention policy. Once the Stream Configuration ( streamConfig ) object is built, creating the Stream is straight forward using createStream() . After the Stream is created, we are all set to start writing Event(s) to the Stream.","title":"Creating a Stream and the StreamManager Interface"},{"location":"basic-reader-and-writer/#writing-an-event-using-eventwriter","text":"Applications use an EventStreamWriter object to write Events to a Stream. The EventStreamWriter is created using the ClientFactory object. The ClientFactory is used to create Readers, Writers and other types of Pravega Client objects such as the State Synchronizer (see Working with Pravega: State Synchronizer ). A ClientFactory is created in the context of a Scope, since all Readers, Writers and other Clients created by the ClientFactory are created in the context of that Scope. The ClientFactory also needs a URI to one of the Pravega Controllers ( ClientFactory.withScope(scope, controllerURI) ) , just like StreamManager . As the ClientFactory and the objects it creates consume resources from Pravega and implement AutoCloseable , it is a good practice to create these objects using a try-with-resources. By doing this, we make sure that, regardless of how the application ends, the Pravega resources will be properly closed in the right order. Once the ClientFactory is instantiated, we can use it to create a Writer. There are several things a developer needs to know before creating a Writer: What is the name of the Stream to write to? (The Scope has already been determined when the ClientFactory was created.) What Type of Event objects will be written to the Stream? What serializer will be used to convert an Event object to bytes? (Recall that Pravega only knows about sequences of bytes, it is unaware about Java objects.) Does the Writer need to be configured with any special behavior? EventStreamWriter < String > writer = clientFactory . createEventWriter ( streamName , new JavaSerializer < String >(), EventWriterConfig . builder (). build ())) The EventStreamWriter writes to the Stream specified in the configuration of the HelloWorldWriter sample application (by default the stream is named \"helloStream\" in the \"examples\" Scope). The Writer processes Java String objects as Events and uses the built in Java serializer for Strings. Note: Pravega allows users to write their own serializer. For more information and example, please refer to Pravega Serializer . The EventWriterConfig allows the developer to specify things like the number of attempts to retry a request before giving up and associated exponential back-off settings. Pravega takes care to retry requests in the presence of connection failures or Pravega component outages, which may temporarily prevent a request from succeeding, so application logic doesn't need to be complicated by dealing with intermittent cluster failures. In the sample application, EventWriterConfig was considered as the default settings. EventStreamWriter provides a writeEvent() operation that writes the given non-null Event object to the Stream using a given Routing key to determine which Stream Segment it should written to. Many operations in Pravega, such as writeEvent() , are asynchronous and return some sort of Future object. If the application needed to make sure the Event was durably written to Pravega and available for Readers, it could wait on the Future before proceeding. In the case of Pravega's HelloWorld example, it does wait on the Future . EventStreamWriter can also be used to begin a Transaction. We cover Transactions in more detail in Working with Pravega: Transactions .","title":"Writing an Event using EventWriter"},{"location":"basic-reader-and-writer/#helloworldreader","text":"The HelloWorldReader is a simple demonstration of using the EventStreamReader . The application reads Events from the given Stream and prints a string representation of those Events onto the console. Just like the HelloWorldWriter example, the key part of the HelloWorldReader application is in the run() method: public void run () { StreamManager streamManager = StreamManager . create ( controllerURI ); final boolean scopeIsNew = streamManager . createScope ( scope ); StreamConfiguration streamConfig = StreamConfiguration . builder () . scalingPolicy ( ScalingPolicy . fixed ( 1 )) . build (); final boolean streamIsNew = streamManager . createStream ( scope , streamName , streamConfig ); final String readerGroup = UUID . randomUUID (). toString (). replace ( \"-\" , \"\" ); final ReaderGroupConfig readerGroupConfig = ReaderGroupConfig . builder () . stream ( Stream . of ( scope , streamName )) . build (); try ( ReaderGroupManager readerGroupManager = ReaderGroupManager . withScope ( scope , controllerURI )) { readerGroupManager . createReaderGroup ( readerGroup , readerGroupConfig ); } try ( ClientFactory clientFactory = ClientFactory . withScope ( scope , controllerURI ); EventStreamReader < String > reader = clientFactory . createReader ( \"reader\" , readerGroup , new JavaSerializer < String >(), ReaderConfig . builder (). build ())) { System . out . format ( \"Reading all the events from %s/%s%n\" , scope , streamName ); EventRead < String > event = null ; do { try { event = reader . readNextEvent ( READER_TIMEOUT_MS ); if ( event . getEvent () != null ) { System . out . format ( \"Read event '%s'%n\" , event . getEvent ()); } } catch ( ReinitializationRequiredException e ) { //There are certain circumstances where the reader needs to be reinitialized e . printStackTrace (); } } while ( event . getEvent () != null ); System . out . format ( \"No more events from %s/%s%n\" , scope , streamName ); } The API streamManager.createScope() and streamManager.createStream() set up the Scope and Stream just like in the HelloWorldWriter application. The API ReaderGroupConfig set up the Reader Group as the prerequisite to creating the EventStreamReader and using it to read Events from the Stream ( createReader() , reader.readNextEvent() ).","title":"HelloWorldReader"},{"location":"basic-reader-and-writer/#reader-group-basics","text":"Any Reader in Pravega belongs to some ReaderGroup . A ReaderGroup is a grouping of one or more Readers that consume from a Stream in parallel. Before we create a Reader, we need to either create a ReaderGroup (or be aware of the name of an existing ReaderGroup ). This application only uses the basics from Reader Group. ReaderGroup objects are created from a ReaderGroupManager object. The ReaderGroupManager object, in turn, is created on a given Scope with a URI to one of the Pravega Controllers, very much like a ClientFactory is created. Note that, the createReaderGroup is also in a try-with-resources statement to make sure that the ReaderGroupManager is properly cleaned up. The ReaderGroupManager allows a developer to create, delete and retrieve ReaderGroup objects using the name. To create a ReaderGroup , the developer needs a name for the Reader Group and a configuration with a set of one or more Streams to read from. The Reader Group's name (alphanumeric) might be meaningful to the application, like \"WebClickStreamReaders\". In cases where we require multiple Readers reading in parallel and each Reader in a separate process, it is helpful to have a human readable name for the Reader Group. In this example, we have one Reader, reading in isolation, so a UUID is a safe way to name the Reader Group. The Reader Group is created via the ReaderGroupManager and since the ReaderGroupManager is created within the context of a Scope, we can safely conclude that Reader Group names are namespaced by that Scope. The developer specifies the Stream which should be the part of the Reader Group and its lower and upper bounds. In the sample application, we start at the beginning of the Stream as follows: final ReaderGroupConfig readerGroupConfig = ReaderGroupConfig . builder () . stream ( Stream . of ( scope , streamName )) . build (); Other configuration items, such as Checkpointing are options that will be available through the ReaderGroupConfig . The Reader Group can be configured to read from multiple Streams. For example, imagine a situation where there is a collection of Stream of sensor data coming from a factory floor, each machine has its own Stream of sensor data. We can build applications that uses a Reader Group per Stream so that the application reasons about data from exactly one machine. We can build other applications that use a Reader Group configured to read from all of the Streams. To keep it simple, in the sample application the Reader Group only reads from one Stream. We can call createReaderGroup with the same parameters multiple times and the same Reader Group will be returned each time after it is initially created (idempotent operation). Note that in other cases, if the developer knows the name of the Reader Group to use and knows it has already been created, they can use getReaderGroup() on ReaderGroupManager to retrieve the ReaderGroup object by name. At this point, we have the Scope and Stream is set up and the ReaderGroup object created. Next, we need to create a Reader and start reading Events.","title":"Reader Group Basics"},{"location":"basic-reader-and-writer/#reading-event-using-an-eventstreamreader","text":"First, we create a ClientFactory object, the same way we did it in the HelloWorldWriter application. Then we use the ClientFactory to create an EventStreamReader object. The following are the four parameters to create a Reader: Name for the Reader. Reader Group it should be part of. The type of object expected on the Stream. Serializer to convert from the bytes stored in Pravega into the Event objects and a ReaderConfig . EventStreamReader < String > reader = clientFactory . createReader ( \"reader\" , readerGroup , new JavaSerializer < String >(), ReaderConfig . builder (). build ())) The name of the Reader can be any valid Pravega naming convention (numbers and letters). Note that the name of the Reader is namespaced within the Scope. EventStreamWriter and EventStreamReader uses Java generic types to allow a developer to specify a type safe Reader. In the sample application, we read Strings from the stream and use the standard Java String Serializer to convert the bytes read from the stream into String objects. Note: Pravega allows users to write their own serializer. For more information and example, please refer to Pravega Serializer . Finally, we use a ReaderConfig object with default values. Note that you cannot create the same Reader multiple times. That is, an application may call createReader() to add new Readers to the Reader Group. But if the Reader Group already contains a Reader with that name, an exception is thrown. After creating an EventStreamReader , we can use it to read Events from the Stream. The readNextEvent() operation returns the next Event available on the Stream, or if there is no such Event, blocks for a specified time. After the expiry of the timeout period, if no Event is available for reading, then Null is returned. The null check ( EventRead<String> event = null ) is used to avoid printing out a spurious Null event message to the console and also used to terminate the loop. Note that the Event itself is wrapped in an EventRead object. It is worth noting that readNextEvent() may throw an exception ReinitializationRequiredException and the object is reinitialized. This exception would be handled in cases where the Readers in the Reader Group need to reset to a Checkpoint or the Reader Group itself has been altered and the set of Streams being read has been therefore changed. TruncatedDataException is thrown when we try to read the deleted data. It is however possible to recover from the later by calling readNextEvent() again (it will just skip forward). Thus, the simple HelloWorldReader loops, reading Events from a Stream until there are no more Events, and then the application terminates.","title":"Reading Event using an EventStreamReader"},{"location":"basic-reader-and-writer/#experimental-batch-reader","text":"BatchClient is used for applications that require parallel, unordered reads of historical stream data. Using the Batch Reader all the segments in a Stream can be listed and read from. Hence, the Events for a given Routing Key which can reside on multiple segments are not read in order. Obviously this API is not for every application, the main advantage is that it allows for low level integration with batch processing frameworks such as MapReduce .","title":"Experimental Batch Reader"},{"location":"basic-reader-and-writer/#example","text":"To iterate over all the segments in the stream: //Passing null to fromStreamCut and toStreamCut will result in using the current start of stream and the current end of stream respectively. Iterator < SegmentRange > segments = client . listSegments ( stream , null , null ). getIterator (); SegmentRange segmentInfo = segments . next (); To read the events from a segment: SegmentIterator < T > events = client . readSegment ( segmentInfo , deserializer ); while ( events . hasNext ()) { processEvent ( events . next ()); }","title":"Example"},{"location":"connectors/","text":"Pravega Connectors \u00b6 Connectors allow integrating Pravega with different data sources and sinks. Flink Connector The initial connector supported is Flink which enables building end-to-end stream processing pipelines with Pravega. This also allows reading and writing data to external data sources and sinks via Flink Streaming Connectors. In Experimental Stage \u00b6 Logstash Hadoop Connector","title":"Pravega Connectors"},{"location":"connectors/#pravega-connectors","text":"Connectors allow integrating Pravega with different data sources and sinks. Flink Connector The initial connector supported is Flink which enables building end-to-end stream processing pipelines with Pravega. This also allows reading and writing data to external data sources and sinks via Flink Streaming Connectors.","title":"Pravega Connectors"},{"location":"connectors/#in-experimental-stage","text":"Logstash Hadoop Connector","title":"In Experimental Stage"},{"location":"contributing/","text":"Contributing to Pravega \u00b6 Contributions guidelines Issue triaging and labeling Review process Happy hacking!","title":"Coding guildelines"},{"location":"contributing/#contributing-to-pravega","text":"Contributions guidelines Issue triaging and labeling Review process Happy hacking!","title":"Contributing to Pravega"},{"location":"controller-service/","text":"Pravega Controller Service \u00b6 Introduction Architecture Stream Management Cluster Management System Diagram Components Service Endpoints Controller Service Stream Metadata Store Stream Metadata Stream Store Caching Stream Buckets Controller Cluster Listener Host Store Background workers Roles and Responsibilities Stream Operations Stream State Create Stream Update Stream Scale Stream Truncate Stream Seal Stream Delete Stream Stream Policy Manager Scaling Infrastructure Retention Infrastructure Transaction Manager Create Transaction Commit Transaction Abort Transaction Ping Transaction Transaction Timeout Management Segment Container to Host Mapping Resources Introduction \u00b6 The Controller Service is a core component of Pravega that implements the control plane. It acts as the central coordinator and manager for various operations performed in the cluster, the major two categories are: Stream Management and Cluster Management . The Controller Service, referred to simply as Controller henceforth, is responsible for providing the abstraction of a Pravega Stream , which is the main abstraction that Pravega exposes to applications. A Pravega Stream comprises one or more Stream Segments . Each Stream Segment is an append-only data structure that stores a sequence of bytes. A Stream Segment on its own is agnostic to the presence of other Stream Segments and is not aware of its logical relationship with its peer Stream Segments. The Segment Store, which owns and manages these Stream Segments, does not have any notion of a Stream. A Stream is a logical view built by the Controller and consists of a dynamically changing set of Stream Segments that satisfy a predefined set of invariants. The Controller provides the Stream abstraction and orchestrates all lifecycle operations on a Pravega Stream while ensuring its consistency. The Controller plays a central role in the lifecycle of a Stream: creation , updation , truncation , sealing , scaling and deletion . To implement these operations, the Controller manages both a Stream's metadata and its associated Stream Segments. For example, as part of Stream\u2019s lifecycle, new segments can be created and existing segments can be sealed. The Controller decides on performing these operations by ensuring the availability and consistency of the Streams for the clients accessing them. Architecture \u00b6 The Controller Service is made up of one or more instances of stateless worker nodes. Each new Controller instance can be invoked independently and becomes part of Pravega cluster by merely pointing to the same Apache Zookeeper . For high availability, it is advised to have more than one instance of Controller service per cluster. Each Controller instance is capable of working independently and uses a shared persistent store as the source of truth for all state-owned and managed by Controller service. We currently use Apache ZooKeeper as the store for persisting all metadata consistently. Each instance comprises various subsystems which are responsible for performing specific operations on different categories of metadata. These subsystems include different API endpoints, metadata store handles, policy managers and background workers . The Controller exposes two endpoints which can be used to interact with The Pravega Controller Service. The first port is for providing programmatic access for Pravega clients and is implemented as an RPC using gRPC . The other endpoint is for administrative operations and is implemented as a REST endpoint. Stream Management \u00b6 The Controller owns and manages the concept of Stream and is responsible for maintaining \"metadata\" and \"lifecycle\" operations ( creating, updating, scaling, truncating, sealing and deleting Streams ) for each Pravega Stream. The Stream management can be divided into the following three categories: Stream Abstraction : A Stream can be viewed as a series of dynamically changing segment sets where the Stream transitions from one set of consistent segments to the next. The Controller is the place for creating and managing Stream abstraction. The Controller decides when and how a Stream transitions from one state to another and is responsible for performing these transitions by keeping the state of the Stream consistent and available. These transitions are governed user-defined policies that the Controller enforces. Consequently, as part of Stream management, the Controller also performs roles of Policy Manager for policies like retention and scaling. Policy Management : The Controller is responsible for storing and enforcing user-defined Stream policies by actively monitoring the state of the Stream. In Pravega we have two policies that users can define, namely Scaling Policy and Retention Policy . Scaling policy describes if and under what circumstances a Stream should automatically scale its number of segments. Retention policy describes a policy about how much data to retain within a Stream based on time ( Time-based Retention ) and data size ( Size-based Retention ). Transaction Management : Implementing Transactions requires the manipulation of Stream Segments. With each Transaction, Pravega creates a set of Transaction segments, which are later merged onto the Stream Segments upon commit or discarded upon aborts. The Controller performs the role of Transaction manager and is responsible for creating and committing Transactions on a given Stream. Upon creating Transactions, Controller also tracks Transaction timeouts and aborts transactions whose timeouts have elapsed. Details of Transaction management can be found later in the Transactions section. Cluster Management \u00b6 The Controller is responsible for managing the Segment Store cluster. Controller manages the lifecycle of Segment Store nodes as they are added to/removed from the cluster and performs redistribution of segment containers across available Segment Store nodes. System Diagram \u00b6 The following diagram shows the main components of a Controller process. The elements of the diagram are discussed in detail in the following sections. Controller Process Diagram Components \u00b6 Service Endpoints \u00b6 There are two ports exposed by Controller: Client-Controller API and Administration API . The client Controller communication is implemented as RPC which exposes API to perform all Stream related control plane operations. Apart from this Controller also exposes an administrative API implemented as REST . Each endpoint performs the appropriate call to the Pravega Controller Service backend subsystem which has the actual implementation for various operations like create, read, update and delete (CRUD) on entities owned and managed by Controller. gRPC \u00b6 Client Controller communication endpoint is implemented as a gRPC interface. Please check the complete list of API . This exposes API used by Pravega clients (Readers, Writers and Stream Manager) and enables Stream management. Requests enabled by this API include creating, modifying, and deleting Streams. The underlying gRPC framework provides both synchronous and asynchronous programming models. We use the asynchronous model in our client Controller interactions so that the client thread does not block on the response from the server. To be able to append to and read data from Streams, Writers and Readers query Controller to get active Stream Segment sets, successor and predecessor Stream Segments while working with a Stream. For Transactions, the client uses specific API calls to request Controller to create , commit , abort and ping Transactions. REST \u00b6 For administration, the Controller implements and exposes a REST interface. This includes API calls for Stream management as well as other administration API primarily dealing with creation and deletion of Scopes . We use swagger to describe our REST API. Please see, the swagger yaml file. Pravega Controller Service \u00b6 This is the backend layer behind the Controller endpoints gRPC and REST . All the business logic required to serve Controller API calls are implemented here. This layer contains handles to all other subsystems like the various store implementations ( Stream store , Host store and Checkpoint store) and background processing frameworks ( Task and Event Processor framework ). Stores are interfaces that provide access to various types of metadata managed by Controller. Background processing frameworks are used to perform asynchronous processing that typically implements workflows involving metadata updates and requests to Segment Store. Stream Metadata Store \u00b6 A Stream is a dynamically changing sequence of Stream Segments, where regions of the Routing Key space map to open Stream Segments. As the set of Segments of a Stream changes, so do the mapping of the Routing Key space to Segments. A set of Segments is consistent iff the union of key space ranges mapping to Segments in the set covers the entire key space and the key space ranges are disjoint. For example, suppose a set S = { S 1 , S 2 , S 3 }, such that: Region [0, 0.3) maps to Segment S 1 . Region [0.3, 0.6) maps to Segment S 2 . Region [0.6, 1.0) maps to Segment S 3 . S is a consistent Segment set. A Stream goes through transformations as it scales over time. A Stream starts with an initial set of Segments that is determined by the Stream configuration when created and it transitions to new sets of Segments as scale operations are performed on the Stream. Each generation of Segments that constitute the Stream at any given point in time are considered to belong to an epoch . A Stream starts with initial epoch 0 and upon each transition, it moves ahead in its epochs to describe the change in generation of Segments in the Stream. The Controller maintains the Stream: it stores the information about all epochs that constitute a given Stream and also about their transitions. The metadata store is designed to persist the information pertaining to Stream Segments, and to enable queries over this information. Apart from the epoch information, it keeps some additional metadata, such as state and its policies and ongoing Transactions on the Stream. Various sub-components of Controller access the stored metadata for each Stream via a well-defined interface . We currently have two concrete implementations of the Stream store interface: in-memory and Zookeeper backed stores. Both share a common base implementation that relies on Stream objects for providing store-type specific implementations for all Stream-specific metadata. The base implementation of Stream store creates and caches these Stream objects. The Stream objects implement a store/Stream interface. The concrete Stream implementation is specific to the store type and is responsible for implementation of store specific methods. We have a common base implementation of all store types that provide optimistic concurrency. This base class encapsulates the logic for queries against Stream store for all concrete stores that support Compare And Swap (CAS). We currently use Zookeeper as our underlying store which also supports CAS. We store all Stream metadata in a hierarchical fashion under Stream specific znodes (ZooKeeper data nodes). For the ZooKeeper-based store, we structure our metadata into different groups to support a variety of queries against this metadata. All Stream specific metadata is stored under a scoped/Stream root node. Queries against this metadata include, but not limited to, querying segment sets that form the Stream at different points in time, segment specific information, segment predecessors and successors. Refer to Stream metadata interface for details about API exposed by Stream metadata store. Stream Metadata \u00b6 Clients need information about what Segments constitute a Stream to start their processing and they obtain it from the epoch information the Controller stores in the stream store. Clients need the ability to query and find Stream Segments at any of the three cases efficiently: A Reader client typically starts from the head of the Stream, But it might also choose to access the Stream starting from any arbitrarily interesting position. Writers on the other hand always append to the tail of the Stream. To enable such queries, the Stream store provides API calls to get the initial set of Stream Segments, Segments at a specific time and current set of Segments. As mentioned earlier, a Stream can transition from one set of Stream Segments (epoch) to another set of Segments that constitute the Stream. A Stream moves from one epoch to another if there is at least one Stream Segment that is sealed and replaced by one or more set of Stream Segments that cover precisely the key space of the sealed Segments. As clients work on Streams, they may encounter the end of sealed Stream Segments and consequently need to find new Segments to be able to move forward. To enable the clients to query for the next Segments, the stream store exposes via the Controller Service efficient queries for finding immediate successors and predecessors for any arbitrary Segment. To enable serving queries like those mentioned above, we need to efficiently store a time series of these Segment transitions and index them against time. We store this information about the current and historical state of a Stream Segments in a set of records which are designed to optimize on aforementioned queries. Apart from Segment-specific metadata record, the current state of Stream comprises of other metadata types that are described henceforth. Records \u00b6 Stream time series is stored as a series of records where each record corresponds to an epoch. As Stream scales and transitions from one epoch to another, a new record is created that has complete information about Stream Segments that forms the epoch. Epoch Records: Epoch: \u27e8time, list-of-segments-in-epoch\u27e9 . We store the series of active Stream Segments as they transition from one epoch to another into individual epoch records. Each epoch record corresponds to an epoch which captures a logically consistent (as defined earlier) set of Stream Segments that form the Stream and are valid through the lifespan of the epoch. The epoch record is stored against the epoch number. This record is optimized to answer to query Segments from an epoch with a single call into the store that also enables retrieval of all Stream Segment records in the epoch in O(1) . This record is also used for fetching a Segment-specific record by first computing Stream Segment's creation epoch from Stream Segment ID and then retrieving the epoch record. Current Epoch: A special epoch record called currentEpoch . This is the currently active epoch in the Stream. At any time exactly one epoch is marked as the current epoch. Typically this is the latest epoch with the highest epoch number. However, during an ongoing Stream update workflow like scale or rolling Transaction , the current epoch may not necessarily be the latest epoch. However, at the completion of these workflows, the current epoch is marked as the latest epoch in the stream. The following are three most commonly used scenarios where we want to efficiently know the set of Segments that form the Stream: Initial set of Stream Segments : The head of the Stream computation is very efficient as it is typically either the first epoch record or the latest truncation record. Current set of Stream Segments : The tail of the Stream is identified by the current epoch record. Successors of a particular Stream Segment : The successor query results in two calls into the store to retrieve Stream Segment's sealed epoch and the corresponding epoch record. The successors are computed as the Stream Segments that overlap with the given Stream Segment. Segment Records: Segment-info: \u27e8segmentid, time, keySpace-start, keySpace-end\u27e9 . The Controller stores Stream Segment information within each epoch record. The Stream Segment ID is composed of two parts and is encoded as a 64 bit number. The high 32 bit identifies the creation epoch of the Stream Segment and the low 32 bit uniquely identifies the Stream Segment. Note : To retrieve Stream Segment record given a Stream Segment ID, we first need to extract the creation epoch and then retrieve the Stream Segment record from the epoch record. Stream Configuration \u00b6 Znode under which Stream configuration is serialized and persisted. A Stream configuration contains Stream policies that need to be enforced. Scaling policy and Retention policy are supplied by the application at the time of Stream creation and enforced by Controller by monitoring the rate and size of data in the Stream. The Scaling policy describes if and when to automatically scale is based on incoming traffic conditions into the Stream. The policy supports two flavors - traffic as the rate of Events per second and traffic as the rate of bytes per second . The application specifies their desired traffic rates into each segment by means of scaling policy and the supplied value is chosen to compute thresholds that determine when to scale a given Stream. Retention Policy describes the amount of data that needs to be retained into Pravega cluster for this Stream. We support a time-based and a size-based retention policy where applications can choose whether they want to retain data in the Stream by size or by time by choosing the appropriate policy and supplying their desired values. Stream State \u00b6 Znode which captures the state of the Stream. It is an enumerator with values from creating, active, updating, scaling, truncating, sealing, and sealed . Once active , a Stream transition between performing a specific operation and remains active until it is sealed. A transition map is defined in the State class which allows and prohibits various state transitions. Stream State describes the current state of the Stream. It transitions from active to respective action based on the action being performed on the Stream. For example, during scaling the state of the Stream transitions from active to scaling and once scaling completes, it transitions back to active . Stream State is used as a barrier to ensure only one type of operation is being performed on a given Stream at any point in time. Only certain state transitions are allowed and are described in the state transition object. Only legitimate state transitions are allowed and any attempt for disallowed transition results in an appropriate exception. Truncation Record \u00b6 The Truncation Record captures the latest truncation point of the Stream which is identified by a StreamCut . The truncation StreamCut logically represents the head of the Stream and all the data before this position has been purged completely. For example, let there be n active Segments S 1 , S 2 , ..., S n in a Stream. If we truncate this Stream at a StreamCut SC = {S 1 /O 1 , S 2 /O 2 ,...,S n /O n } , then all data before the given StreamCut could be removed from the durable store. This translates to all the data in Segments that are predecessor Segments of S i for i ={ 1 to n } ; and all the data in Segments S i till offset O i . So we could delete all such predecessor Segments from the Stream and purge all the data before respective offsets from the Segments in StreamCut . Sealed Segments Maps \u00b6 Once the Stream Segments are sealed, the Controller needs to store additional information about the Stream Segment. Presently, we have two types of information: Epoch, the Stream Segment was sealed in. Size of the Stream Segment at the time of sealing. These records have two different characteristics and are used in different types of queries. For example; Sealing epoch is important for querying successor Stream Segments. For each Stream Segment, we store its sealing epoch directly in the metadata store. Stream Segment sizes are used during truncation workflows. For sealed sizes, we store it in a map of Segment to size at the time of sealing. Successor queries are performed on a single Stream Segment whereas truncation workflows work on a group of Stream Segments. This ensures that during truncation we are able to retrieve sealed sizes for multiple Stream Segments with a minimal number of calls into the underlying metadata store. Since we could have an arbitrarily large number of Stream Segments that have been sealed away, we cannot store all of the information in a single map and hence we shard the map and store it. The sharding function we use is to hash the creation epoch and get the shard number. The following are the Transaction Related metadata records: Active Transactions : Each new Transaction is created under the znode. This stores metadata corresponding to each Transaction as Active Transaction Record . Once a Transaction is completed, a new node is created under the global Completed Transaction znode and removed from under the Stream specific Active Transaction node. Completed Transactions : All completed transactions for all Streams are moved under a separate znode upon completion (via either commit or abort paths). The completion status of Transaction is recorded under this record. To avoid proliferation of stale Transaction records, we provide a cluster level configuration to specify the duration for which a completed Transaction's record should be preserved. Controller periodically garbage collects all Transactions that were completed before the aforesaid configured duration. Stream Store Caching \u00b6 In-memory Cache \u00b6 Since there could be multiple concurrent requests for a given Stream being processed by the same Controller instance, it is suboptimal to read the value by querying Zookeeper every time. So we have introduced an in-memory cache that each Stream store maintains. It caches retrieved metadata per Stream so that there is maximum one copy of the data per Stream in the cache. There are two in-memory caches: A cache of multiple Stream objects in the store Cache properties of a Stream in the Stream object . The cache can contain both mutable and immutable values. Immutable values, by definition are not a problem. For mutable values, we have introduced a notion of Operation Context and for each new operation, which ensures that during an operation we lazily load latest value of entities into the cache and then use them for all computations within that Operation's context . Operation Context \u00b6 At the start of any new operation, we create a context for this operation. The creation of a new operation context invalidates all mutable cached entities for a Stream and each entity is lazily retrieved from the store whenever requested. If a value is updated during the course of the operation, it is again invalidated in the cache so that other concurrent read/update operations on the Stream get the new value for their subsequent steps. Stream Buckets \u00b6 To enable some scenarios, we may need the background workers to periodically work on each of the Streams in our cluster to perform some specific action on them. The concept of Stream Bucket is to distribute this periodic background work across all available Controller instances. Controller instances map all available streams in the system into buckets and these buckets are distributed amongst themselves. Hence, all the long-running background work can be uniformly distributed across multiple Controller instances. Note : Number of buckets for a cluster is a fixed (configurable) value for the lifetime of a cluster. Each bucket corresponds to a unique znode in Zookeeper. A qualified scoped Stream name is used to compute a hash value to assign the Stream to a bucket. All Controller instances, upon startup, attempt to take ownership of buckets. Upon failover , ownerships are transferred, as surviving nodes compete to acquire ownership of orphaned buckets. The Controller instance which owns a bucket is responsible for all long running scheduled background work corresponding to all nodes under the bucket. Presently this entails running periodic workflows to capture StreamCut (s) (called Retention-Set) for each Stream at desired frequencies. Retention Set \u00b6 One retention set per Stream is stored under the corresponding bucket/Stream znode. As we compute StreamCut (s) periodically, we keep preserving them under this znode. As some automatic truncation is performed, the StreamCut (s) that are no longer valid are purged from this set. Controller Cluster Listener \u00b6 Each node in Pravega Cluster registers itself under a cluster znode as an ephemeral node. This includes both Controller and Segment Store nodes. Each Controller instance registers a watch on the cluster znode to listen for cluster change notifications. These notify about the added and removed nodes. One Controller instance assumes leadership amongst all Controller instances. This leader Controller instance is responsible for handling Segment Store node change notifications. Based on the changes in topology, Controller instance periodically rebalances segment containers to Segment Store node mapping. All Controller instances listen for Controller node change notifications. Each Controller instance has multiple sub components that implement the failover sweeper interface. Presently there are three components that implement failover sweeper interface namely: TaskSweeper EventProcessors TransactionSweeper Whenever a Controller instance is identified to have been removed from the cluster, the cluster listener invokes all registered failover sweepers to optimistically try to sweep all the orphaned work previously owned by the failed Controller host. Host Store \u00b6 The implementation of the Host store interface is used to store Segment Container to Segment Store node mapping. It exposes API like getHostForSegment where it computes a consistent hash of Segment ID to compute the owner Segment Container. Then based on the container-host mapping, it returns the appropriate URI to the caller. Background Workers \u00b6 Controller process has two different mechanisms or frameworks for processing background work. These background works typically entail multiple steps and updates to metadata under a specific metadata root entity and potential interactions with one or more Segment Stores. We initially started with a simple task framework that gave us the ability to run tasks that take exclusive rights over a given resource (typically a Stream) and allowed for tasks to failover from one Controller instance to another. However, this model was limiting in scope and locking semantics, and had no inherent notion of task ordering as multiple tasks could race to acquire working rights (lock) on a resource concurrently. To overcome this limitation we came up with a new infrastructure called Event Processor . It is built using Pravega Streams and provides a clear mechanism to ensure mutually exclusive and ordered processing . Task Framework \u00b6 The Task Framework is designed to run exclusive background processing per resource such that in case of Controller instance failure, the work can easily failover to another Controller instance and brought to completion. The framework, on its own, does not guarantee idempotent processing and the author of a task has to handle it if required. The model of tasks is defined to work on a given resource exclusively, which means no other task can run concurrently on the same resource. This is implemented by way of a persisted distributed lock implemented on Zookeeper. The failover of a task is achieved by following a scheme of indexing the work a given process is performing. So if a process fails, another process will sweep all outstanding work and attempt to transfer ownership to itself. Note that, upon failure of a Controller process, multiple surviving Controller processes can concurrently attempt sweeping of orphaned tasks. Each of them will index the task in their host-index but exactly one of them will be able to successfully acquire the lock on the resource and hence permission to process the task. The parameters for executing a task are serialized and stored under the resource. Currently, we use the Task Framework only to create Stream tasks. All the other background processing is done using the Event Processor Framework. Event Processor Framework \u00b6 Event processors Framework is a background worker subsystem which reads Events from an internal Stream and processes it, hence the name Event Processor. In Pravega all Event Processors provides at least once processing guarantee. And in its basic flavor, the framework also provides strong ordering guarantees. The Event Processor framework on its own does not guarantee idempotent execution and it is the responsibility of the individual workflows implemented to ensure that the processing is idempotent and safe across multiple executions. In Pravega, there exist different subtypes of Event Processors which allow concurrent processing. We create different Event Processors for different kinds of work. In Pravega, there are three different Event Processors: Committing Transaction, Aborting Transactions, Processing Stream specific requests (scale, update, seal, etc). Each Controller instance has one Event Processor of each type. The Event Processor Framework allows for multiple Readers to be created per Event Processor. All Readers for a specific Event Processor across Controller instances share the same Reader Group, which guarantees mutually exclusive distribution of work across Controller instances. Each Reader gets a dedicated thread where it reads the Event, calls for its processing and upon completion of processing, updates its Checkpoint . Events are posted in the Event Processor-specific Stream and are routed to specific Stream Segments using scoped Stream name as the Routing Key. Serial Event Processor \u00b6 It essentially reads an Event and initiates its processing and waits on it to complete before moving on to the next Event. This provides strong ordering guarantees in processing. And it Checkpoints after processing each Event. Commit Transaction is implemented using this Serial Event Processor. The degree of parallelism for processing these Events is upper bounded by the number of Stream Segments in the internal Stream and lower bounded by the number of Readers. Multiple Events from across different Streams could land up in the same Stream Segment due to Serial processing. Serial processing has a drawback that, processing stalls or flooding of Events from one Stream could adversely impact latencies for unrelated Streams. Concurrent Event Processor \u00b6 To overcome the drawbacks of Serial Event Processor, in Pravega we designed Concurrent Event Processor . Concurrent Event Processor, as the name implies, allows us to process multiple Events concurrently. Here the Reader thread, reads an Event, schedules it\u2019s asynchronous processing and returns to read the next event. There is a ceiling on the number of Events that are concurrently processed at any point in time and as the processing of some Event completes, newer Events are allowed to be fetched. The Checkpoint scheme here becomes slightly more involved to ensure the guarantee at least once processing . However, with concurrent processing the ordering guarantees get broken. But, it is important to note that only ordering guarantees are needed for processing Events from a Stream and not across Streams. In order to satisfy ordering guarantee, we overlay Concurrent Event processor with Serialized Request Handler , which queues up Events from the same Stream in the in-memory queue and processes them in order. Commit Transaction processing is implemented on a dedicated Serial Event Processor because strong commit ordering is required by ensuring that commit does not interfere with processing of other kinds of requests on the Stream. Abort Transaction processing is implemented on a dedicated Concurrent Event Processor which performs abort processing on Transactions from across Streams concurrently. All other requests for Streams are implemented on a Serialized Request Handler which ensures exactly one request per Stream is being processed at any given time and there is ordering guarantee within request processing. However, it allows for concurrent requests from across Streams to go on concurrently. Workflows like scale, truncation, seal, update and delete Stream are implemented for processing on the request Event Processor. Roles and Responsibilities \u00b6 Stream Operations \u00b6 The Controller is the source of truth for all Stream related metadata. Pravega clients (e.g., EventStreamReaders and EventStreamWriters ), in conjunction with the Controller, ensure that Stream invariants are satisfied and honored as they work on Streams. The Controller maintains the metadata of Streams, including the entire history of Stream Segments. The Client accessing a Stream need to contact the Controller to obtain information about Stream Segments. Clients query Controller in order to know how to navigate Streams. For this purpose Controller exposes appropriate API to get active Stream Segments, successors, predecessors and URIs. These queries are served using metadata stored and accessed via Stream store interface. The Controller also provides workflows to modify the state and behavior of the Stream. These workflows include create, scale, truncation, update, seal, and delete . These workflows are invoked both via direct API and in some cases as applicable via background policy manager ( Auto Scaling and Retention ). Request Processing Flow Diagram Create Stream \u00b6 The Create Stream is implemented as a task on Task Framework . The Create Stream workflow first sets the initial Stream set to Creating . Next, it identifies the Segment Containers that should create and own the new Segments for this Stream, and calls CreateSegment() concurrently for all Segments. Once all CreateSegment() (s) return, the createStream() task completes its execution and change the Stream state to Active . In the case of recoverable failures, the operations are retried. However, if it is unable to complete any step, the Stream is left dangling in Creating state. Update Stream \u00b6 Update Stream is implemented as a task on Serialized Request Handler over Concurrent Event Processor Framework. Update Stream is invoked by an explicit API updateStream() call into Controller. It first posts an Update Request Event into request Stream. Following that it tries to create a temporary update property. If it fails to create the temporary update property, the request is failed and the caller is notified of the failure to update a Stream due to conflict with another ongoing update. The Event is picked by Request Event Processor . When the processing starts, the update Stream task expects to find the temporary update Stream property to be present. If it does not find the property, the update processing is delayed by pushing Event the back in the in-memory queue until it deems the Event expired. If it finds the property to be updated during this period, before the expiry, the Event is processed and updateStream() operation is performed. Once the update Stream processing starts, it first sets the Stream state to Updating . Then, the Stream configuration is updated in the metadata store followed by notifying Segment Stores for all active Stream Segments of the Stream, about the change in policy. Now the state is reset to Active . Scale Stream \u00b6 The Scale can be invoked either by explicit API call (referred to as manual scale) or performed automatically based on scale policy (referred to as Auto-scaling ). We first write the Event followed by updating the metadata store to capture our intent to scale a Stream. This step is idempotent and ensures that if an existing ongoing scale operation is in progress, then this attempt to start a new scale is ignored. Also, if there is an ongoing scale operation with a conflicting request input parameter, then the new request is rejected. Which essentially guarantees that there can be exactly one scale operation that can be performed at any given point in time. The start of processing is similar to the mechanism followed in update Stream. If metadata is updated, the Event processes and proceeds with executing the task. If the metadata is not updated within the desired time frame, the Event is discarded. Once scale processing starts, it first sets the Stream State to Scaling . Then creates new Stream Segments in Segment Store. The workflow is as follows: After successfully creating new segments, it creates a new epoch record in the metadata store. The created new epoch record corresponds to a new epoch which contains the list of Stream Segments as they would appear post scale. Each new epoch creation also creates a new root epoch node under which the metadata for all transactions from that epoch resides. After creating requisite metadata records, scale workflow attempts to seal the old Stream segments in the Segment Store. After the old Stream Segments are sealed, we can safely mark the new epoch as the currently active epoch and reset state to Active . Truncate Stream \u00b6 Truncating a Stream follows a similar mechanism to update and has a temporary Stream property for truncation that is used to supply input for truncate Stream. Once the truncate workflow process starts, the Stream State is set to Truncating . Truncate workflow then looks at the requested StreamCut , and checks if it is greater than or equal to the existing truncation point, only then is it a valid input for truncation and the workflow commences. The truncation workflow takes the requested StreamCut and computes all Stream Segments that are to be deleted as part of this truncation request. Then calls into respective Segment Stores to delete identified Stream Segments. Post deletion, we call truncate on Stream Segments that are described in the StreamCut at the offsets as described in the Streamcut . Following this, the truncation record is updated with the new truncation point and deleted Stream Segments. The state is reset to Active . Seal Stream \u00b6 Seal Stream can be requested via an explicit API call into Controller. It first posts a seal Stream Event into request Stream. Once the Seal Stream process starts, the Stream State is set to Sealing . If the event is picked and does not find the Stream to be in the desired state, it postpones the seal Stream processing by reposting it at the back of in-memory queue. Once the Stream is set to sealing state, all active Stream Segments for the Stream are sealed by calling into Segment Store. After this, the Stream is marked as Sealed in the Stream metadata. Delete Stream \u00b6 Delete Stream can be requested via an explicit API call into Controller. The request first verifies if the Stream is in Sealed state. - Only sealed Streams can be deleted and an event to this effect is posted in the request Stream. - When the event is picked for processing, it verifies the Stream state again and then proceeds to delete all Stream Segments that belong to this Stream from its inception by calling into Segment Store. - Once all Stream Segments are deleted successfully, the Stream metadata corresponding to this Stream is cleaned up. Stream Policy Manager \u00b6 As described earlier, there are two types of user-defined policies that Controller is responsible for enforcing, namely Automatic Scaling and Automatic Retention . The Controller is not just the store for Stream policy but it actively enforces those user-defined policies for their Streams. Scaling Infrastructure \u00b6 Scaling infrastructure is built in conjunction with Segment Stores. As the Controller creates new Stream Segments in Segment Stores, it passes user-defined scaling policies to Segment Stores. The Segment Store then monitors traffic for the said Stream Segment and reports to Controller if some thresholds, as determined from policy, are breached. The Controller receives these notifications via Events posted in dedicated internal Streams. There are two types of traffic reports that can be received for segments. It identifies if a Stream Segment should be scaled up (Split). It identifies if a Stream Segment should be scaled down (Merge). For Stream Segments eligible for scale up, the Controller immediately posts the request for Stream Segment scale up in the request Stream for Request Event Processor to process. However, for scale down, the Controller needs to wait for at least two neighboring Stream Segments to become eligible for scale down. For this purpose, it marks the Stream Segment as cold in the metadata store. The Controller consolidates the neighboring Stream Segments that are marked as cold and posts a scale down the request for them. The scale requests processing is then performed asynchronously on the Request Event Processor. Retention Infrastructure \u00b6 The retention policy defines how much data should be retained for a given Stream. This can be defined as time-based or size-based . To apply this policy, Controller periodically collects StreamCut (s) for the Stream and opportunistically performs truncation on previously collected StreamCut (s) if policy dictates it. Since this is a periodic background work that needs to be performed for all Streams that have a retention policy defined, there is an imperative need to fairly distribute this workload across all available Controller instances. To achieve this we rely on bucketing Streams into predefined sets and distributing these sets across Controller instances. This is done by using Zookeeper to store this distribution. Each Controller instance, during bootstrap, attempts to acquire ownership of buckets. All Streams under a bucket are monitored for retention opportunities by the owning Controller. At each period, Controller collects a new StreamCut and adds it to a retention set for the said Stream. Post this it looks for the candidate StreamCut (s) stored in retention set which are eligible for truncation based on the defined retention policy. For example, in time-based retention, the latest StreamCut older than the specified retention period is chosen as the truncation point. Transaction Manager \u00b6 Another important role played by the Controller is that of the Transaction manager. It is responsible for the beginning and ending Transactions. The Controller plays an active role in providing guarantees for Transactions from the time they are created until the time they are committed or aborted. The Controller tracks each Transaction for their specified timeouts, and automatically aborts the Transaction if the timeout exceeds. The Controller is responsible for ensuring that the Transaction and a potential concurrent scale operation play well with each other and ensure all promises made with respect to either are honored and enforced. Transaction Management Diagram Client calls into Controller process to create, ping commit or abort transactions . Each of these requests is received on Controller and handled by the Transaction Management module which implements the business logic for processing each request. Create Transaction \u00b6 Writers interact with Controller to create new Transactions. Controller Service passes the create transaction request to Transaction Management module. The create Transaction function in the module performs the following steps: Generates a unique UUID for the Transaction. It fetches the current active set of Stream Segments for the Stream from metadata store and its corresponding epoch identifier from the history. It creates a new Transaction record in the Zookeeper using the metadata store interface. It then requests Segment Store to create special Transaction Segments that are inherently linked to the parent active Stream Segments. The Controller creates shadow Stream Segments for current active Segments by associating Transaction ID to compute unique shadow Stream Segment identifiers. The lifecycle of shadow Stream Segments are not linked to original Stream Segments and original Stream Segments can be sealed, truncated or deleted without affecting the lifecycle of shadow Stream Segment. Commit Transaction \u00b6 Upon receiving the request to commit a Transaction, Controller Service passes the request to Transaction Management module. This module first tries to mark the Transaction for commit in the Transaction specific metadata record via metadata store. Following this, it posts a commit Event in the internal Commit Stream. The commit event only captures the epoch in which the Transaction has to be committed. Commit Transaction workflow is implemented on commit Event processor and thereby processed asynchronously. When commit workflow starts, it opportunistically collects all available Transactions that have been marked for commit in the given epoch and proceeds to commit them in order and one Transaction at a time. A Transaction commit entails merging the Transaction Segment into its parent Segment. This works perfectly in absence of scale. However, because of scaling of a Stream, some of the parent Segments for Transaction's shadow Stream Segments could have been sealed away. In such instance, when we attempt to commit a Transactions we may not have parent Segments in which Transaction Segments could be merged into. One approach to mitigate this could have been to prevent scaling operation while there were ongoing Transactions. However, this could stall scaling for an arbitrarily large period of time and would be detrimental. Instead, controller decouples scale and Transactions and allows either to occur concurrently without impacting workings of the other. This is achieved by using a scheme called Rolling Transactions . Rolling Transactions \u00b6 This is achieved by using a scheme (Rolling Transactions) where controller allows Transaction Segments to outlive their parent Segments and whenever their commits are issued, at a logical level controller elevates the Transaction Segments as first class Segments and includes them in a new epoch in the epoch time series of the Stream. Transactions are created in an older epoch and when they are attempted to be committed, the latest epoch is sealed, Transactions are rolled over and included and then a duplicate of the latest epoch is created for Stream to restore its previous state before rolling of Transactions. This ensures that Transactions could be created at any time and then be committed at any time without interfering with any other Stream processing. The commit workflow on the controller guarantees that once started it will attempt to commit each of the identified Transactions with indefinite retries until they all succeed. Once a Transaction is committed successfully, the record for the Transaction is removed from under its epoch root. Abort Transaction \u00b6 Abort, like commit, can be requested explicitly by the application. However, abort can also be initiated automatically if the Transaction\u2019s timeout elapses. The Controller tracks the timeout for each and every Transaction in the system and whenever timeout elapses, or upon explicit user request, Transaction Management module marks the Transaction for abort in its respective metadata. After this, the Event is picked for processing by abort Event Processor and the Transactions abort is immediately attempted. There is no ordering requirement for abort Transaction and hence it is performed concurrently and across Streams. Ping Transaction \u00b6 Since Controller has no visibility into data path with respect to data being written to segments in a Transaction, Controller is unaware if a Transaction is being actively worked upon or not and if the timeout elapses it may attempt to abort the Transaction. To enable applications to control the destiny of a Transaction, Controller exposes an API to allow applications to renew Transaction timeout period. This mechanism is called ping and whenever application pings a Transaction, Controller resets its timer for respective transaction. Transaction Timeout Management \u00b6 Controllers track each Transaction for their timeouts. This is implemented as timer wheel service . Each Transaction, upon creation gets registered into the timer service on the Controller where it is created. Subsequent pings for the Transaction could be received on different Controller instances and timer management is transferred to the latest Controller instance based on ownership mechanism implemented via Zookeeper. Upon timeout expiry, an automatic abort is attempted and if it is able to successfully set Transaction status to abort, the abort workflow is initiated. Each Transaction that a Controller is monitoring for timeouts is added to this processes index. If such a Controller instance fails or crashes, other Controller instances will receive node failed notification and attempt to sweep all outstanding Transactions from the failed instance and monitor their timeouts from that point onward. Segment Container to Host Mapping \u00b6 The Controller is also responsible for the assignment of Segment Containers to Segment Store nodes. The responsibility of maintaining this mapping befalls a single Controller instance that is chosen via a leader election using Zookeeper. This leader Controller monitors lifecycle of Segment Store nodes as they are added to/removed from the cluster and performs redistribution of Segment Containers across available Segment Store nodes. This distribution mapping is stored in a dedicated znode. Each Segment Store periodically polls this znode to look for changes and if changes are found, it shuts down and relinquishes containers it no longer owns and attempts to acquire ownership of containers that are assigned to it. The details about implementation, especially with respect to how the metadata is stored and managed is already discussed in the section Cluster Listener . Resources \u00b6 Pravega Code","title":"Controller Service"},{"location":"controller-service/#pravega-controller-service","text":"Introduction Architecture Stream Management Cluster Management System Diagram Components Service Endpoints Controller Service Stream Metadata Store Stream Metadata Stream Store Caching Stream Buckets Controller Cluster Listener Host Store Background workers Roles and Responsibilities Stream Operations Stream State Create Stream Update Stream Scale Stream Truncate Stream Seal Stream Delete Stream Stream Policy Manager Scaling Infrastructure Retention Infrastructure Transaction Manager Create Transaction Commit Transaction Abort Transaction Ping Transaction Transaction Timeout Management Segment Container to Host Mapping Resources","title":"Pravega Controller Service"},{"location":"controller-service/#introduction","text":"The Controller Service is a core component of Pravega that implements the control plane. It acts as the central coordinator and manager for various operations performed in the cluster, the major two categories are: Stream Management and Cluster Management . The Controller Service, referred to simply as Controller henceforth, is responsible for providing the abstraction of a Pravega Stream , which is the main abstraction that Pravega exposes to applications. A Pravega Stream comprises one or more Stream Segments . Each Stream Segment is an append-only data structure that stores a sequence of bytes. A Stream Segment on its own is agnostic to the presence of other Stream Segments and is not aware of its logical relationship with its peer Stream Segments. The Segment Store, which owns and manages these Stream Segments, does not have any notion of a Stream. A Stream is a logical view built by the Controller and consists of a dynamically changing set of Stream Segments that satisfy a predefined set of invariants. The Controller provides the Stream abstraction and orchestrates all lifecycle operations on a Pravega Stream while ensuring its consistency. The Controller plays a central role in the lifecycle of a Stream: creation , updation , truncation , sealing , scaling and deletion . To implement these operations, the Controller manages both a Stream's metadata and its associated Stream Segments. For example, as part of Stream\u2019s lifecycle, new segments can be created and existing segments can be sealed. The Controller decides on performing these operations by ensuring the availability and consistency of the Streams for the clients accessing them.","title":"Introduction"},{"location":"controller-service/#architecture","text":"The Controller Service is made up of one or more instances of stateless worker nodes. Each new Controller instance can be invoked independently and becomes part of Pravega cluster by merely pointing to the same Apache Zookeeper . For high availability, it is advised to have more than one instance of Controller service per cluster. Each Controller instance is capable of working independently and uses a shared persistent store as the source of truth for all state-owned and managed by Controller service. We currently use Apache ZooKeeper as the store for persisting all metadata consistently. Each instance comprises various subsystems which are responsible for performing specific operations on different categories of metadata. These subsystems include different API endpoints, metadata store handles, policy managers and background workers . The Controller exposes two endpoints which can be used to interact with The Pravega Controller Service. The first port is for providing programmatic access for Pravega clients and is implemented as an RPC using gRPC . The other endpoint is for administrative operations and is implemented as a REST endpoint.","title":"Architecture"},{"location":"controller-service/#stream-management","text":"The Controller owns and manages the concept of Stream and is responsible for maintaining \"metadata\" and \"lifecycle\" operations ( creating, updating, scaling, truncating, sealing and deleting Streams ) for each Pravega Stream. The Stream management can be divided into the following three categories: Stream Abstraction : A Stream can be viewed as a series of dynamically changing segment sets where the Stream transitions from one set of consistent segments to the next. The Controller is the place for creating and managing Stream abstraction. The Controller decides when and how a Stream transitions from one state to another and is responsible for performing these transitions by keeping the state of the Stream consistent and available. These transitions are governed user-defined policies that the Controller enforces. Consequently, as part of Stream management, the Controller also performs roles of Policy Manager for policies like retention and scaling. Policy Management : The Controller is responsible for storing and enforcing user-defined Stream policies by actively monitoring the state of the Stream. In Pravega we have two policies that users can define, namely Scaling Policy and Retention Policy . Scaling policy describes if and under what circumstances a Stream should automatically scale its number of segments. Retention policy describes a policy about how much data to retain within a Stream based on time ( Time-based Retention ) and data size ( Size-based Retention ). Transaction Management : Implementing Transactions requires the manipulation of Stream Segments. With each Transaction, Pravega creates a set of Transaction segments, which are later merged onto the Stream Segments upon commit or discarded upon aborts. The Controller performs the role of Transaction manager and is responsible for creating and committing Transactions on a given Stream. Upon creating Transactions, Controller also tracks Transaction timeouts and aborts transactions whose timeouts have elapsed. Details of Transaction management can be found later in the Transactions section.","title":"Stream Management"},{"location":"controller-service/#cluster-management","text":"The Controller is responsible for managing the Segment Store cluster. Controller manages the lifecycle of Segment Store nodes as they are added to/removed from the cluster and performs redistribution of segment containers across available Segment Store nodes.","title":"Cluster Management"},{"location":"controller-service/#system-diagram","text":"The following diagram shows the main components of a Controller process. The elements of the diagram are discussed in detail in the following sections. Controller Process Diagram","title":"System Diagram"},{"location":"controller-service/#components","text":"","title":"Components"},{"location":"controller-service/#service-endpoints","text":"There are two ports exposed by Controller: Client-Controller API and Administration API . The client Controller communication is implemented as RPC which exposes API to perform all Stream related control plane operations. Apart from this Controller also exposes an administrative API implemented as REST . Each endpoint performs the appropriate call to the Pravega Controller Service backend subsystem which has the actual implementation for various operations like create, read, update and delete (CRUD) on entities owned and managed by Controller.","title":"Service Endpoints"},{"location":"controller-service/#grpc","text":"Client Controller communication endpoint is implemented as a gRPC interface. Please check the complete list of API . This exposes API used by Pravega clients (Readers, Writers and Stream Manager) and enables Stream management. Requests enabled by this API include creating, modifying, and deleting Streams. The underlying gRPC framework provides both synchronous and asynchronous programming models. We use the asynchronous model in our client Controller interactions so that the client thread does not block on the response from the server. To be able to append to and read data from Streams, Writers and Readers query Controller to get active Stream Segment sets, successor and predecessor Stream Segments while working with a Stream. For Transactions, the client uses specific API calls to request Controller to create , commit , abort and ping Transactions.","title":"gRPC"},{"location":"controller-service/#rest","text":"For administration, the Controller implements and exposes a REST interface. This includes API calls for Stream management as well as other administration API primarily dealing with creation and deletion of Scopes . We use swagger to describe our REST API. Please see, the swagger yaml file.","title":"REST"},{"location":"controller-service/#pravega-controller-service_1","text":"This is the backend layer behind the Controller endpoints gRPC and REST . All the business logic required to serve Controller API calls are implemented here. This layer contains handles to all other subsystems like the various store implementations ( Stream store , Host store and Checkpoint store) and background processing frameworks ( Task and Event Processor framework ). Stores are interfaces that provide access to various types of metadata managed by Controller. Background processing frameworks are used to perform asynchronous processing that typically implements workflows involving metadata updates and requests to Segment Store.","title":"Pravega Controller Service "},{"location":"controller-service/#stream-metadata-store","text":"A Stream is a dynamically changing sequence of Stream Segments, where regions of the Routing Key space map to open Stream Segments. As the set of Segments of a Stream changes, so do the mapping of the Routing Key space to Segments. A set of Segments is consistent iff the union of key space ranges mapping to Segments in the set covers the entire key space and the key space ranges are disjoint. For example, suppose a set S = { S 1 , S 2 , S 3 }, such that: Region [0, 0.3) maps to Segment S 1 . Region [0.3, 0.6) maps to Segment S 2 . Region [0.6, 1.0) maps to Segment S 3 . S is a consistent Segment set. A Stream goes through transformations as it scales over time. A Stream starts with an initial set of Segments that is determined by the Stream configuration when created and it transitions to new sets of Segments as scale operations are performed on the Stream. Each generation of Segments that constitute the Stream at any given point in time are considered to belong to an epoch . A Stream starts with initial epoch 0 and upon each transition, it moves ahead in its epochs to describe the change in generation of Segments in the Stream. The Controller maintains the Stream: it stores the information about all epochs that constitute a given Stream and also about their transitions. The metadata store is designed to persist the information pertaining to Stream Segments, and to enable queries over this information. Apart from the epoch information, it keeps some additional metadata, such as state and its policies and ongoing Transactions on the Stream. Various sub-components of Controller access the stored metadata for each Stream via a well-defined interface . We currently have two concrete implementations of the Stream store interface: in-memory and Zookeeper backed stores. Both share a common base implementation that relies on Stream objects for providing store-type specific implementations for all Stream-specific metadata. The base implementation of Stream store creates and caches these Stream objects. The Stream objects implement a store/Stream interface. The concrete Stream implementation is specific to the store type and is responsible for implementation of store specific methods. We have a common base implementation of all store types that provide optimistic concurrency. This base class encapsulates the logic for queries against Stream store for all concrete stores that support Compare And Swap (CAS). We currently use Zookeeper as our underlying store which also supports CAS. We store all Stream metadata in a hierarchical fashion under Stream specific znodes (ZooKeeper data nodes). For the ZooKeeper-based store, we structure our metadata into different groups to support a variety of queries against this metadata. All Stream specific metadata is stored under a scoped/Stream root node. Queries against this metadata include, but not limited to, querying segment sets that form the Stream at different points in time, segment specific information, segment predecessors and successors. Refer to Stream metadata interface for details about API exposed by Stream metadata store.","title":"Stream Metadata Store"},{"location":"controller-service/#stream-metadata","text":"Clients need information about what Segments constitute a Stream to start their processing and they obtain it from the epoch information the Controller stores in the stream store. Clients need the ability to query and find Stream Segments at any of the three cases efficiently: A Reader client typically starts from the head of the Stream, But it might also choose to access the Stream starting from any arbitrarily interesting position. Writers on the other hand always append to the tail of the Stream. To enable such queries, the Stream store provides API calls to get the initial set of Stream Segments, Segments at a specific time and current set of Segments. As mentioned earlier, a Stream can transition from one set of Stream Segments (epoch) to another set of Segments that constitute the Stream. A Stream moves from one epoch to another if there is at least one Stream Segment that is sealed and replaced by one or more set of Stream Segments that cover precisely the key space of the sealed Segments. As clients work on Streams, they may encounter the end of sealed Stream Segments and consequently need to find new Segments to be able to move forward. To enable the clients to query for the next Segments, the stream store exposes via the Controller Service efficient queries for finding immediate successors and predecessors for any arbitrary Segment. To enable serving queries like those mentioned above, we need to efficiently store a time series of these Segment transitions and index them against time. We store this information about the current and historical state of a Stream Segments in a set of records which are designed to optimize on aforementioned queries. Apart from Segment-specific metadata record, the current state of Stream comprises of other metadata types that are described henceforth.","title":"Stream Metadata"},{"location":"controller-service/#records","text":"Stream time series is stored as a series of records where each record corresponds to an epoch. As Stream scales and transitions from one epoch to another, a new record is created that has complete information about Stream Segments that forms the epoch. Epoch Records: Epoch: \u27e8time, list-of-segments-in-epoch\u27e9 . We store the series of active Stream Segments as they transition from one epoch to another into individual epoch records. Each epoch record corresponds to an epoch which captures a logically consistent (as defined earlier) set of Stream Segments that form the Stream and are valid through the lifespan of the epoch. The epoch record is stored against the epoch number. This record is optimized to answer to query Segments from an epoch with a single call into the store that also enables retrieval of all Stream Segment records in the epoch in O(1) . This record is also used for fetching a Segment-specific record by first computing Stream Segment's creation epoch from Stream Segment ID and then retrieving the epoch record. Current Epoch: A special epoch record called currentEpoch . This is the currently active epoch in the Stream. At any time exactly one epoch is marked as the current epoch. Typically this is the latest epoch with the highest epoch number. However, during an ongoing Stream update workflow like scale or rolling Transaction , the current epoch may not necessarily be the latest epoch. However, at the completion of these workflows, the current epoch is marked as the latest epoch in the stream. The following are three most commonly used scenarios where we want to efficiently know the set of Segments that form the Stream: Initial set of Stream Segments : The head of the Stream computation is very efficient as it is typically either the first epoch record or the latest truncation record. Current set of Stream Segments : The tail of the Stream is identified by the current epoch record. Successors of a particular Stream Segment : The successor query results in two calls into the store to retrieve Stream Segment's sealed epoch and the corresponding epoch record. The successors are computed as the Stream Segments that overlap with the given Stream Segment. Segment Records: Segment-info: \u27e8segmentid, time, keySpace-start, keySpace-end\u27e9 . The Controller stores Stream Segment information within each epoch record. The Stream Segment ID is composed of two parts and is encoded as a 64 bit number. The high 32 bit identifies the creation epoch of the Stream Segment and the low 32 bit uniquely identifies the Stream Segment. Note : To retrieve Stream Segment record given a Stream Segment ID, we first need to extract the creation epoch and then retrieve the Stream Segment record from the epoch record.","title":"Records"},{"location":"controller-service/#stream-configuration","text":"Znode under which Stream configuration is serialized and persisted. A Stream configuration contains Stream policies that need to be enforced. Scaling policy and Retention policy are supplied by the application at the time of Stream creation and enforced by Controller by monitoring the rate and size of data in the Stream. The Scaling policy describes if and when to automatically scale is based on incoming traffic conditions into the Stream. The policy supports two flavors - traffic as the rate of Events per second and traffic as the rate of bytes per second . The application specifies their desired traffic rates into each segment by means of scaling policy and the supplied value is chosen to compute thresholds that determine when to scale a given Stream. Retention Policy describes the amount of data that needs to be retained into Pravega cluster for this Stream. We support a time-based and a size-based retention policy where applications can choose whether they want to retain data in the Stream by size or by time by choosing the appropriate policy and supplying their desired values.","title":"Stream Configuration"},{"location":"controller-service/#stream-state","text":"Znode which captures the state of the Stream. It is an enumerator with values from creating, active, updating, scaling, truncating, sealing, and sealed . Once active , a Stream transition between performing a specific operation and remains active until it is sealed. A transition map is defined in the State class which allows and prohibits various state transitions. Stream State describes the current state of the Stream. It transitions from active to respective action based on the action being performed on the Stream. For example, during scaling the state of the Stream transitions from active to scaling and once scaling completes, it transitions back to active . Stream State is used as a barrier to ensure only one type of operation is being performed on a given Stream at any point in time. Only certain state transitions are allowed and are described in the state transition object. Only legitimate state transitions are allowed and any attempt for disallowed transition results in an appropriate exception.","title":"Stream State"},{"location":"controller-service/#truncation-record","text":"The Truncation Record captures the latest truncation point of the Stream which is identified by a StreamCut . The truncation StreamCut logically represents the head of the Stream and all the data before this position has been purged completely. For example, let there be n active Segments S 1 , S 2 , ..., S n in a Stream. If we truncate this Stream at a StreamCut SC = {S 1 /O 1 , S 2 /O 2 ,...,S n /O n } , then all data before the given StreamCut could be removed from the durable store. This translates to all the data in Segments that are predecessor Segments of S i for i ={ 1 to n } ; and all the data in Segments S i till offset O i . So we could delete all such predecessor Segments from the Stream and purge all the data before respective offsets from the Segments in StreamCut .","title":"Truncation Record"},{"location":"controller-service/#sealed-segments-maps","text":"Once the Stream Segments are sealed, the Controller needs to store additional information about the Stream Segment. Presently, we have two types of information: Epoch, the Stream Segment was sealed in. Size of the Stream Segment at the time of sealing. These records have two different characteristics and are used in different types of queries. For example; Sealing epoch is important for querying successor Stream Segments. For each Stream Segment, we store its sealing epoch directly in the metadata store. Stream Segment sizes are used during truncation workflows. For sealed sizes, we store it in a map of Segment to size at the time of sealing. Successor queries are performed on a single Stream Segment whereas truncation workflows work on a group of Stream Segments. This ensures that during truncation we are able to retrieve sealed sizes for multiple Stream Segments with a minimal number of calls into the underlying metadata store. Since we could have an arbitrarily large number of Stream Segments that have been sealed away, we cannot store all of the information in a single map and hence we shard the map and store it. The sharding function we use is to hash the creation epoch and get the shard number. The following are the Transaction Related metadata records: Active Transactions : Each new Transaction is created under the znode. This stores metadata corresponding to each Transaction as Active Transaction Record . Once a Transaction is completed, a new node is created under the global Completed Transaction znode and removed from under the Stream specific Active Transaction node. Completed Transactions : All completed transactions for all Streams are moved under a separate znode upon completion (via either commit or abort paths). The completion status of Transaction is recorded under this record. To avoid proliferation of stale Transaction records, we provide a cluster level configuration to specify the duration for which a completed Transaction's record should be preserved. Controller periodically garbage collects all Transactions that were completed before the aforesaid configured duration.","title":"Sealed Segments Maps"},{"location":"controller-service/#stream-store-caching","text":"","title":"Stream Store Caching"},{"location":"controller-service/#in-memory-cache","text":"Since there could be multiple concurrent requests for a given Stream being processed by the same Controller instance, it is suboptimal to read the value by querying Zookeeper every time. So we have introduced an in-memory cache that each Stream store maintains. It caches retrieved metadata per Stream so that there is maximum one copy of the data per Stream in the cache. There are two in-memory caches: A cache of multiple Stream objects in the store Cache properties of a Stream in the Stream object . The cache can contain both mutable and immutable values. Immutable values, by definition are not a problem. For mutable values, we have introduced a notion of Operation Context and for each new operation, which ensures that during an operation we lazily load latest value of entities into the cache and then use them for all computations within that Operation's context .","title":"In-memory Cache"},{"location":"controller-service/#operation-context","text":"At the start of any new operation, we create a context for this operation. The creation of a new operation context invalidates all mutable cached entities for a Stream and each entity is lazily retrieved from the store whenever requested. If a value is updated during the course of the operation, it is again invalidated in the cache so that other concurrent read/update operations on the Stream get the new value for their subsequent steps.","title":"Operation Context"},{"location":"controller-service/#stream-buckets","text":"To enable some scenarios, we may need the background workers to periodically work on each of the Streams in our cluster to perform some specific action on them. The concept of Stream Bucket is to distribute this periodic background work across all available Controller instances. Controller instances map all available streams in the system into buckets and these buckets are distributed amongst themselves. Hence, all the long-running background work can be uniformly distributed across multiple Controller instances. Note : Number of buckets for a cluster is a fixed (configurable) value for the lifetime of a cluster. Each bucket corresponds to a unique znode in Zookeeper. A qualified scoped Stream name is used to compute a hash value to assign the Stream to a bucket. All Controller instances, upon startup, attempt to take ownership of buckets. Upon failover , ownerships are transferred, as surviving nodes compete to acquire ownership of orphaned buckets. The Controller instance which owns a bucket is responsible for all long running scheduled background work corresponding to all nodes under the bucket. Presently this entails running periodic workflows to capture StreamCut (s) (called Retention-Set) for each Stream at desired frequencies.","title":"Stream Buckets"},{"location":"controller-service/#retention-set","text":"One retention set per Stream is stored under the corresponding bucket/Stream znode. As we compute StreamCut (s) periodically, we keep preserving them under this znode. As some automatic truncation is performed, the StreamCut (s) that are no longer valid are purged from this set.","title":"Retention Set"},{"location":"controller-service/#controller-cluster-listener","text":"Each node in Pravega Cluster registers itself under a cluster znode as an ephemeral node. This includes both Controller and Segment Store nodes. Each Controller instance registers a watch on the cluster znode to listen for cluster change notifications. These notify about the added and removed nodes. One Controller instance assumes leadership amongst all Controller instances. This leader Controller instance is responsible for handling Segment Store node change notifications. Based on the changes in topology, Controller instance periodically rebalances segment containers to Segment Store node mapping. All Controller instances listen for Controller node change notifications. Each Controller instance has multiple sub components that implement the failover sweeper interface. Presently there are three components that implement failover sweeper interface namely: TaskSweeper EventProcessors TransactionSweeper Whenever a Controller instance is identified to have been removed from the cluster, the cluster listener invokes all registered failover sweepers to optimistically try to sweep all the orphaned work previously owned by the failed Controller host.","title":"Controller Cluster Listener"},{"location":"controller-service/#host-store","text":"The implementation of the Host store interface is used to store Segment Container to Segment Store node mapping. It exposes API like getHostForSegment where it computes a consistent hash of Segment ID to compute the owner Segment Container. Then based on the container-host mapping, it returns the appropriate URI to the caller.","title":"Host Store"},{"location":"controller-service/#background-workers","text":"Controller process has two different mechanisms or frameworks for processing background work. These background works typically entail multiple steps and updates to metadata under a specific metadata root entity and potential interactions with one or more Segment Stores. We initially started with a simple task framework that gave us the ability to run tasks that take exclusive rights over a given resource (typically a Stream) and allowed for tasks to failover from one Controller instance to another. However, this model was limiting in scope and locking semantics, and had no inherent notion of task ordering as multiple tasks could race to acquire working rights (lock) on a resource concurrently. To overcome this limitation we came up with a new infrastructure called Event Processor . It is built using Pravega Streams and provides a clear mechanism to ensure mutually exclusive and ordered processing .","title":"Background Workers"},{"location":"controller-service/#task-framework","text":"The Task Framework is designed to run exclusive background processing per resource such that in case of Controller instance failure, the work can easily failover to another Controller instance and brought to completion. The framework, on its own, does not guarantee idempotent processing and the author of a task has to handle it if required. The model of tasks is defined to work on a given resource exclusively, which means no other task can run concurrently on the same resource. This is implemented by way of a persisted distributed lock implemented on Zookeeper. The failover of a task is achieved by following a scheme of indexing the work a given process is performing. So if a process fails, another process will sweep all outstanding work and attempt to transfer ownership to itself. Note that, upon failure of a Controller process, multiple surviving Controller processes can concurrently attempt sweeping of orphaned tasks. Each of them will index the task in their host-index but exactly one of them will be able to successfully acquire the lock on the resource and hence permission to process the task. The parameters for executing a task are serialized and stored under the resource. Currently, we use the Task Framework only to create Stream tasks. All the other background processing is done using the Event Processor Framework.","title":"Task Framework"},{"location":"controller-service/#event-processor-framework","text":"Event processors Framework is a background worker subsystem which reads Events from an internal Stream and processes it, hence the name Event Processor. In Pravega all Event Processors provides at least once processing guarantee. And in its basic flavor, the framework also provides strong ordering guarantees. The Event Processor framework on its own does not guarantee idempotent execution and it is the responsibility of the individual workflows implemented to ensure that the processing is idempotent and safe across multiple executions. In Pravega, there exist different subtypes of Event Processors which allow concurrent processing. We create different Event Processors for different kinds of work. In Pravega, there are three different Event Processors: Committing Transaction, Aborting Transactions, Processing Stream specific requests (scale, update, seal, etc). Each Controller instance has one Event Processor of each type. The Event Processor Framework allows for multiple Readers to be created per Event Processor. All Readers for a specific Event Processor across Controller instances share the same Reader Group, which guarantees mutually exclusive distribution of work across Controller instances. Each Reader gets a dedicated thread where it reads the Event, calls for its processing and upon completion of processing, updates its Checkpoint . Events are posted in the Event Processor-specific Stream and are routed to specific Stream Segments using scoped Stream name as the Routing Key.","title":"Event Processor Framework"},{"location":"controller-service/#serial-event-processor","text":"It essentially reads an Event and initiates its processing and waits on it to complete before moving on to the next Event. This provides strong ordering guarantees in processing. And it Checkpoints after processing each Event. Commit Transaction is implemented using this Serial Event Processor. The degree of parallelism for processing these Events is upper bounded by the number of Stream Segments in the internal Stream and lower bounded by the number of Readers. Multiple Events from across different Streams could land up in the same Stream Segment due to Serial processing. Serial processing has a drawback that, processing stalls or flooding of Events from one Stream could adversely impact latencies for unrelated Streams.","title":"Serial Event Processor"},{"location":"controller-service/#concurrent-event-processor","text":"To overcome the drawbacks of Serial Event Processor, in Pravega we designed Concurrent Event Processor . Concurrent Event Processor, as the name implies, allows us to process multiple Events concurrently. Here the Reader thread, reads an Event, schedules it\u2019s asynchronous processing and returns to read the next event. There is a ceiling on the number of Events that are concurrently processed at any point in time and as the processing of some Event completes, newer Events are allowed to be fetched. The Checkpoint scheme here becomes slightly more involved to ensure the guarantee at least once processing . However, with concurrent processing the ordering guarantees get broken. But, it is important to note that only ordering guarantees are needed for processing Events from a Stream and not across Streams. In order to satisfy ordering guarantee, we overlay Concurrent Event processor with Serialized Request Handler , which queues up Events from the same Stream in the in-memory queue and processes them in order. Commit Transaction processing is implemented on a dedicated Serial Event Processor because strong commit ordering is required by ensuring that commit does not interfere with processing of other kinds of requests on the Stream. Abort Transaction processing is implemented on a dedicated Concurrent Event Processor which performs abort processing on Transactions from across Streams concurrently. All other requests for Streams are implemented on a Serialized Request Handler which ensures exactly one request per Stream is being processed at any given time and there is ordering guarantee within request processing. However, it allows for concurrent requests from across Streams to go on concurrently. Workflows like scale, truncation, seal, update and delete Stream are implemented for processing on the request Event Processor.","title":"Concurrent Event Processor"},{"location":"controller-service/#roles-and-responsibilities","text":"","title":"Roles and Responsibilities"},{"location":"controller-service/#stream-operations","text":"The Controller is the source of truth for all Stream related metadata. Pravega clients (e.g., EventStreamReaders and EventStreamWriters ), in conjunction with the Controller, ensure that Stream invariants are satisfied and honored as they work on Streams. The Controller maintains the metadata of Streams, including the entire history of Stream Segments. The Client accessing a Stream need to contact the Controller to obtain information about Stream Segments. Clients query Controller in order to know how to navigate Streams. For this purpose Controller exposes appropriate API to get active Stream Segments, successors, predecessors and URIs. These queries are served using metadata stored and accessed via Stream store interface. The Controller also provides workflows to modify the state and behavior of the Stream. These workflows include create, scale, truncation, update, seal, and delete . These workflows are invoked both via direct API and in some cases as applicable via background policy manager ( Auto Scaling and Retention ). Request Processing Flow Diagram","title":"Stream Operations"},{"location":"controller-service/#create-stream","text":"The Create Stream is implemented as a task on Task Framework . The Create Stream workflow first sets the initial Stream set to Creating . Next, it identifies the Segment Containers that should create and own the new Segments for this Stream, and calls CreateSegment() concurrently for all Segments. Once all CreateSegment() (s) return, the createStream() task completes its execution and change the Stream state to Active . In the case of recoverable failures, the operations are retried. However, if it is unable to complete any step, the Stream is left dangling in Creating state.","title":"Create Stream"},{"location":"controller-service/#update-stream","text":"Update Stream is implemented as a task on Serialized Request Handler over Concurrent Event Processor Framework. Update Stream is invoked by an explicit API updateStream() call into Controller. It first posts an Update Request Event into request Stream. Following that it tries to create a temporary update property. If it fails to create the temporary update property, the request is failed and the caller is notified of the failure to update a Stream due to conflict with another ongoing update. The Event is picked by Request Event Processor . When the processing starts, the update Stream task expects to find the temporary update Stream property to be present. If it does not find the property, the update processing is delayed by pushing Event the back in the in-memory queue until it deems the Event expired. If it finds the property to be updated during this period, before the expiry, the Event is processed and updateStream() operation is performed. Once the update Stream processing starts, it first sets the Stream state to Updating . Then, the Stream configuration is updated in the metadata store followed by notifying Segment Stores for all active Stream Segments of the Stream, about the change in policy. Now the state is reset to Active .","title":"Update Stream"},{"location":"controller-service/#scale-stream","text":"The Scale can be invoked either by explicit API call (referred to as manual scale) or performed automatically based on scale policy (referred to as Auto-scaling ). We first write the Event followed by updating the metadata store to capture our intent to scale a Stream. This step is idempotent and ensures that if an existing ongoing scale operation is in progress, then this attempt to start a new scale is ignored. Also, if there is an ongoing scale operation with a conflicting request input parameter, then the new request is rejected. Which essentially guarantees that there can be exactly one scale operation that can be performed at any given point in time. The start of processing is similar to the mechanism followed in update Stream. If metadata is updated, the Event processes and proceeds with executing the task. If the metadata is not updated within the desired time frame, the Event is discarded. Once scale processing starts, it first sets the Stream State to Scaling . Then creates new Stream Segments in Segment Store. The workflow is as follows: After successfully creating new segments, it creates a new epoch record in the metadata store. The created new epoch record corresponds to a new epoch which contains the list of Stream Segments as they would appear post scale. Each new epoch creation also creates a new root epoch node under which the metadata for all transactions from that epoch resides. After creating requisite metadata records, scale workflow attempts to seal the old Stream segments in the Segment Store. After the old Stream Segments are sealed, we can safely mark the new epoch as the currently active epoch and reset state to Active .","title":"Scale Stream"},{"location":"controller-service/#truncate-stream","text":"Truncating a Stream follows a similar mechanism to update and has a temporary Stream property for truncation that is used to supply input for truncate Stream. Once the truncate workflow process starts, the Stream State is set to Truncating . Truncate workflow then looks at the requested StreamCut , and checks if it is greater than or equal to the existing truncation point, only then is it a valid input for truncation and the workflow commences. The truncation workflow takes the requested StreamCut and computes all Stream Segments that are to be deleted as part of this truncation request. Then calls into respective Segment Stores to delete identified Stream Segments. Post deletion, we call truncate on Stream Segments that are described in the StreamCut at the offsets as described in the Streamcut . Following this, the truncation record is updated with the new truncation point and deleted Stream Segments. The state is reset to Active .","title":"Truncate Stream"},{"location":"controller-service/#seal-stream","text":"Seal Stream can be requested via an explicit API call into Controller. It first posts a seal Stream Event into request Stream. Once the Seal Stream process starts, the Stream State is set to Sealing . If the event is picked and does not find the Stream to be in the desired state, it postpones the seal Stream processing by reposting it at the back of in-memory queue. Once the Stream is set to sealing state, all active Stream Segments for the Stream are sealed by calling into Segment Store. After this, the Stream is marked as Sealed in the Stream metadata.","title":"Seal Stream"},{"location":"controller-service/#delete-stream","text":"Delete Stream can be requested via an explicit API call into Controller. The request first verifies if the Stream is in Sealed state. - Only sealed Streams can be deleted and an event to this effect is posted in the request Stream. - When the event is picked for processing, it verifies the Stream state again and then proceeds to delete all Stream Segments that belong to this Stream from its inception by calling into Segment Store. - Once all Stream Segments are deleted successfully, the Stream metadata corresponding to this Stream is cleaned up.","title":"Delete Stream"},{"location":"controller-service/#stream-policy-manager","text":"As described earlier, there are two types of user-defined policies that Controller is responsible for enforcing, namely Automatic Scaling and Automatic Retention . The Controller is not just the store for Stream policy but it actively enforces those user-defined policies for their Streams.","title":"Stream Policy Manager"},{"location":"controller-service/#scaling-infrastructure","text":"Scaling infrastructure is built in conjunction with Segment Stores. As the Controller creates new Stream Segments in Segment Stores, it passes user-defined scaling policies to Segment Stores. The Segment Store then monitors traffic for the said Stream Segment and reports to Controller if some thresholds, as determined from policy, are breached. The Controller receives these notifications via Events posted in dedicated internal Streams. There are two types of traffic reports that can be received for segments. It identifies if a Stream Segment should be scaled up (Split). It identifies if a Stream Segment should be scaled down (Merge). For Stream Segments eligible for scale up, the Controller immediately posts the request for Stream Segment scale up in the request Stream for Request Event Processor to process. However, for scale down, the Controller needs to wait for at least two neighboring Stream Segments to become eligible for scale down. For this purpose, it marks the Stream Segment as cold in the metadata store. The Controller consolidates the neighboring Stream Segments that are marked as cold and posts a scale down the request for them. The scale requests processing is then performed asynchronously on the Request Event Processor.","title":"Scaling Infrastructure"},{"location":"controller-service/#retention-infrastructure","text":"The retention policy defines how much data should be retained for a given Stream. This can be defined as time-based or size-based . To apply this policy, Controller periodically collects StreamCut (s) for the Stream and opportunistically performs truncation on previously collected StreamCut (s) if policy dictates it. Since this is a periodic background work that needs to be performed for all Streams that have a retention policy defined, there is an imperative need to fairly distribute this workload across all available Controller instances. To achieve this we rely on bucketing Streams into predefined sets and distributing these sets across Controller instances. This is done by using Zookeeper to store this distribution. Each Controller instance, during bootstrap, attempts to acquire ownership of buckets. All Streams under a bucket are monitored for retention opportunities by the owning Controller. At each period, Controller collects a new StreamCut and adds it to a retention set for the said Stream. Post this it looks for the candidate StreamCut (s) stored in retention set which are eligible for truncation based on the defined retention policy. For example, in time-based retention, the latest StreamCut older than the specified retention period is chosen as the truncation point.","title":"Retention Infrastructure"},{"location":"controller-service/#transaction-manager","text":"Another important role played by the Controller is that of the Transaction manager. It is responsible for the beginning and ending Transactions. The Controller plays an active role in providing guarantees for Transactions from the time they are created until the time they are committed or aborted. The Controller tracks each Transaction for their specified timeouts, and automatically aborts the Transaction if the timeout exceeds. The Controller is responsible for ensuring that the Transaction and a potential concurrent scale operation play well with each other and ensure all promises made with respect to either are honored and enforced. Transaction Management Diagram Client calls into Controller process to create, ping commit or abort transactions . Each of these requests is received on Controller and handled by the Transaction Management module which implements the business logic for processing each request.","title":"Transaction Manager"},{"location":"controller-service/#create-transaction","text":"Writers interact with Controller to create new Transactions. Controller Service passes the create transaction request to Transaction Management module. The create Transaction function in the module performs the following steps: Generates a unique UUID for the Transaction. It fetches the current active set of Stream Segments for the Stream from metadata store and its corresponding epoch identifier from the history. It creates a new Transaction record in the Zookeeper using the metadata store interface. It then requests Segment Store to create special Transaction Segments that are inherently linked to the parent active Stream Segments. The Controller creates shadow Stream Segments for current active Segments by associating Transaction ID to compute unique shadow Stream Segment identifiers. The lifecycle of shadow Stream Segments are not linked to original Stream Segments and original Stream Segments can be sealed, truncated or deleted without affecting the lifecycle of shadow Stream Segment.","title":"Create Transaction"},{"location":"controller-service/#commit-transaction","text":"Upon receiving the request to commit a Transaction, Controller Service passes the request to Transaction Management module. This module first tries to mark the Transaction for commit in the Transaction specific metadata record via metadata store. Following this, it posts a commit Event in the internal Commit Stream. The commit event only captures the epoch in which the Transaction has to be committed. Commit Transaction workflow is implemented on commit Event processor and thereby processed asynchronously. When commit workflow starts, it opportunistically collects all available Transactions that have been marked for commit in the given epoch and proceeds to commit them in order and one Transaction at a time. A Transaction commit entails merging the Transaction Segment into its parent Segment. This works perfectly in absence of scale. However, because of scaling of a Stream, some of the parent Segments for Transaction's shadow Stream Segments could have been sealed away. In such instance, when we attempt to commit a Transactions we may not have parent Segments in which Transaction Segments could be merged into. One approach to mitigate this could have been to prevent scaling operation while there were ongoing Transactions. However, this could stall scaling for an arbitrarily large period of time and would be detrimental. Instead, controller decouples scale and Transactions and allows either to occur concurrently without impacting workings of the other. This is achieved by using a scheme called Rolling Transactions .","title":"Commit Transaction"},{"location":"controller-service/#rolling-transactions","text":"This is achieved by using a scheme (Rolling Transactions) where controller allows Transaction Segments to outlive their parent Segments and whenever their commits are issued, at a logical level controller elevates the Transaction Segments as first class Segments and includes them in a new epoch in the epoch time series of the Stream. Transactions are created in an older epoch and when they are attempted to be committed, the latest epoch is sealed, Transactions are rolled over and included and then a duplicate of the latest epoch is created for Stream to restore its previous state before rolling of Transactions. This ensures that Transactions could be created at any time and then be committed at any time without interfering with any other Stream processing. The commit workflow on the controller guarantees that once started it will attempt to commit each of the identified Transactions with indefinite retries until they all succeed. Once a Transaction is committed successfully, the record for the Transaction is removed from under its epoch root.","title":"Rolling Transactions"},{"location":"controller-service/#abort-transaction","text":"Abort, like commit, can be requested explicitly by the application. However, abort can also be initiated automatically if the Transaction\u2019s timeout elapses. The Controller tracks the timeout for each and every Transaction in the system and whenever timeout elapses, or upon explicit user request, Transaction Management module marks the Transaction for abort in its respective metadata. After this, the Event is picked for processing by abort Event Processor and the Transactions abort is immediately attempted. There is no ordering requirement for abort Transaction and hence it is performed concurrently and across Streams.","title":"Abort Transaction"},{"location":"controller-service/#ping-transaction","text":"Since Controller has no visibility into data path with respect to data being written to segments in a Transaction, Controller is unaware if a Transaction is being actively worked upon or not and if the timeout elapses it may attempt to abort the Transaction. To enable applications to control the destiny of a Transaction, Controller exposes an API to allow applications to renew Transaction timeout period. This mechanism is called ping and whenever application pings a Transaction, Controller resets its timer for respective transaction.","title":"Ping Transaction"},{"location":"controller-service/#transaction-timeout-management","text":"Controllers track each Transaction for their timeouts. This is implemented as timer wheel service . Each Transaction, upon creation gets registered into the timer service on the Controller where it is created. Subsequent pings for the Transaction could be received on different Controller instances and timer management is transferred to the latest Controller instance based on ownership mechanism implemented via Zookeeper. Upon timeout expiry, an automatic abort is attempted and if it is able to successfully set Transaction status to abort, the abort workflow is initiated. Each Transaction that a Controller is monitoring for timeouts is added to this processes index. If such a Controller instance fails or crashes, other Controller instances will receive node failed notification and attempt to sweep all outstanding Transactions from the failed instance and monitor their timeouts from that point onward.","title":"Transaction Timeout Management"},{"location":"controller-service/#segment-container-to-host-mapping","text":"The Controller is also responsible for the assignment of Segment Containers to Segment Store nodes. The responsibility of maintaining this mapping befalls a single Controller instance that is chosen via a leader election using Zookeeper. This leader Controller monitors lifecycle of Segment Store nodes as they are added to/removed from the cluster and performs redistribution of Segment Containers across available Segment Store nodes. This distribution mapping is stored in a dedicated znode. Each Segment Store periodically polls this znode to look for changes and if changes are found, it shuts down and relinquishes containers it no longer owns and attempts to acquire ownership of containers that are assigned to it. The details about implementation, especially with respect to how the metadata is stored and managed is already discussed in the section Cluster Listener .","title":"Segment Container to Host Mapping"},{"location":"controller-service/#resources","text":"Pravega Code","title":"Resources"},{"location":"faq/","text":"Frequently Asked Questions \u00b6 What is Pravega? Pravega is an open source storage primitive implementing Streams for continuous and unbounded data. See here for more definitions of terms related to Pravega. What does \"Pravega\" mean? \"Pravega\" is a word from Sanskrit referring to \"good speed\". Is Pravega similiar to systems such as Kafka and Kinesis? Pravega is built from the ground up as an enterprise grade storage system to support features such as exactly once, durability etc. Pravega is an ideal store for streaming data, data from real-time applications and IoT data. How can I participate in open source? Disruptive innovation is accelerated by open source. When Pravega was created, there was no question it made sense to make it open source. We welcome contributions from experienced and new developers alike. Check out the code in Github . More detail about how to get involved can be found here . How do I get started? Read the Getting Started guide for more information, and also visit sample-apps repo for some sample applications. I am stuck. Where can I get help? Don\u2019t hesitate to ask! Contact the developers and community on the mailing lists if you need any help. See Join the Community for more details. Does Pravega support exactly once semantics? Absolutely. See Key Features for a discussion on how Pravega supports exactly once semantics. How does Pravega work with stream processors such as Apache Flink? So many features of Pravega make it ideal for stream processors. First, Pravega comes out of the box with a Flink connector. Critically, Pravega provides exactly once semantics, making it much easier to develop accurate stream processing applications. The combination of exactly once semantics, durable storage and transactions makes Pravega an ideal way to chain Flink jobs together, providing end-end consistency and exactly once semantics. See here for a list of key features of Pravega. How does auto scaling work between stream processors and Flink Auto scaling is a feature of Pravega where the number of segments in a stream changes based on the ingestion rate of data. If data arrives at a faster rate, Pravega increases the capacity of a stream by adding segments. When the data rate falls, Pravega can reduce capacity of a stream. As Pravega scales up and down the capacity of a stream, applications, such as a Flink job can observe this change and respond by adding or reducing the number of job instances consuming the stream. See the \"Auto Scaling\" section in Key Features for more discussion of auto scaling. What consistency guarantees does Pravega provide? Pravega makes several guarantees. Durability - once data is acknowledged to a client, Pravega guarantees it is protected. Ordering - events with the same routing key will always be read in the order they were written. Exactly once - data written to Pravega will not be duplicated. Why is supporting consistency and durability so important for storage systems such as Pravega? Primarily because it makes building applications easier. Consistency and durability are key for supporting exactly once semantics. Without exactly once semantics, it is difficult to build fault tolerant applications that consistency produce accurate results. See Key Features for a discussion on consistency and durability guarantees play a role in Pravega's support of exactly once semantics. Does Pravega support transactions? Yes. The Pravega API allows an application to create a transaction on a stream and write data to the transaction. The data is durably stored, just like any other data written to Pravega. When the application chooses, it can commit or abort the transaction. When a transaction is committed, the data in the transaction is atomically appended to the stream. See here for more details on Pravega's transaction support. Does Pravega support transactions across different routing keys? Yes. A transaction in Pravega is itself a stream; it can have 1 or more segments and data written to the transaction is placed into the segment associated with the data's routing key. When the transaction is committed, the transaction data is appended to the appropriate segment in the stream. Do I need HDFS installed in order to use Pravega? Yes. Normally, you would deploy an HDFS for Pravega to use as its Tier 2 storage. However, for simple test/dev environments, the so-called standAlone version of Pravega provides its own simulated HDFS. See the Running Pravega guide for more details. Which Tier 2 storage systems does Pravega support? Pravega is designed to support various types of Tier 2 storage systems. Currently we have implemented HDFS as the first embodiment of Tier 2 storage. What distributed computing primitives does Pravega provide? Pravega provides an API construct called StateSynchronizer. Using the StateSynchronizer, a developer can use Pravega to build synchronized shared state between multiple processes. This primitive can be used to build all sorts of distributed computing solutions such as shared configuration, leader election, etc. See the \"Distributed Computing Primitive\" section in Key Features for more details. What hardware do you recommend for Pravega? The Segment Store requires faster access to storage and more memory for its cache. It can run on 1 GB memory and 2 core CPU. 10 GB is a good start for storage. The Controller is less resource intensive, 1 CPU and 0.5 GB memory is a good start.","title":"Pravega FAQ"},{"location":"faq/#frequently-asked-questions","text":"What is Pravega? Pravega is an open source storage primitive implementing Streams for continuous and unbounded data. See here for more definitions of terms related to Pravega. What does \"Pravega\" mean? \"Pravega\" is a word from Sanskrit referring to \"good speed\". Is Pravega similiar to systems such as Kafka and Kinesis? Pravega is built from the ground up as an enterprise grade storage system to support features such as exactly once, durability etc. Pravega is an ideal store for streaming data, data from real-time applications and IoT data. How can I participate in open source? Disruptive innovation is accelerated by open source. When Pravega was created, there was no question it made sense to make it open source. We welcome contributions from experienced and new developers alike. Check out the code in Github . More detail about how to get involved can be found here . How do I get started? Read the Getting Started guide for more information, and also visit sample-apps repo for some sample applications. I am stuck. Where can I get help? Don\u2019t hesitate to ask! Contact the developers and community on the mailing lists if you need any help. See Join the Community for more details. Does Pravega support exactly once semantics? Absolutely. See Key Features for a discussion on how Pravega supports exactly once semantics. How does Pravega work with stream processors such as Apache Flink? So many features of Pravega make it ideal for stream processors. First, Pravega comes out of the box with a Flink connector. Critically, Pravega provides exactly once semantics, making it much easier to develop accurate stream processing applications. The combination of exactly once semantics, durable storage and transactions makes Pravega an ideal way to chain Flink jobs together, providing end-end consistency and exactly once semantics. See here for a list of key features of Pravega. How does auto scaling work between stream processors and Flink Auto scaling is a feature of Pravega where the number of segments in a stream changes based on the ingestion rate of data. If data arrives at a faster rate, Pravega increases the capacity of a stream by adding segments. When the data rate falls, Pravega can reduce capacity of a stream. As Pravega scales up and down the capacity of a stream, applications, such as a Flink job can observe this change and respond by adding or reducing the number of job instances consuming the stream. See the \"Auto Scaling\" section in Key Features for more discussion of auto scaling. What consistency guarantees does Pravega provide? Pravega makes several guarantees. Durability - once data is acknowledged to a client, Pravega guarantees it is protected. Ordering - events with the same routing key will always be read in the order they were written. Exactly once - data written to Pravega will not be duplicated. Why is supporting consistency and durability so important for storage systems such as Pravega? Primarily because it makes building applications easier. Consistency and durability are key for supporting exactly once semantics. Without exactly once semantics, it is difficult to build fault tolerant applications that consistency produce accurate results. See Key Features for a discussion on consistency and durability guarantees play a role in Pravega's support of exactly once semantics. Does Pravega support transactions? Yes. The Pravega API allows an application to create a transaction on a stream and write data to the transaction. The data is durably stored, just like any other data written to Pravega. When the application chooses, it can commit or abort the transaction. When a transaction is committed, the data in the transaction is atomically appended to the stream. See here for more details on Pravega's transaction support. Does Pravega support transactions across different routing keys? Yes. A transaction in Pravega is itself a stream; it can have 1 or more segments and data written to the transaction is placed into the segment associated with the data's routing key. When the transaction is committed, the transaction data is appended to the appropriate segment in the stream. Do I need HDFS installed in order to use Pravega? Yes. Normally, you would deploy an HDFS for Pravega to use as its Tier 2 storage. However, for simple test/dev environments, the so-called standAlone version of Pravega provides its own simulated HDFS. See the Running Pravega guide for more details. Which Tier 2 storage systems does Pravega support? Pravega is designed to support various types of Tier 2 storage systems. Currently we have implemented HDFS as the first embodiment of Tier 2 storage. What distributed computing primitives does Pravega provide? Pravega provides an API construct called StateSynchronizer. Using the StateSynchronizer, a developer can use Pravega to build synchronized shared state between multiple processes. This primitive can be used to build all sorts of distributed computing solutions such as shared configuration, leader election, etc. See the \"Distributed Computing Primitive\" section in Key Features for more details. What hardware do you recommend for Pravega? The Segment Store requires faster access to storage and more memory for its cache. It can run on 1 GB memory and 2 core CPU. 10 GB is a good start for storage. The Controller is less resource intensive, 1 CPU and 0.5 GB memory is a good start.","title":"Frequently Asked Questions"},{"location":"getting-started/","text":"Getting Started \u00b6 The best way to get to know Pravega is to start it up and run a sample Pravega application. Running Pravega is Simple \u00b6 Verify the following prerequisite Java 8 Download Pravega Download the Pravega release from the Github Releases . If you prefer to build Pravega yourself, you can download the code and run ./gradlew distribution . More details are shown in the Pravega README . $ tar xfvz pravega-<version>.tgz Run Pravega in standalone mode This launches all the components of Pravega on your local machine. Note: This is for testing/demo purposes only, do not use this mode of deployment in Production! More options and additional ways to run Pravega can be found in Running Pravega guide. $ cd pravega-<version> $ bin/pravega-standalone The command above runs Pravega locally for development and testing purposes. It does not persist in the storage tiers like we do with a real deployment of Pravega and as such you shouldn't expect it to recover from crashes, and further, not rely on it for production use. For production use, we strongly encourage a full deployment of Pravega. Running a sample Pravega Application \u00b6 We have developed a few samples to introduce the developer to coding with Pravega here: Pravega Samples . Download and run the \"Hello World\" Pravega sample reader and writer applications. Pravega dependencies will be pulled from maven central. Note: The samples can also use a locally compiled version of Pravega. For more information, please see the README note on maven publishing. Download the Pravega-Samples git repo $ git clone https://github.com/pravega/pravega-samples $ cd pravega-samples Generate the scripts to run the applications $ ./gradlew installDist Run the sample \"HelloWorldWriter\" This runs a simple Java application that writes a \"hello world\" message as an event into a Pravega stream. $ cd pravega-samples/pravega-client-examples/build/install/pravega-client-examples $ bin/helloWorldWriter Example HelloWorldWriter output ... Writing message: 'hello world' with routing-key: 'helloRoutingKey' to stream 'examples / helloStream' ... See the README file in the standalone-examples for more details on running the HelloWorldWriter with different parameters. Run the sample \"HelloWorldReader\" $ cd pravega-samples/pravega-client-examples/build/install/pravega-client-examples $ bin/helloWorldReader Example HelloWorldReader output ... Reading all the events from examples/helloStream ... Read event 'hello world' No more events from examples/helloStream ... See the README file in the pravega-client-examples for more details on running the HelloWorldReader application.","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"The best way to get to know Pravega is to start it up and run a sample Pravega application.","title":"Getting Started"},{"location":"getting-started/#running-pravega-is-simple","text":"Verify the following prerequisite Java 8 Download Pravega Download the Pravega release from the Github Releases . If you prefer to build Pravega yourself, you can download the code and run ./gradlew distribution . More details are shown in the Pravega README . $ tar xfvz pravega-<version>.tgz Run Pravega in standalone mode This launches all the components of Pravega on your local machine. Note: This is for testing/demo purposes only, do not use this mode of deployment in Production! More options and additional ways to run Pravega can be found in Running Pravega guide. $ cd pravega-<version> $ bin/pravega-standalone The command above runs Pravega locally for development and testing purposes. It does not persist in the storage tiers like we do with a real deployment of Pravega and as such you shouldn't expect it to recover from crashes, and further, not rely on it for production use. For production use, we strongly encourage a full deployment of Pravega.","title":"Running Pravega is Simple"},{"location":"getting-started/#running-a-sample-pravega-application","text":"We have developed a few samples to introduce the developer to coding with Pravega here: Pravega Samples . Download and run the \"Hello World\" Pravega sample reader and writer applications. Pravega dependencies will be pulled from maven central. Note: The samples can also use a locally compiled version of Pravega. For more information, please see the README note on maven publishing. Download the Pravega-Samples git repo $ git clone https://github.com/pravega/pravega-samples $ cd pravega-samples Generate the scripts to run the applications $ ./gradlew installDist Run the sample \"HelloWorldWriter\" This runs a simple Java application that writes a \"hello world\" message as an event into a Pravega stream. $ cd pravega-samples/pravega-client-examples/build/install/pravega-client-examples $ bin/helloWorldWriter Example HelloWorldWriter output ... Writing message: 'hello world' with routing-key: 'helloRoutingKey' to stream 'examples / helloStream' ... See the README file in the standalone-examples for more details on running the HelloWorldWriter with different parameters. Run the sample \"HelloWorldReader\" $ cd pravega-samples/pravega-client-examples/build/install/pravega-client-examples $ bin/helloWorldReader Example HelloWorldReader output ... Reading all the events from examples/helloStream ... Read event 'hello world' No more events from examples/helloStream ... See the README file in the pravega-client-examples for more details on running the HelloWorldReader application.","title":"Running a sample Pravega Application"},{"location":"javadoc/","text":"Java API Reference \u00b6 Clients \u00b6 A Writer is a client that creates Events and publishes them into Streams. A Reader is a client that Consumes events from Streams. We provide a Java library, which implements a convenient API for Writer and Reader applications to use. The client library encapsulates the wire protocol that is used to convey requests and responses between Pravega Clients and the Pravega service. Writer and Reader API","title":"Java API Reference"},{"location":"javadoc/#java-api-reference","text":"","title":"Java API Reference"},{"location":"javadoc/#clients","text":"A Writer is a client that creates Events and publishes them into Streams. A Reader is a client that Consumes events from Streams. We provide a Java library, which implements a convenient API for Writer and Reader applications to use. The client library encapsulates the wire protocol that is used to convey requests and responses between Pravega Clients and the Pravega service. Writer and Reader API","title":"Clients"},{"location":"join-community/","text":"Join the Pravega Community \u00b6 Slack Channel User Groups \u00b6 pravega-users@googlegroups.com Developer Mailing List \u00b6 pravega-dev@googlegroups.com","title":"Join the Community"},{"location":"join-community/#join-the-pravega-community","text":"Slack Channel","title":"Join the Pravega Community"},{"location":"join-community/#user-groups","text":"pravega-users@googlegroups.com","title":"User Groups"},{"location":"join-community/#developer-mailing-list","text":"pravega-dev@googlegroups.com","title":"Developer Mailing List"},{"location":"key-features/","text":"Pravega Key Features \u00b6 This document explains some of the key features of Pravega. It may be advantageous if you are already familiar with the core Pravega Concepts . Pravega Design Principles \u00b6 Pravega was designed to support the new generation of streaming applications. Applications that deal with a large amount of data arriving continuously that needs to generate an accurate analysis of that data by considering the factors like: Delayed data, Data arriving out of order, Failure conditions. There are several Open Source tools to enable developers to build such applications, including Apache Flink , Apache Beam , Spark Streaming , etc. These applications uses the following systems to ingest and store data: Apache Kafka , Apache ActiveMQ , RabbitMQ , Apache Cassandra , Apache HDFS . Pravega focuses on both ingesting and storing Stream data. Pravega approaches streaming applications from a storage perspective. It enables applications to ingest Stream data continuously and stores it permanently. Such Stream data can be accessed with low latency (order of milliseconds) and also analyzes historical data. The design of Pravega incorporates lessons learned from using the Lambda Architecture to build streaming applications and the challenges to deploy streaming applications at scale that consistently deliver accurate results in a fault tolerant manner. The Pravega Architecture provides strong durability and consistency guarantees, delivering a rock solid foundation to build streaming applications upon. With the Lambda Architecture, the developer uses a complex combination of middleware tools that include batch style middleware mainly influenced by Hadoop and continuous processing tools like Storm, Samza, Kafka and others. In this architecture, batch processing is used to deliver accurate, but potentially out of date analysis of data. The second path processes data as it is ingested, and in principle the results are inaccurate, which justifies the first batch path. The programming models of the speed layer are different than those used in the batch layer. An implementation of the Lambda Architecture can be difficult to maintain and manage in production. This style of big data application design consequently has been losing traction. A different kind of architecture has been gaining traction recently that does not rely on a batch processing data path. This architecture is called Kappa . The Kappa Architecture style is a reaction to the complexity of the Lambda Architecture and relies on components that are designed for streaming, supporting stronger semantics and delivering both fast and accurate data analysis. The Kappa Architecture provides a simpler approach: There is only one data path to execute, and one implementation of the application logic to maintain. With the right tools, built for the demands of processing streaming data in a fast and accurate fashion, it becomes simpler to design and run applications in the space of IoT:(connected cars, finance, risk management, online services, etc.). Using the right tools, it is possible to build such pipelines and serve applications that present high volume and demand low latency. Applications often require more than one stage of processing. Any practical system for stream analytics must be able to accommodate the composition of stages in the form of data pipelines: With data pipelines, it is important to think of guarantees end-to-end rather than on a per component basis. Our goal in Pravega is to enable the design and implementation of data pipelines with strong guarantees end-to-end. Pravega: Storage Reimagined for a Streaming World \u00b6 Pravega introduces a new storage primitive, a Stream, that matches continuous processing of unbounded data. In Pravega, a Stream is a named, durable, append-only and unbounded sequence of bytes. With this primitive, and the key features discussed in this document, Pravega is an ideal component to combine with Stream processing engines such as Flink to build streaming applications. Because of Pravega's key features, we imagine that it will be the fundamental storage primitive for a new generation of streaming-oriented middleware. Let's examine the key features of Pravega: Exactly Once Semantics \u00b6 By exactly once semantics we mean that Pravega ensures that data is not duplicated and no event is missed despite failures. Of course, this statement comes with a number of caveats, like any other system that promises exactly-once semantics, but let's not dive into the gory details here. An important consideration is that exactly-once semantics is a natural part of Pravega and has been a goal and part of the design from day zero. To achieve exactly once semantics, Pravega Streams are durable, ordered, consistent and transactional . We discuss durable and transactional in separate sections below. Pravega Streams are Ordered \u00b6 By ordering, we mean that data is observed by Readers in the order it is written. In Pravega, data is written along with an application-defined Routing Key. Pravega makes ordering guarantees in terms of Routing Keys. For example, two Events with the same Routing Key will always be read by a Reader in the order they were written. Pravega's ordering guarantees allow data reads to be replayed (e.g. when applications crash) and the results of replaying the reads will be the same. By consistency, we mean all Readers see the same ordered view of data for a given Routing Key, even in the face of failure. Systems that are \"mostly consistent\" are not sufficient for building accurate data processing. Systems that provide \"at least once\" semantics might present duplication. In such systems, a data producer might write the same data twice in some scenarios. In Pravega, writes are idempotent, rewrites done as a result of reconnection don't result in data duplication. Note that we make no guarantee when the data coming from the source already contains duplicates. Written data is opaque to Pravega and it makes no attempt to remove existing duplicates. Pravega has not limited the focus to exactly-once semantics for writing, however. We also provide, and are actively working on extending the features, that enable exactly-once end-to-end for a data pipeline. The strong consistency guarantees that the Pravega store provides along with the semantics of a data analytics engine like Flink enables such end-to-end guarantees. Auto Scaling \u00b6 Unlike systems with static partitioning, Pravega can automatically scale individual data streams to accommodate changes in data ingestion rate. Imagine an IoT application with millions of devices feeding thousands of data streams with information about those devices. Imagine a pipeline of Flink jobs that process those Streams to derive business value from all that raw IoT data: Predicting device failures, Optimizing service delivery through those devices, Tailoring a customer's experience when interacting with those devices. Building such an application at scale is difficult without having the components be able to scale automatically as the rate of data increases and decreases. With Pravega, it is easy to elastically and independently scale data ingestion, storage and processing \u2013 orchestrating the scaling of every component in a data pipeline. Pravega's support of Auto Scaling starts with the idea that Streams are partitioned into Stream Segments. A Stream may have one or more Stream Segments; recall that a Stream Segment is a partition of the Stream associated with a range of Routing Keys. Any data written into the Stream is written to the Stream Segment associated with the data's Routing Key. Writers use domain specific meaningful Routing Keys (like customer ID, Timestamp, Machine ID, etc.) to group similar together. A Stream Segment is the fundamental unit of parallelism in Pravega Streams. Parallel Writes: A Stream with multiple Stream Segments can support more parallelism of data writes; multiple Writers writing data into the different Stream Segments potentially involving all the Pravega Servers in the cluster. Parallel reads: On the Reader side, the number of Stream Segments represents the maximum degree of read parallelism possible. If a Stream has N Stream Segments, then a Reader Group with N Readers can consume from the Stream in parallel. Increase the number of Stream Segments, you can increase the number of Readers in the Reader Group to increase the scale of processing the data from that Stream. And of course if the number of Stream Segments decreases, it would be a good idea to reduce the number of Readers. A Stream can be configured to grow the number of Stream Segments as more data is written to the Stream, and to shrink when data volume drops off. We refer to this configuration as the Stream's Service Level Objective or SLO. Pravega monitors the rate of data input to the Stream and uses the SLO to add or remove Stream Segments from a Stream. Segments are added by splitting a Segment. Segments are removed by merging two Segments. See Auto Scaling for more detail on how Pravega manages Stream Segments. It is possible to coordinate the Auto Scaling of Streams in Pravega with application scale out (in the works). Using metadata available from Pravega, applications can configure the scaling of their application components; for example, to drive the number of instances of a Flink job. Alternatively, you could use software such as Cloud Foundry , Mesos/Marathon , Kubernetes or the Docker stack to deploy new instances of an application to react to increased parallelism at the Pravega level, or to terminate instances as Pravega scales down in response to reduced rate of data ingestion. Distributed Computing Primitive \u00b6 Pravega is great for distributed applications, such as microservices; it can be used as a data storage mechanism, for messaging between microservices and for other distributed computing services such as leader election. State Synchronizer, a part of the Pravega API, is the basis of sharing state across a cluster with consistency and optimistic concurrency. State Synchronizer is based on a fundamental conditional write operation in Pravega, so that data is written only if it would appear at a given position in the Stream. If a conditional write operation cannot meet the condition, it fails. State Synchronizer is therefore a strong synchronization primitive that can be used for shared state in a cluster, membership management, leader election and other distributed computing scenarios. For more information, refer to State Synchronizer . Write Efficiency \u00b6 Pravega write latency is of the order of milliseconds. It seamlessly scales to handle high throughput reads and writes from thousands of concurrent clients, making it ideal for IoT and other time sensitive applications. Streams are light weight, Pravega can support millions of Streams, this frees the application from worrying about static configuration of Streams and preallocating a small fixed number of Streams and limiting Stream resource. Write operations in Pravega are low latency, under 10ms to return an acknowledgment is returned to a Writer. Furthermore, writes are optimized so that I/O throughput is limited by network bandwidth; the persistence mechanism is not the bottleneck. Pravega uses Apache BookKeeper to persist all write operations. BookKeeper persists and protects the data very efficiently. Because data is protected before the write operation is acknowledged to the Writer, data is always durable. As we discuss below, data durability is a fundamental characteristic of a storage primitive. To add further efficiency, writes to BookKeeper often involve data from multiple Stream Segments, so the cost of persisting data to disk can be amortized over several write operations. There is no durability performance trade-off with Pravega. Read Efficiency \u00b6 A Reader can read from a Stream either at the tail of the Stream or at any part of the Stream's history. Unlike some log-based systems that use the same kind of storage for tail reads and writes as well as reads to historical data, Pravega uses two types of storage. The tail of the Stream is in so-called Tier 1 storage . The historical part of the Stream is in Tier 2 Storage . Pravega uses efficient in-memory read ahead cache, taking advantage of the fact that Streams are usually read in large contiguous chunks and that HDFS is well suited for those sort of large, high-throughput reads. It is also worth noting that tail reads do not impact the performance of writes. Unlimited Retention \u00b6 Data in Streams can be retained based on the application needs. It is constrained to the amount of data available, which is unbounded given the use of cloud storage in Tier 2. Pravega provides one convenient API to access both real-time and historical data . With Pravega, batch and real-time applications can both be handled efficiently; yet another reason why Pravega is a great storage primitive for Kappa architectures . If there is a value to retain old data, why not keep it around? For example, in a machine learning example, you may want to periodically change the model and train the new version of the model against as much historical data as possible to enhance and yield more accurate predictive power of the model. With Pravega auto-tiering, retaining lots of historical data does not affect the performance of tail reads and writes. Size of a stream is not limited by the storage capacity of a single server, but rather, it is limited only by the storage capacity of your storage cluster or cloud provider. As cost of storage decreases, the economic incentive to delete data goes away. Storage Efficiency \u00b6 Use Pravega to build pipelines of data processing, combining batch, real-time and other applications without duplicating data for every step of the pipeline. Consider the following data processing environment: Real time processing using Spark, Flink, and or Storm Batch processing using Hadoop Full text search can be performed using Lucene-based or Search mechanism like Elastic Search. Micro-services apps can be supported using one (or several) NoSQL databases. Using traditional approaches, one set of source data, for example, sensor data from an IoT app, would be ingested and replicated separately by each system. You would end up with three replicas of the data protected in the pub/sub system, three copies in HDFS, three copies in Lucene and three copies in the NoSQL database. When we consider the source data is measured in TB, the cost of data replication separated by middleware category becomes prohibitively expensive. Consider the same pipeline using Pravega and middleware adapted to use Pravega for its storage: With Pravega, the data is ingested and protected in one place; Pravega provides the single source of truth for the entire pipeline. Furthermore, with the bulk of the data being stored in Tier 2 enabled with erasure coding to efficiently protect the data, the storage cost of the data is substantially reduced. Durability \u00b6 With Pravega, you don't face a compromise between performance, durability and consistency. Pravega provides durable storage of streaming data with strong consistency, ordering guarantees and great performance. Durability is a fundamental storage primitive requirement. Storage that could lose data is not reliable storage. Systems based on such storage are not production quality. Once a write operation is acknowledged, the data will never be lost, even when failures occur. This is because Pravega always saves data in protected, persistent storage before the write operation returns to the Writer. With Pravega, data in the Stream is protected. A Stream can be treated as a system of record, just as you would treat data stored in databases or files. Transaction Support \u00b6 A developer uses a Pravega Transaction to ensure that a set of events are written to a Stream atomically. A Pravega Transaction is part of Pravega's Writer API. Data can be written to a Stream directly through the API, or an application can write data through a Transaction. With Transactions, a Writer can persist data now, and later decide whether the data should be appended to a Stream or abandoned. Using a Transaction, data is written to the Stream only when the Transaction is committed. When the Transaction is committed, all data written to the Transaction is atomically appended to the Stream. Because Transactions are implemented in the same way as Stream Segments, data written to a Transaction is just as durable as data written directly to a Stream. If a Transaction is abandoned (e.g. if the Writer crashes) the Transaction is aborted and all data is discarded. Of course, an application can choose to abort the Transaction through the API if a condition occurs that suggests the Writer should discard the data. Transactions are key to chaining Flink jobs together. When a Flink job uses Pravega as a sink, it can begin a Transaction, and if it successfully finishes processing, commit the Transaction, writing the data into its Pravega based sink. If the job fails for some reason, the Transaction times out and data is not written. When the job is restarted, there is no \"partial result\" in the sink that would need to be managed or cleaned up. Combining Transactions and other key features of Pravega, it is possible to chain Flink jobs together, having one job's Pravega based sink be the source for a downstream Flink job. This provides the ability for an entire pipeline of Flink jobs to have end-to-end exactly once, guaranteed ordering of data processing. Of course, it is possible for Transactions across multiple Streams be coordinated with Transactions, so that a Flink job can use two or more Pravega based sinks to provide source input to downstream Flink jobs. In addition, it is possible for application logic to coordinate Pravega Transactions with external databases such as Flink's checkpoint store. For more information, see Transaction section.","title":"Key Features"},{"location":"key-features/#pravega-key-features","text":"This document explains some of the key features of Pravega. It may be advantageous if you are already familiar with the core Pravega Concepts .","title":"Pravega Key Features"},{"location":"key-features/#pravega-design-principles","text":"Pravega was designed to support the new generation of streaming applications. Applications that deal with a large amount of data arriving continuously that needs to generate an accurate analysis of that data by considering the factors like: Delayed data, Data arriving out of order, Failure conditions. There are several Open Source tools to enable developers to build such applications, including Apache Flink , Apache Beam , Spark Streaming , etc. These applications uses the following systems to ingest and store data: Apache Kafka , Apache ActiveMQ , RabbitMQ , Apache Cassandra , Apache HDFS . Pravega focuses on both ingesting and storing Stream data. Pravega approaches streaming applications from a storage perspective. It enables applications to ingest Stream data continuously and stores it permanently. Such Stream data can be accessed with low latency (order of milliseconds) and also analyzes historical data. The design of Pravega incorporates lessons learned from using the Lambda Architecture to build streaming applications and the challenges to deploy streaming applications at scale that consistently deliver accurate results in a fault tolerant manner. The Pravega Architecture provides strong durability and consistency guarantees, delivering a rock solid foundation to build streaming applications upon. With the Lambda Architecture, the developer uses a complex combination of middleware tools that include batch style middleware mainly influenced by Hadoop and continuous processing tools like Storm, Samza, Kafka and others. In this architecture, batch processing is used to deliver accurate, but potentially out of date analysis of data. The second path processes data as it is ingested, and in principle the results are inaccurate, which justifies the first batch path. The programming models of the speed layer are different than those used in the batch layer. An implementation of the Lambda Architecture can be difficult to maintain and manage in production. This style of big data application design consequently has been losing traction. A different kind of architecture has been gaining traction recently that does not rely on a batch processing data path. This architecture is called Kappa . The Kappa Architecture style is a reaction to the complexity of the Lambda Architecture and relies on components that are designed for streaming, supporting stronger semantics and delivering both fast and accurate data analysis. The Kappa Architecture provides a simpler approach: There is only one data path to execute, and one implementation of the application logic to maintain. With the right tools, built for the demands of processing streaming data in a fast and accurate fashion, it becomes simpler to design and run applications in the space of IoT:(connected cars, finance, risk management, online services, etc.). Using the right tools, it is possible to build such pipelines and serve applications that present high volume and demand low latency. Applications often require more than one stage of processing. Any practical system for stream analytics must be able to accommodate the composition of stages in the form of data pipelines: With data pipelines, it is important to think of guarantees end-to-end rather than on a per component basis. Our goal in Pravega is to enable the design and implementation of data pipelines with strong guarantees end-to-end.","title":"Pravega Design Principles"},{"location":"key-features/#pravega-storage-reimagined-for-a-streaming-world","text":"Pravega introduces a new storage primitive, a Stream, that matches continuous processing of unbounded data. In Pravega, a Stream is a named, durable, append-only and unbounded sequence of bytes. With this primitive, and the key features discussed in this document, Pravega is an ideal component to combine with Stream processing engines such as Flink to build streaming applications. Because of Pravega's key features, we imagine that it will be the fundamental storage primitive for a new generation of streaming-oriented middleware. Let's examine the key features of Pravega:","title":"Pravega: Storage Reimagined for a Streaming World"},{"location":"key-features/#exactly-once-semantics","text":"By exactly once semantics we mean that Pravega ensures that data is not duplicated and no event is missed despite failures. Of course, this statement comes with a number of caveats, like any other system that promises exactly-once semantics, but let's not dive into the gory details here. An important consideration is that exactly-once semantics is a natural part of Pravega and has been a goal and part of the design from day zero. To achieve exactly once semantics, Pravega Streams are durable, ordered, consistent and transactional . We discuss durable and transactional in separate sections below.","title":"Exactly Once Semantics"},{"location":"key-features/#pravega-streams-are-ordered","text":"By ordering, we mean that data is observed by Readers in the order it is written. In Pravega, data is written along with an application-defined Routing Key. Pravega makes ordering guarantees in terms of Routing Keys. For example, two Events with the same Routing Key will always be read by a Reader in the order they were written. Pravega's ordering guarantees allow data reads to be replayed (e.g. when applications crash) and the results of replaying the reads will be the same. By consistency, we mean all Readers see the same ordered view of data for a given Routing Key, even in the face of failure. Systems that are \"mostly consistent\" are not sufficient for building accurate data processing. Systems that provide \"at least once\" semantics might present duplication. In such systems, a data producer might write the same data twice in some scenarios. In Pravega, writes are idempotent, rewrites done as a result of reconnection don't result in data duplication. Note that we make no guarantee when the data coming from the source already contains duplicates. Written data is opaque to Pravega and it makes no attempt to remove existing duplicates. Pravega has not limited the focus to exactly-once semantics for writing, however. We also provide, and are actively working on extending the features, that enable exactly-once end-to-end for a data pipeline. The strong consistency guarantees that the Pravega store provides along with the semantics of a data analytics engine like Flink enables such end-to-end guarantees.","title":"Pravega Streams are Ordered"},{"location":"key-features/#auto-scaling","text":"Unlike systems with static partitioning, Pravega can automatically scale individual data streams to accommodate changes in data ingestion rate. Imagine an IoT application with millions of devices feeding thousands of data streams with information about those devices. Imagine a pipeline of Flink jobs that process those Streams to derive business value from all that raw IoT data: Predicting device failures, Optimizing service delivery through those devices, Tailoring a customer's experience when interacting with those devices. Building such an application at scale is difficult without having the components be able to scale automatically as the rate of data increases and decreases. With Pravega, it is easy to elastically and independently scale data ingestion, storage and processing \u2013 orchestrating the scaling of every component in a data pipeline. Pravega's support of Auto Scaling starts with the idea that Streams are partitioned into Stream Segments. A Stream may have one or more Stream Segments; recall that a Stream Segment is a partition of the Stream associated with a range of Routing Keys. Any data written into the Stream is written to the Stream Segment associated with the data's Routing Key. Writers use domain specific meaningful Routing Keys (like customer ID, Timestamp, Machine ID, etc.) to group similar together. A Stream Segment is the fundamental unit of parallelism in Pravega Streams. Parallel Writes: A Stream with multiple Stream Segments can support more parallelism of data writes; multiple Writers writing data into the different Stream Segments potentially involving all the Pravega Servers in the cluster. Parallel reads: On the Reader side, the number of Stream Segments represents the maximum degree of read parallelism possible. If a Stream has N Stream Segments, then a Reader Group with N Readers can consume from the Stream in parallel. Increase the number of Stream Segments, you can increase the number of Readers in the Reader Group to increase the scale of processing the data from that Stream. And of course if the number of Stream Segments decreases, it would be a good idea to reduce the number of Readers. A Stream can be configured to grow the number of Stream Segments as more data is written to the Stream, and to shrink when data volume drops off. We refer to this configuration as the Stream's Service Level Objective or SLO. Pravega monitors the rate of data input to the Stream and uses the SLO to add or remove Stream Segments from a Stream. Segments are added by splitting a Segment. Segments are removed by merging two Segments. See Auto Scaling for more detail on how Pravega manages Stream Segments. It is possible to coordinate the Auto Scaling of Streams in Pravega with application scale out (in the works). Using metadata available from Pravega, applications can configure the scaling of their application components; for example, to drive the number of instances of a Flink job. Alternatively, you could use software such as Cloud Foundry , Mesos/Marathon , Kubernetes or the Docker stack to deploy new instances of an application to react to increased parallelism at the Pravega level, or to terminate instances as Pravega scales down in response to reduced rate of data ingestion.","title":"Auto Scaling"},{"location":"key-features/#distributed-computing-primitive","text":"Pravega is great for distributed applications, such as microservices; it can be used as a data storage mechanism, for messaging between microservices and for other distributed computing services such as leader election. State Synchronizer, a part of the Pravega API, is the basis of sharing state across a cluster with consistency and optimistic concurrency. State Synchronizer is based on a fundamental conditional write operation in Pravega, so that data is written only if it would appear at a given position in the Stream. If a conditional write operation cannot meet the condition, it fails. State Synchronizer is therefore a strong synchronization primitive that can be used for shared state in a cluster, membership management, leader election and other distributed computing scenarios. For more information, refer to State Synchronizer .","title":"Distributed Computing Primitive"},{"location":"key-features/#write-efficiency","text":"Pravega write latency is of the order of milliseconds. It seamlessly scales to handle high throughput reads and writes from thousands of concurrent clients, making it ideal for IoT and other time sensitive applications. Streams are light weight, Pravega can support millions of Streams, this frees the application from worrying about static configuration of Streams and preallocating a small fixed number of Streams and limiting Stream resource. Write operations in Pravega are low latency, under 10ms to return an acknowledgment is returned to a Writer. Furthermore, writes are optimized so that I/O throughput is limited by network bandwidth; the persistence mechanism is not the bottleneck. Pravega uses Apache BookKeeper to persist all write operations. BookKeeper persists and protects the data very efficiently. Because data is protected before the write operation is acknowledged to the Writer, data is always durable. As we discuss below, data durability is a fundamental characteristic of a storage primitive. To add further efficiency, writes to BookKeeper often involve data from multiple Stream Segments, so the cost of persisting data to disk can be amortized over several write operations. There is no durability performance trade-off with Pravega.","title":"Write Efficiency"},{"location":"key-features/#read-efficiency","text":"A Reader can read from a Stream either at the tail of the Stream or at any part of the Stream's history. Unlike some log-based systems that use the same kind of storage for tail reads and writes as well as reads to historical data, Pravega uses two types of storage. The tail of the Stream is in so-called Tier 1 storage . The historical part of the Stream is in Tier 2 Storage . Pravega uses efficient in-memory read ahead cache, taking advantage of the fact that Streams are usually read in large contiguous chunks and that HDFS is well suited for those sort of large, high-throughput reads. It is also worth noting that tail reads do not impact the performance of writes.","title":"Read Efficiency"},{"location":"key-features/#unlimited-retention","text":"Data in Streams can be retained based on the application needs. It is constrained to the amount of data available, which is unbounded given the use of cloud storage in Tier 2. Pravega provides one convenient API to access both real-time and historical data . With Pravega, batch and real-time applications can both be handled efficiently; yet another reason why Pravega is a great storage primitive for Kappa architectures . If there is a value to retain old data, why not keep it around? For example, in a machine learning example, you may want to periodically change the model and train the new version of the model against as much historical data as possible to enhance and yield more accurate predictive power of the model. With Pravega auto-tiering, retaining lots of historical data does not affect the performance of tail reads and writes. Size of a stream is not limited by the storage capacity of a single server, but rather, it is limited only by the storage capacity of your storage cluster or cloud provider. As cost of storage decreases, the economic incentive to delete data goes away.","title":"Unlimited Retention"},{"location":"key-features/#storage-efficiency","text":"Use Pravega to build pipelines of data processing, combining batch, real-time and other applications without duplicating data for every step of the pipeline. Consider the following data processing environment: Real time processing using Spark, Flink, and or Storm Batch processing using Hadoop Full text search can be performed using Lucene-based or Search mechanism like Elastic Search. Micro-services apps can be supported using one (or several) NoSQL databases. Using traditional approaches, one set of source data, for example, sensor data from an IoT app, would be ingested and replicated separately by each system. You would end up with three replicas of the data protected in the pub/sub system, three copies in HDFS, three copies in Lucene and three copies in the NoSQL database. When we consider the source data is measured in TB, the cost of data replication separated by middleware category becomes prohibitively expensive. Consider the same pipeline using Pravega and middleware adapted to use Pravega for its storage: With Pravega, the data is ingested and protected in one place; Pravega provides the single source of truth for the entire pipeline. Furthermore, with the bulk of the data being stored in Tier 2 enabled with erasure coding to efficiently protect the data, the storage cost of the data is substantially reduced.","title":"Storage Efficiency"},{"location":"key-features/#durability","text":"With Pravega, you don't face a compromise between performance, durability and consistency. Pravega provides durable storage of streaming data with strong consistency, ordering guarantees and great performance. Durability is a fundamental storage primitive requirement. Storage that could lose data is not reliable storage. Systems based on such storage are not production quality. Once a write operation is acknowledged, the data will never be lost, even when failures occur. This is because Pravega always saves data in protected, persistent storage before the write operation returns to the Writer. With Pravega, data in the Stream is protected. A Stream can be treated as a system of record, just as you would treat data stored in databases or files.","title":"Durability"},{"location":"key-features/#transaction-support","text":"A developer uses a Pravega Transaction to ensure that a set of events are written to a Stream atomically. A Pravega Transaction is part of Pravega's Writer API. Data can be written to a Stream directly through the API, or an application can write data through a Transaction. With Transactions, a Writer can persist data now, and later decide whether the data should be appended to a Stream or abandoned. Using a Transaction, data is written to the Stream only when the Transaction is committed. When the Transaction is committed, all data written to the Transaction is atomically appended to the Stream. Because Transactions are implemented in the same way as Stream Segments, data written to a Transaction is just as durable as data written directly to a Stream. If a Transaction is abandoned (e.g. if the Writer crashes) the Transaction is aborted and all data is discarded. Of course, an application can choose to abort the Transaction through the API if a condition occurs that suggests the Writer should discard the data. Transactions are key to chaining Flink jobs together. When a Flink job uses Pravega as a sink, it can begin a Transaction, and if it successfully finishes processing, commit the Transaction, writing the data into its Pravega based sink. If the job fails for some reason, the Transaction times out and data is not written. When the job is restarted, there is no \"partial result\" in the sink that would need to be managed or cleaned up. Combining Transactions and other key features of Pravega, it is possible to chain Flink jobs together, having one job's Pravega based sink be the source for a downstream Flink job. This provides the ability for an entire pipeline of Flink jobs to have end-to-end exactly once, guaranteed ordering of data processing. Of course, it is possible for Transactions across multiple Streams be coordinated with Transactions, so that a Flink job can use two or more Pravega based sinks to provide source input to downstream Flink jobs. In addition, it is possible for application logic to coordinate Pravega Transactions with external databases such as Flink's checkpoint store. For more information, see Transaction section.","title":"Transaction Support"},{"location":"metrics/","text":"Pravega Metrics \u00b6 Metrics Interfaces and Examples Usage Metrics Service Provider \u2014 Interface StatsProvider Metric Logger \u2014 Interface StatsLogger Metric Sub Logger \u2014 OpStatsLogger Metric Logger \u2014 Interface DynamicLogger Example for Starting a Metric Service Example for Dynamic Counter and OpStatsLogger(Timer) Example for Dynamic Gauge Example for Dynamic Meter Metric Registries and Configurations Creating Own Metrics Metrics Naming Conventions Available Metrics and Their Names Metrics in JVM Metrics in Segment Store Service Metrics in Controller Service Resources In the Pravega Metrics Framework, we use Micrometer Metrics as the underlying library, and provide our own API to make it easier to use. Metrics Interfaces and Examples Usage \u00b6 StatsProvider : The Statistics Provider which provides the whole Metric service. StatsLogger : The Statistics Logger is where the required Metrics ( Counter / Gauge / Timer / Distribution Summary ) are registered. OpStatsLogger : The Operation Statistics Logger is a sub-metric for the complex ones ( Timer / Distribution Summary ). It is included in StatsLogger and DynamicLogger . Metrics Service Provider \u2014 Interface StatsProvider \u00b6 Pravega Metric Framework is initiated using the StatsProvider interface: it provides the start and stop methods for the Metric service. It also provides startWithoutExporting() for testing purpose, which only stores metrics in memory without exporting them to external systems. Currently we have support for StatsD and InfluxDB registries. StatsProvider start() : Initializes the MetricRegistry and Reporters for our Metric service. startWithoutExporting() : Initializes SimpleMeterRegistry that holds the latest value of each Meter in memory and does not export the data anywhere, typically for unit tests. close() : Shuts down the Metric Service. createStatsLogger() : Create a StatsLogger instance which is used to register and return metric objects. Application code could then perform metric operations directly with the returned metric objects. createDynamicLogger() : Creates a Dynamic Logger. Metric Logger \u2014 Interface StatsLogger \u00b6 This interface can be used to register the required metrics for simple types like Counter and Gauge and some complex statistics type of Metric like OpStatsLogger , through which we provide Timer and Distribution Summary . StatsLogger createStats() : Register and get a OpStatsLogger , which is used for complex type of metrics. Notice the optional metric tags. createCounter() : Register and get a Counter Metric. createMeter() : Create and register a Meter Metric. registerGauge() : Register a Gauge Metric. createScopeLogger() : Create the StatsLogger under the given scope name. Metric Sub Logger \u2014 OpStatsLogger \u00b6 OpStatsLogger can be used if the user is interested in measuring the latency of operations like CreateSegment and ReadSegment . Further, we could use it to record the number of operation and time/duration of each operation. OpStatsLogger reportSuccessEvent() : Used to track the Timer of a successful operation and will record the latency in nanoseconds in the required metric. reportFailEvent() : Used to track the Timer of a failed operation and will record the latency in nanoseconds in required metric. reportSuccessValue() : Used to track the Histogram of a success value. reportFailValue() : Used to track the Histogram of a failed value. toOpStatsData() : Used to support the JMX Reporters and unit tests. clear : Used to clear the stats for this operation. Metric Logger \u2014 Interface DynamicLogger \u00b6 The following is an example of a simple interface that exposes only the simple type metrics: ( Counter / Gauge / Meter ). DynamicLogger incCounterValue() : Increases the Counter with the given value. Notice the optional metric tags. updateCounterValue() : Updates the Counter with the given value. freezeCounter() : Notifies that, the Counter will not be updated. reportGaugeValue() : Reports the Gauge value. freezeGaugeValue() : Notifies that, the Gauge value will not be updated. recordMeterEvents() : Records the occurrences of a given number of events in Meter . Example for Starting a Metric Service \u00b6 This example is from io.pravega.segmentstore.server.host.ServiceStarter . The code for this example can be found here . It starts Pravega Segment Store service and the Metrics Service is started as a sub service. Example for Dynamic Counter and OpStatsLogger(Timer) \u00b6 This is an example from io.pravega.segmentstore.server.host.stat.SegmentStatsRecorderImpl.java . The code for this example can be found here . In the class PravegaRequestProcessor , we have registered two metrics: one Timer ( createStreamSegment ) one dynamic counter ( dynamicLogger ) From the above example, we can see the required steps to register and use dynamic counter: Get a dynamic logger from MetricsProvider: DynamicLogger dynamicLogger = MetricsProvider.getDynamicLogger(); Increase the counter by providing metric base name and optional tags associated with the metric. DynamicLogger dl = getDynamicLogger(); dl.incCounterValue(globalMetricName(SEGMENT_WRITE_BYTES), dataLength); ... dl.incCounterValue(SEGMENT_WRITE_BYTES, dataLength, segmentTags(streamSegmentName)); Here SEGMENT_WRITE_BYTES is the base name of the metric. Below are the two metrics associated with it: The global Counter which has no tags associated. A Segment specific Counter which has a list of Segment tags associated. Note that, the segmentTags is a method to generate tags based on fully qualified Segment name. The following are the required steps to register and use OpStatsLogger(Timer) : Get a StatsLogger from MetricsProvider . StatsLogger STATS_LOGGER = MetricsProvider.getStatsLogger(\"segmentstore\"); 2. Register all the desired metrics through StatsLogger . @Getter(AccessLevel.PROTECTED) final OpStatsLogger createStreamSegment = STATS_LOGGER.createStats(SEGMENT_CREATE_LATENCY); 3. Use these metrics within code at the appropriate places where the values should be collected and recorded. getCreateStreamSegment().reportSuccessEvent(elapsed); Here SEGMENT_CREATE_LATENCY is the name of this metric, and createStreamSegment is the metric object, which tracks operations of createSegment and we will get the time (i.e. time taken by each operation and other numbers computed based on them) for each createSegment operation happened. Example for Dynamic Gauge \u00b6 This is an example from io.pravega.controller.metrics.StreamMetrics . In this class, we report a Dynamic Gauge which represents the open Transactions of a Stream. The code for this example can be found here . Example for Dynamic Meter \u00b6 This is an example from io.pravega.segmentstore.server.SegmentStoreMetrics . The code for this example can be found here . In the class SegmentStoreMetrics , we report a Dynamic Meter which represents the Segments created with a particular container. Metric Registries and Configurations \u00b6 With Micrometer, each meter registry is responsible for both storage and exporting of metrics objects. In order to have a unified interface, Micrometer provides the CompositeMeterRegistry for the application to interact with, CompositeMeterRegistry will forward metric operations to all the concrete registries bounded to it. Note that when metrics service start() , initially only a global registry (of type CompositeMeterRegistry ) is provided, which will bind concrete registries (e.g. statsD, Influxdb) based on the configurations. If no registry is switched on in config , metrics service throws error to prevent the global registry runs into no-op mode. Mainly for testing purpose, metrics service can also startWithoutExporting() , where a SimpleMeterRegistry is bound to the global registry. SimpleMeterRegistry holds memory only storage but does not export metrics, makes it ideal for tests to verify metrics objects. Currently Pravega supports the following: - StatsD registry in Telegraf flavor. - Dimensional metrics data model (or metric tags). - UDP as Communication protocol. - Direct InfluxDB connection. The reporter could be configured using the MetricsConfig . Please refer to the example . Creating Own Metrics \u00b6 When starting a Segment Store/Controller Service, start a Metric Service as a sub service. Please check ServiceStarter.start() public class AddMetrics { MetricsProvider . initialize ( Config . METRICS_CONFIG ); statsProvider . start ( metricsConfig ); statsProvider = MetricsProvider . getMetricsProvider (); statsProvider . start (); Create a new StatsLogger instance through the MetricsProvider.createStatsLogger(String loggerName) , and register metric using name, e.g. STATS_LOGGER.createCounter(String name) ; and then update the metric object as appropriately in the code. static final StatsLogger STATS_LOGGER = MetricsProvider . getStatsLogger (); // <--- 1 DynamicLogger dynamicLogger = MetricsProvider . getDynamicLogger (); static class Metrics { // < --- 2 //Using Stats Logger static final String CREATE_STREAM = \"stream_created\" ; static final OpStatsLogger CREATE_STREAM = STATS_LOGGER . createStats ( CREATE_STREAM ); static final String SEGMENT_CREATE_LATENCY = \"segmentstore.segment.create_latency_ms\" ; static final OpStatsLogger createStreamSegment = STATS_LOGGER . createStats ( SEGMENT_CREATE_LATENCY ); //Using Dynamic Logger static final String SEGMENT_READ_BYTES = \"segmentstore.segment.read_bytes\" ; //Dynamic Counter static final String OPEN_TRANSACTIONS = \"controller.transactions.opened\" ; //Dynamic Gauge ... } //to report success or increment Metrics . CREATE_STREAM . reportSuccessValue ( 1 ); // < --- 3 Metrics . createStreamSegment . reportSuccessEvent ( timer . getElapsed ()); dynamicLogger . incCounterValue ( Metrics . SEGMENT_READ_BYTES , 1 ); dynamicLogger . reportGaugeValue ( OPEN_TRANSACTIONS , 0 ); //in case of failure Metrics . CREATE_STREAM . reportFailValue ( 1 ); Metrics . createStreamSegment . reportFailEvent ( timer . getElapsed ()); //to freeze dynamicLogger . freezeCounter ( Metrics . SEGMENT_READ_BYTES ); dynamicLogger . freezeGaugeValue ( OPEN_TRANSACTIONS ); } Metrics Naming Conventions \u00b6 All metric names are in the following format: Metrics Prefix + Component Origin + Sub-Component (or Abstraction) + Metric Base Name 1. Metric Prefix : By default pravega is configurable. Component Origin : Indicates which component generates the metric, such as segmentstore , controller . Sub-Component (or Abstraction) : Indicates the second level component or abstraction, such as cache , transaction , storage . Metric Base Name : Indicates the read_latency_ms , create_count . For example: pravega.segmentstore.segment.create_latency_ms Following are some common combinations of component and sub-components (or abstractions) being used: segmentstore.segment : Metrics for individual Segments segmentstore.storage : Metrics related to long-term storage (Tier 2) segmentstore.bookkeeper : Metrics related to Bookkeeper (Tier 1) segmentstore.container : Metrics for Segment Containers segmentstore.thread_pool : Metrics for Segment Store thread pool segmentstore.cache : Cache-related metrics controller.stream : Metrics for operations on Streams (e.g., number of streams created) controller.segments : Metrics about Segments, per Stream (e.g., count, splits, merges) controller.transactions : Metrics related to Transactions (e.g., created, committed, aborted) controller.retention : Metrics related to data retention, per Stream (e.g., frequency, size of truncated data) controller.hosts : Metrics related to Pravega servers in the cluster (e.g., number of servers, failures) controller.container : Metrics related to Container lifecycle (e.g., failovers) Following are the two types of metrics: Global Metric : _global metrics are reporting global values per component (Segment Store or Controller) instance, and further aggregation logic is needed if looking for Pravega cluster globals. For instance, STORAGE_READ_BYTES can be classified as a Global metric. Object-based Metric : Sometimes, we need to report metrics only based on specific objects, such as Streams or Segments. This kind of metrics use metric name as a base name in the file and are \"dynamically\" created based on the objects to be measured. For instance, in CONTAINER_APPEND_COUNT we actually report multiple metrics, one per each containerId measured, with different container tag (e.g. [\"containerId\", \"3\"] ). There are cases in which we may want both a Global and Object-based versions for the same metric. For example, regarding SEGMENT_READ_BYTES we publish the Global version of it by adding _global suffix to the base name segmentstore.segment.read_bytes_global to track the globally total number of bytes read, as well as the per-segment version of it by using the same base name and also supplying additional Segment tags to report in a finer granularity the events read per Segment. segmentstore.segment.read_bytes, [\"scope\", \"...\", \"stream\", \"...\", \"segment\", \"...\", \"epoch\", \"...\"]) Available Metrics and Their Names \u00b6 Metrics in JVM \u00b6 jvm_gc_live_data_size jvm_gc_max_data_size jvm_gc_memory_allocated jvm_gc_memory_prompted jvm_gc_pause jvm_memory_committed jvm_memory_max jvm_memory_used jvm_threads_daemon jvm_threads_live jvm_threads_peak jvm_threads_states Metrics in Segment Store Service \u00b6 Segment Store Read/Write latency of storage operations ( Histograms ): ``` segmentstore.segment.create_latency_ms segmentstore.segment.read_latency_ms segmentstore.segment.write_latency_ms ``` Segment Store global and per-segment Read/Write Metrics ( Counters ): ``` // Global counters segmentstore.segment.read_bytes_global segmentstore.segment.write_bytes_global segmentstore.segment.write_events_global // Per segment counters - all with tags {\"scope\", $scope, \"stream\", $stream, \"segment\", $segment, \"epoch\", $epoch} segmentstore.segment.write_bytes segmentstore.segment.read_bytes segmentstore.segment.write_events ``` Segment Store cache Read/Write latency Metrics ( Histogram ): segmentstore.cache.insert_latency_ms segmentstore.cache.get_latency Segment Store cache Read/Write Metrics ( Counters ): segmentstore.cache.write_bytes segmentstore.cache.read_bytes Segment Store cache size ( Gauge ) and generation spread ( Histogram ) Metrics: segmentstore.cache.size_bytes segmentstore.cache.gen Tier 1 Storage DurableDataLog Read/Write latency and queuing Metrics ( Histogram ): segmentstore.bookkeeper.total_write_latency_ms segmentstore.bookkeeper.write_latency_ms segmentstore.bookkeeper.write_queue_size segmentstore.bookkeeper.write_queue_fill Tier 1 Storage DurableDataLog Read/Write ( Counter ) and per-container ledger count Metrics ( Gauge ): segmentstore.bookkeeper.write_bytes segmentstore.bookkeeper.bookkeeper_ledger_count - with tag {\"container\", $containerId} Tier 2 Storage Read/Write latency Metrics ( Histogram ): segmentstore.storage.read_latency_ms segmentstore.storage.write_latency_ms Tier 2 Storage Read/Write data and file creation Metrics ( Counters ): segmentstore.storage.read_bytes segmentstore.storage.write_bytes segmentstore.storage.create_count Segment Store container-specific operation Metrics: // Histograms - all with tags {\"container\", $containerId} segmentstore.container.process_operations.latency_ms segmentstore.container.process_operations.batch_size segmentstore.container.operation_queue.size segmentstore.container.operation_processor.in_flight segmentstore.container.operation_queue.wait_time segmentstore.container.operation_processor.delay_ms segmentstore.container.operation_commit.latency_ms segmentstore.container.operation.latency_ms segmentstore.container.operation_commit.metadata_txn_count segmentstore.container.operation_commit.memory_latency_ms // Gauge segmentstore.container.operation.log_size Segment Store operation processor ( Counter ) Metrics - all with tags {\"container\", $containerId}. // Counters/Meters segmentstore.container.append_count segmentstore.container.append_offset_count segmentstore.container.update_attributes_count segmentstore.container.get_attributes_count segmentstore.container.read_count segmentstore.container.get_info_count segmentstore.container.create_segment_count segmentstore.container.delete_segment_count segmentstore.container.merge_segment_count segmentstore.container.seal_count segmentstore.container.truncate_count Segment Store active Segments ( Gauge ) and thread pool status ( Histogram ) Metrics: // Gauge - with tags {\"container\", $containerId} segmentstore.active_segments // Histograms segmentstore.thread_pool.queue_size segmentstore.thread_pool.active_threads Metrics in Controller Service \u00b6 Controller Stream operation latency Metrics ( Histograms ): controller.stream.created_latency_ms controller.stream.sealed_latency_ms controller.stream.deleted_latency_ms controller.stream.updated_latency_ms controller.stream.truncated_latency_ms Controller global and per-Stream operation Metrics ( Counters ): controller.stream.created controller.stream.create_failed_global controller.stream.create_failed - with tags {\"scope\", $scope, \"stream\", $stream} controller.stream.sealed controller.stream.seal_failed_global controller.stream.seal_failed - with tags {\"scope\", $scope, \"stream\", $stream} controller.stream.deleted controller.stream.delete_failed_global controller.stream.delete_failed - with tags {\"scope\", $scope, \"stream\", $stream} controller.stream.updated_global controller.stream.updated - with tags {\"scope\", $scope, \"stream\", $stream} controller.stream.update_failed_global controller.stream.update_failed - with tags {\"scope\", $scope, \"stream\", $stream} controller.stream.truncated_global controller.stream.truncated - with tags {\"scope\", $scope, \"stream\", $stream} controller.stream.truncate_failed_global controller.stream.truncate_failed - with tags {\"scope\", $scope, \"stream\", $stream} Controller Stream retention frequency ( Counter ) and truncated size ( Gauge ) Metrics: controller.retention.frequency - with tags {\"scope\", $scope, \"stream\", $stream} controller.retention.truncated_size - with tags {\"scope\", $scope, \"stream\", $stream} Controller Stream Segment operations ( Counters ) and open/timed out Transactions on a Stream ( Gauge ) Metrics - all with tags {\"scope\", $scope, \"stream\", $stream}: controller.transactions.opened controller.transactions.timedout controller.segments.count controller.segment.splits controller.segment.merges Controller Transaction operation latency Metrics: controller.transactions.created_latency_ms controller.transactions.committed_latency_ms controller.transactions.aborted_latency_ms Controller Transaction operation counter Metrics: controller.transactions.created_global controller.transactions.created - with tags {\"scope\", $scope, \"stream\", $stream} controller.transactions.create_failed_global controller.transactions.create_failed - with tags {\"scope\", $scope, \"stream\", $stream} controller.transactions.committed_global controller.transactions.committed - with tags {\"scope\", $scope, \"stream\", $stream} controller.transactions.commit_failed_global controller.transactions.commit_failed - with tags {\"scope\", $scope, \"stream\", $stream} controller.transactions.commit_failed - with tags {\"scope\", $scope, \"stream\", $stream, \"transaction\", $txnId} controller.transactions.aborted_global controller.transactions.aborted - with tags {\"scope\", $scope, \"stream\", $stream} controller.transactions.abort_failed_global controller.transactions.abort_failed - with tags {\"scope\", $scope, \"stream\", $stream} controller.transactions.abort_failed - with tags {\"scope\", $scope, \"stream\", $stream, \"transaction\", $txnId} Controller hosts available ( Gauge ) and host failure ( Counter ) Metrics: controller.hosts.count controller.hosts.failures_global controller.hosts.failures - with tags {\"host\", $host} Controller Container count per host ( Gauge ) and failover ( Counter ) Metrics: controller.hosts.container_count controller.container.failovers_global controller.container.failovers - with tags {\"container\", $containerId} Controller Zookeeper session expiration ( Counter ) metrics: controller.zookeeper.session_expiration Resources \u00b6 Micrometer Metrics Statsd_spec","title":"Pravega Metrics"},{"location":"metrics/#pravega-metrics","text":"Metrics Interfaces and Examples Usage Metrics Service Provider \u2014 Interface StatsProvider Metric Logger \u2014 Interface StatsLogger Metric Sub Logger \u2014 OpStatsLogger Metric Logger \u2014 Interface DynamicLogger Example for Starting a Metric Service Example for Dynamic Counter and OpStatsLogger(Timer) Example for Dynamic Gauge Example for Dynamic Meter Metric Registries and Configurations Creating Own Metrics Metrics Naming Conventions Available Metrics and Their Names Metrics in JVM Metrics in Segment Store Service Metrics in Controller Service Resources In the Pravega Metrics Framework, we use Micrometer Metrics as the underlying library, and provide our own API to make it easier to use.","title":"Pravega Metrics"},{"location":"metrics/#metrics-interfaces-and-examples-usage","text":"StatsProvider : The Statistics Provider which provides the whole Metric service. StatsLogger : The Statistics Logger is where the required Metrics ( Counter / Gauge / Timer / Distribution Summary ) are registered. OpStatsLogger : The Operation Statistics Logger is a sub-metric for the complex ones ( Timer / Distribution Summary ). It is included in StatsLogger and DynamicLogger .","title":"Metrics Interfaces and Examples Usage"},{"location":"metrics/#metrics-service-provider-interface-statsprovider","text":"Pravega Metric Framework is initiated using the StatsProvider interface: it provides the start and stop methods for the Metric service. It also provides startWithoutExporting() for testing purpose, which only stores metrics in memory without exporting them to external systems. Currently we have support for StatsD and InfluxDB registries. StatsProvider start() : Initializes the MetricRegistry and Reporters for our Metric service. startWithoutExporting() : Initializes SimpleMeterRegistry that holds the latest value of each Meter in memory and does not export the data anywhere, typically for unit tests. close() : Shuts down the Metric Service. createStatsLogger() : Create a StatsLogger instance which is used to register and return metric objects. Application code could then perform metric operations directly with the returned metric objects. createDynamicLogger() : Creates a Dynamic Logger.","title":"Metrics Service Provider \u2014 Interface StatsProvider"},{"location":"metrics/#metric-logger-interface-statslogger","text":"This interface can be used to register the required metrics for simple types like Counter and Gauge and some complex statistics type of Metric like OpStatsLogger , through which we provide Timer and Distribution Summary . StatsLogger createStats() : Register and get a OpStatsLogger , which is used for complex type of metrics. Notice the optional metric tags. createCounter() : Register and get a Counter Metric. createMeter() : Create and register a Meter Metric. registerGauge() : Register a Gauge Metric. createScopeLogger() : Create the StatsLogger under the given scope name.","title":"Metric Logger \u2014 Interface StatsLogger"},{"location":"metrics/#metric-sub-logger-opstatslogger","text":"OpStatsLogger can be used if the user is interested in measuring the latency of operations like CreateSegment and ReadSegment . Further, we could use it to record the number of operation and time/duration of each operation. OpStatsLogger reportSuccessEvent() : Used to track the Timer of a successful operation and will record the latency in nanoseconds in the required metric. reportFailEvent() : Used to track the Timer of a failed operation and will record the latency in nanoseconds in required metric. reportSuccessValue() : Used to track the Histogram of a success value. reportFailValue() : Used to track the Histogram of a failed value. toOpStatsData() : Used to support the JMX Reporters and unit tests. clear : Used to clear the stats for this operation.","title":"Metric Sub Logger \u2014 OpStatsLogger"},{"location":"metrics/#metric-logger-interface-dynamiclogger","text":"The following is an example of a simple interface that exposes only the simple type metrics: ( Counter / Gauge / Meter ). DynamicLogger incCounterValue() : Increases the Counter with the given value. Notice the optional metric tags. updateCounterValue() : Updates the Counter with the given value. freezeCounter() : Notifies that, the Counter will not be updated. reportGaugeValue() : Reports the Gauge value. freezeGaugeValue() : Notifies that, the Gauge value will not be updated. recordMeterEvents() : Records the occurrences of a given number of events in Meter .","title":"Metric Logger \u2014 Interface DynamicLogger"},{"location":"metrics/#example-for-starting-a-metric-service","text":"This example is from io.pravega.segmentstore.server.host.ServiceStarter . The code for this example can be found here . It starts Pravega Segment Store service and the Metrics Service is started as a sub service.","title":"Example for Starting a Metric Service"},{"location":"metrics/#example-for-dynamic-counter-and-opstatsloggertimer","text":"This is an example from io.pravega.segmentstore.server.host.stat.SegmentStatsRecorderImpl.java . The code for this example can be found here . In the class PravegaRequestProcessor , we have registered two metrics: one Timer ( createStreamSegment ) one dynamic counter ( dynamicLogger ) From the above example, we can see the required steps to register and use dynamic counter: Get a dynamic logger from MetricsProvider: DynamicLogger dynamicLogger = MetricsProvider.getDynamicLogger(); Increase the counter by providing metric base name and optional tags associated with the metric. DynamicLogger dl = getDynamicLogger(); dl.incCounterValue(globalMetricName(SEGMENT_WRITE_BYTES), dataLength); ... dl.incCounterValue(SEGMENT_WRITE_BYTES, dataLength, segmentTags(streamSegmentName)); Here SEGMENT_WRITE_BYTES is the base name of the metric. Below are the two metrics associated with it: The global Counter which has no tags associated. A Segment specific Counter which has a list of Segment tags associated. Note that, the segmentTags is a method to generate tags based on fully qualified Segment name. The following are the required steps to register and use OpStatsLogger(Timer) : Get a StatsLogger from MetricsProvider . StatsLogger STATS_LOGGER = MetricsProvider.getStatsLogger(\"segmentstore\"); 2. Register all the desired metrics through StatsLogger . @Getter(AccessLevel.PROTECTED) final OpStatsLogger createStreamSegment = STATS_LOGGER.createStats(SEGMENT_CREATE_LATENCY); 3. Use these metrics within code at the appropriate places where the values should be collected and recorded. getCreateStreamSegment().reportSuccessEvent(elapsed); Here SEGMENT_CREATE_LATENCY is the name of this metric, and createStreamSegment is the metric object, which tracks operations of createSegment and we will get the time (i.e. time taken by each operation and other numbers computed based on them) for each createSegment operation happened.","title":"Example for Dynamic Counter and OpStatsLogger(Timer)"},{"location":"metrics/#example-for-dynamic-gauge","text":"This is an example from io.pravega.controller.metrics.StreamMetrics . In this class, we report a Dynamic Gauge which represents the open Transactions of a Stream. The code for this example can be found here .","title":"Example for Dynamic Gauge"},{"location":"metrics/#example-for-dynamic-meter","text":"This is an example from io.pravega.segmentstore.server.SegmentStoreMetrics . The code for this example can be found here . In the class SegmentStoreMetrics , we report a Dynamic Meter which represents the Segments created with a particular container.","title":"Example for Dynamic Meter"},{"location":"metrics/#metric-registries-and-configurations","text":"With Micrometer, each meter registry is responsible for both storage and exporting of metrics objects. In order to have a unified interface, Micrometer provides the CompositeMeterRegistry for the application to interact with, CompositeMeterRegistry will forward metric operations to all the concrete registries bounded to it. Note that when metrics service start() , initially only a global registry (of type CompositeMeterRegistry ) is provided, which will bind concrete registries (e.g. statsD, Influxdb) based on the configurations. If no registry is switched on in config , metrics service throws error to prevent the global registry runs into no-op mode. Mainly for testing purpose, metrics service can also startWithoutExporting() , where a SimpleMeterRegistry is bound to the global registry. SimpleMeterRegistry holds memory only storage but does not export metrics, makes it ideal for tests to verify metrics objects. Currently Pravega supports the following: - StatsD registry in Telegraf flavor. - Dimensional metrics data model (or metric tags). - UDP as Communication protocol. - Direct InfluxDB connection. The reporter could be configured using the MetricsConfig . Please refer to the example .","title":"Metric Registries and Configurations"},{"location":"metrics/#creating-own-metrics","text":"When starting a Segment Store/Controller Service, start a Metric Service as a sub service. Please check ServiceStarter.start() public class AddMetrics { MetricsProvider . initialize ( Config . METRICS_CONFIG ); statsProvider . start ( metricsConfig ); statsProvider = MetricsProvider . getMetricsProvider (); statsProvider . start (); Create a new StatsLogger instance through the MetricsProvider.createStatsLogger(String loggerName) , and register metric using name, e.g. STATS_LOGGER.createCounter(String name) ; and then update the metric object as appropriately in the code. static final StatsLogger STATS_LOGGER = MetricsProvider . getStatsLogger (); // <--- 1 DynamicLogger dynamicLogger = MetricsProvider . getDynamicLogger (); static class Metrics { // < --- 2 //Using Stats Logger static final String CREATE_STREAM = \"stream_created\" ; static final OpStatsLogger CREATE_STREAM = STATS_LOGGER . createStats ( CREATE_STREAM ); static final String SEGMENT_CREATE_LATENCY = \"segmentstore.segment.create_latency_ms\" ; static final OpStatsLogger createStreamSegment = STATS_LOGGER . createStats ( SEGMENT_CREATE_LATENCY ); //Using Dynamic Logger static final String SEGMENT_READ_BYTES = \"segmentstore.segment.read_bytes\" ; //Dynamic Counter static final String OPEN_TRANSACTIONS = \"controller.transactions.opened\" ; //Dynamic Gauge ... } //to report success or increment Metrics . CREATE_STREAM . reportSuccessValue ( 1 ); // < --- 3 Metrics . createStreamSegment . reportSuccessEvent ( timer . getElapsed ()); dynamicLogger . incCounterValue ( Metrics . SEGMENT_READ_BYTES , 1 ); dynamicLogger . reportGaugeValue ( OPEN_TRANSACTIONS , 0 ); //in case of failure Metrics . CREATE_STREAM . reportFailValue ( 1 ); Metrics . createStreamSegment . reportFailEvent ( timer . getElapsed ()); //to freeze dynamicLogger . freezeCounter ( Metrics . SEGMENT_READ_BYTES ); dynamicLogger . freezeGaugeValue ( OPEN_TRANSACTIONS ); }","title":"Creating Own Metrics"},{"location":"metrics/#metrics-naming-conventions","text":"All metric names are in the following format: Metrics Prefix + Component Origin + Sub-Component (or Abstraction) + Metric Base Name 1. Metric Prefix : By default pravega is configurable. Component Origin : Indicates which component generates the metric, such as segmentstore , controller . Sub-Component (or Abstraction) : Indicates the second level component or abstraction, such as cache , transaction , storage . Metric Base Name : Indicates the read_latency_ms , create_count . For example: pravega.segmentstore.segment.create_latency_ms Following are some common combinations of component and sub-components (or abstractions) being used: segmentstore.segment : Metrics for individual Segments segmentstore.storage : Metrics related to long-term storage (Tier 2) segmentstore.bookkeeper : Metrics related to Bookkeeper (Tier 1) segmentstore.container : Metrics for Segment Containers segmentstore.thread_pool : Metrics for Segment Store thread pool segmentstore.cache : Cache-related metrics controller.stream : Metrics for operations on Streams (e.g., number of streams created) controller.segments : Metrics about Segments, per Stream (e.g., count, splits, merges) controller.transactions : Metrics related to Transactions (e.g., created, committed, aborted) controller.retention : Metrics related to data retention, per Stream (e.g., frequency, size of truncated data) controller.hosts : Metrics related to Pravega servers in the cluster (e.g., number of servers, failures) controller.container : Metrics related to Container lifecycle (e.g., failovers) Following are the two types of metrics: Global Metric : _global metrics are reporting global values per component (Segment Store or Controller) instance, and further aggregation logic is needed if looking for Pravega cluster globals. For instance, STORAGE_READ_BYTES can be classified as a Global metric. Object-based Metric : Sometimes, we need to report metrics only based on specific objects, such as Streams or Segments. This kind of metrics use metric name as a base name in the file and are \"dynamically\" created based on the objects to be measured. For instance, in CONTAINER_APPEND_COUNT we actually report multiple metrics, one per each containerId measured, with different container tag (e.g. [\"containerId\", \"3\"] ). There are cases in which we may want both a Global and Object-based versions for the same metric. For example, regarding SEGMENT_READ_BYTES we publish the Global version of it by adding _global suffix to the base name segmentstore.segment.read_bytes_global to track the globally total number of bytes read, as well as the per-segment version of it by using the same base name and also supplying additional Segment tags to report in a finer granularity the events read per Segment. segmentstore.segment.read_bytes, [\"scope\", \"...\", \"stream\", \"...\", \"segment\", \"...\", \"epoch\", \"...\"])","title":"Metrics Naming Conventions"},{"location":"metrics/#available-metrics-and-their-names","text":"","title":"Available Metrics and Their Names"},{"location":"metrics/#metrics-in-jvm","text":"jvm_gc_live_data_size jvm_gc_max_data_size jvm_gc_memory_allocated jvm_gc_memory_prompted jvm_gc_pause jvm_memory_committed jvm_memory_max jvm_memory_used jvm_threads_daemon jvm_threads_live jvm_threads_peak jvm_threads_states","title":"Metrics in JVM"},{"location":"metrics/#metrics-in-segment-store-service","text":"Segment Store Read/Write latency of storage operations ( Histograms ): ``` segmentstore.segment.create_latency_ms segmentstore.segment.read_latency_ms segmentstore.segment.write_latency_ms ``` Segment Store global and per-segment Read/Write Metrics ( Counters ): ``` // Global counters segmentstore.segment.read_bytes_global segmentstore.segment.write_bytes_global segmentstore.segment.write_events_global // Per segment counters - all with tags {\"scope\", $scope, \"stream\", $stream, \"segment\", $segment, \"epoch\", $epoch} segmentstore.segment.write_bytes segmentstore.segment.read_bytes segmentstore.segment.write_events ``` Segment Store cache Read/Write latency Metrics ( Histogram ): segmentstore.cache.insert_latency_ms segmentstore.cache.get_latency Segment Store cache Read/Write Metrics ( Counters ): segmentstore.cache.write_bytes segmentstore.cache.read_bytes Segment Store cache size ( Gauge ) and generation spread ( Histogram ) Metrics: segmentstore.cache.size_bytes segmentstore.cache.gen Tier 1 Storage DurableDataLog Read/Write latency and queuing Metrics ( Histogram ): segmentstore.bookkeeper.total_write_latency_ms segmentstore.bookkeeper.write_latency_ms segmentstore.bookkeeper.write_queue_size segmentstore.bookkeeper.write_queue_fill Tier 1 Storage DurableDataLog Read/Write ( Counter ) and per-container ledger count Metrics ( Gauge ): segmentstore.bookkeeper.write_bytes segmentstore.bookkeeper.bookkeeper_ledger_count - with tag {\"container\", $containerId} Tier 2 Storage Read/Write latency Metrics ( Histogram ): segmentstore.storage.read_latency_ms segmentstore.storage.write_latency_ms Tier 2 Storage Read/Write data and file creation Metrics ( Counters ): segmentstore.storage.read_bytes segmentstore.storage.write_bytes segmentstore.storage.create_count Segment Store container-specific operation Metrics: // Histograms - all with tags {\"container\", $containerId} segmentstore.container.process_operations.latency_ms segmentstore.container.process_operations.batch_size segmentstore.container.operation_queue.size segmentstore.container.operation_processor.in_flight segmentstore.container.operation_queue.wait_time segmentstore.container.operation_processor.delay_ms segmentstore.container.operation_commit.latency_ms segmentstore.container.operation.latency_ms segmentstore.container.operation_commit.metadata_txn_count segmentstore.container.operation_commit.memory_latency_ms // Gauge segmentstore.container.operation.log_size Segment Store operation processor ( Counter ) Metrics - all with tags {\"container\", $containerId}. // Counters/Meters segmentstore.container.append_count segmentstore.container.append_offset_count segmentstore.container.update_attributes_count segmentstore.container.get_attributes_count segmentstore.container.read_count segmentstore.container.get_info_count segmentstore.container.create_segment_count segmentstore.container.delete_segment_count segmentstore.container.merge_segment_count segmentstore.container.seal_count segmentstore.container.truncate_count Segment Store active Segments ( Gauge ) and thread pool status ( Histogram ) Metrics: // Gauge - with tags {\"container\", $containerId} segmentstore.active_segments // Histograms segmentstore.thread_pool.queue_size segmentstore.thread_pool.active_threads","title":"Metrics in Segment Store Service"},{"location":"metrics/#metrics-in-controller-service","text":"Controller Stream operation latency Metrics ( Histograms ): controller.stream.created_latency_ms controller.stream.sealed_latency_ms controller.stream.deleted_latency_ms controller.stream.updated_latency_ms controller.stream.truncated_latency_ms Controller global and per-Stream operation Metrics ( Counters ): controller.stream.created controller.stream.create_failed_global controller.stream.create_failed - with tags {\"scope\", $scope, \"stream\", $stream} controller.stream.sealed controller.stream.seal_failed_global controller.stream.seal_failed - with tags {\"scope\", $scope, \"stream\", $stream} controller.stream.deleted controller.stream.delete_failed_global controller.stream.delete_failed - with tags {\"scope\", $scope, \"stream\", $stream} controller.stream.updated_global controller.stream.updated - with tags {\"scope\", $scope, \"stream\", $stream} controller.stream.update_failed_global controller.stream.update_failed - with tags {\"scope\", $scope, \"stream\", $stream} controller.stream.truncated_global controller.stream.truncated - with tags {\"scope\", $scope, \"stream\", $stream} controller.stream.truncate_failed_global controller.stream.truncate_failed - with tags {\"scope\", $scope, \"stream\", $stream} Controller Stream retention frequency ( Counter ) and truncated size ( Gauge ) Metrics: controller.retention.frequency - with tags {\"scope\", $scope, \"stream\", $stream} controller.retention.truncated_size - with tags {\"scope\", $scope, \"stream\", $stream} Controller Stream Segment operations ( Counters ) and open/timed out Transactions on a Stream ( Gauge ) Metrics - all with tags {\"scope\", $scope, \"stream\", $stream}: controller.transactions.opened controller.transactions.timedout controller.segments.count controller.segment.splits controller.segment.merges Controller Transaction operation latency Metrics: controller.transactions.created_latency_ms controller.transactions.committed_latency_ms controller.transactions.aborted_latency_ms Controller Transaction operation counter Metrics: controller.transactions.created_global controller.transactions.created - with tags {\"scope\", $scope, \"stream\", $stream} controller.transactions.create_failed_global controller.transactions.create_failed - with tags {\"scope\", $scope, \"stream\", $stream} controller.transactions.committed_global controller.transactions.committed - with tags {\"scope\", $scope, \"stream\", $stream} controller.transactions.commit_failed_global controller.transactions.commit_failed - with tags {\"scope\", $scope, \"stream\", $stream} controller.transactions.commit_failed - with tags {\"scope\", $scope, \"stream\", $stream, \"transaction\", $txnId} controller.transactions.aborted_global controller.transactions.aborted - with tags {\"scope\", $scope, \"stream\", $stream} controller.transactions.abort_failed_global controller.transactions.abort_failed - with tags {\"scope\", $scope, \"stream\", $stream} controller.transactions.abort_failed - with tags {\"scope\", $scope, \"stream\", $stream, \"transaction\", $txnId} Controller hosts available ( Gauge ) and host failure ( Counter ) Metrics: controller.hosts.count controller.hosts.failures_global controller.hosts.failures - with tags {\"host\", $host} Controller Container count per host ( Gauge ) and failover ( Counter ) Metrics: controller.hosts.container_count controller.container.failovers_global controller.container.failovers - with tags {\"container\", $containerId} Controller Zookeeper session expiration ( Counter ) metrics: controller.zookeeper.session_expiration","title":"Metrics in Controller Service"},{"location":"metrics/#resources","text":"Micrometer Metrics Statsd_spec","title":"Resources"},{"location":"pravega-concepts/","text":"Pravega Concepts \u00b6 Introduction Streams Events Writers Readers Reader Groups Stream Segments Events in a Stream Segment Stream Segments and Connection Pooling Elastic Streams Auto Scaling Events Stream Segments and Auto scaling Stream Segments and Reader Groups Ordering Guarantees Reader Group Checkpoints Transactions State Synchronizers Architecture Putting the Concepts Together A Note on Tiered Storage Stream Retention Policies Introduction \u00b6 Pravega is an open source storage system implementing Streams as first-class primitive for storing/serving continuous and unbounded data. Next, we overview the key concepts in Pravega. For a concise definition of key terms of Pravega concepts, please see Terminology . Streams \u00b6 Pravega organizes data into Streams. A Stream is a durable, elastic, append-only, unbounded sequence of bytes having good performance and strong consistency. A Pravega Stream is similar to but more flexible than a \"topic\" in popular message oriented middleware such as RabbitMQ or Apache Kafka . Pravega Streams are based on an append-only log data structure. By using append-only logs, Pravega rapidly ingests data into durable storage. It supports a large variety of application use cases: Stream processing with frameworks like Flink . Publish/subscribe messaging. NoSQL databases like Time Series Database (TSDB). Workflow engines. Event-oriented applications, etc. When a developer creates a Stream in Pravega, s/he gives the Stream a meaningful name such as \"IoTSensorData\" or \"WebApplicationLog20170330\" to inform about the type of data it stores. Moreover, Pravega Stream names are organized within Scopes. A Scope acts as a namespace for Stream names; all Stream names are unique within their Scope. Therefore, a Stream is uniquely identified by the combination of its name and Scope. Developers can also define meaningful Scope names, such as \"FactoryMachines\" or \"HRWebsitelogs\", to effectively organize collections of Streams. For example, Scopes can be used to classify Streams by tenant (in a multi tenant environment), by department in an organization or by geographic location. A Stream is unbounded in size \u2013 Pravega itself does not impose any limits on how many Events (i.e., bytes) are stored in a Stream. Pravega\u2019s design horizontally scales from few machines to a whole datacenter\u2019s capacity. Pravega Streams are divided into Stream Segments to handle a large volume of data within a Stream. A Stream Segment is a shard, or partition of the data within a Stream. For more information, please see Stream Segments section. Applications, such as a Java program reading from an IoT sensor, write data to the tail (front) of the Stream. Analytics applications, such as a Flink or Hadoop jobs, can read from any point in the Stream. Many applications can read and write the same Stream in parallel. Elasticity, scalability, support for large volume of Stream data and applications are the highlights of Pravega's design. More information on read and write operations in the Streams will be discussed in the Readers and Writers section. Events \u00b6 Pravega's client API allows applications to write and read data to/from Pravega in the form of Events . An Event is represented as a set of bytes within a Stream. For example, an Event could be as simple as a small number of bytes containing a temperature reading from an IoT sensor composed of a timestamp, a metric identifier and a value. An Event could also be a web log data associated with a user click on a website. Applications make sense of Events using standard Java serializers and deserializers , allowing them to read and write objects in Pravega similarly to reading and writing objects from files. Every Event has a Routing Key . A Routing Key is a string used by developers to group similar Events. A Routing Key is often derived from data naturally occurring in the Event, like \"customer-id\" or \"machine-id\" or a declared/user-defined string. For example, a Routing Key could be a date (to group Events together by time) or it could be a IoT sensor id (to group Events by machine). A Routing Key is important in defining the read and write semantics that Pravega guarantees. Writers, Readers, Reader Groups \u00b6 Pravega provides a client library, written in Java, that implements a convenient API for Writer and Reader applications. The Pravega Java Client Library encapsulates the Wire Protocol used to communicate Pravega clients and servers. Writer: An application that creates Events and writes them into a Stream. All data is written by appending to the tail (front) of a Stream. Reader: An application that reads Events from a Stream. Readers can read from any point in the Stream. Many Readers will be reading Events from the tail of the Stream. Tail reads corresponding to recently written Events are immediately delivered to Readers. Some Readers will read from earlier parts of the Stream (called catch-up reads ). The application developer has control over the Reader's start position in the Stream. Position: Abstraction that represents where in a Stream a Reader is currently located. The Position object can be used as a recovery mechanism by replacing the failed Reader by a new Reader starting at the last saved successful read Position. Using this pattern of persisting position objects, applications can be built guaranteeing exactly-once Event processing in the presence of Reader failures. Reader Groups: Readers are organized into Reader Groups. A Reader Group is a named collection of Readers, which together perform parallel reads from a given Stream. When a Reader is created through the Pravega data plane API, the developer includes the name of the Reader Group associated with it. Pravega guarantees that each Event published to a Stream is sent to exactly one Reader within the Reader Group. There could be one or more Readers in the Reader Group and there could be many different Reader Groups simultaneously reading from any given Stream. A Reader Group can be considered as a \"composite Reader\" or \"distributed Reader\", that allows a distributed application to read and process Stream data in parallel. A large amount of Stream data can be consumed by a coordinated group of Readers in a Reader Group. For example, a collection of Flink tasks processing Stream data in parallel using Reader Group. For more details on the basics of working with Pravega Readers and Writers, please see Working with Pravega: Basic Reader and Writer . Stream Segments \u00b6 A Stream is split into a set of shards or partitions generally referred as Stream Segments . Events in a Stream Segment \u00b6 The Stream Segments acts as a container for Events within the Stream. When an Event is written into a Stream, it is stored in one of the Stream Segments based on the Event's Routing Key. Pravega uses consistent hashing to assign Events to Stream Segments. Event Routing Keys are hashed to form a \"key space\". The key space is then divided into a number of partitions, corresponding to the number of Stream Segments. Consistent hashing determines of Events to Stream Segments. Stream Segments and Connection pooling \u00b6 Event is written to one of the Stream Segments by the Pravega Client based on the Event Routing Key in the Stream. The Stream Segments are managed by the different Segment Store Service instances in the Stream. A new connection to a Segment Store is created even when multiple Segments are owned by the same Segment Store. Every Segment being read by the Pravega client maps to a new connection. The number of connections created increases if the user is writing and reading from multiple Streams. The goal of connection pooling is to ensure a common pool of connections between the client process and the Segment Stores, which does not require a linear growth of the number of connections with the number of Segments. Elastic Streams: Auto Scaling \u00b6 A unique feature of Pravega is that the number of parallel Stream Segments in a Stream can automatically grow and shrink over time based on the I/O load it receives. This feature is known as Auto Scaling . Consider the following figure that shows the relationship between Routing Keys and time. A Stream starts at time t0 with a configurable number of Stream Segments. If the rate of data written to the Stream is constant, there will be no change in the number of Stream Segments. At time t1 , the system noted an increase in the ingestion rate and splits Stream Segment 1 into two parts. This process is referred as Scale-Up Event. Before t1 , Events with a Routing Key that hashes to the upper part of the key space (i.e., values ranging from 200-399 ) would be placed in Stream Segment 1 and those that hash into the lower part of the key space (i.e., values ranging from 0-199 ) would be placed in Stream Segment 0 . After t1 , Stream Segment 1 is split into Stream Segment 2 and Stream Segment 3 . The Stream Segment 1 is sealed and stops accepting writes. At this point in time, Events with Routing Key 300 and above are written to Stream Segment 3 and those between 200 and 299 would be written into Stream Segment 2 . Stream Segment 0 continues accepting the same range of Events as before t1 . Another scale-up Event occurs at time t2 , as Stream Segment 0 \u2019s range of Routing Key is split into Stream Segment 5 and Stream Segment 4 . Also at this time, Stream Segment 0 is sealed and allows no further writes. Stream Segments covering a contiguous range of the key space can also be merged. At time t3 , Stream Segment 2 \u2019s range and Stream Segment 5 \u2019s range are merged to Stream Segment 6 to accommodate a decrease in the load on the Stream. When a Stream is created, it is configured with a Scaling Policy that determines how a Stream handles the varying changes in its load. Pravega has three kinds of Scaling Policy: Fixed : The number of Stream Segments does not vary with load. Data-based : Pravega splits a Stream Segment into multiple ones (i.e., Scale-up Event) if the number of bytes per second written to that Stream Segment increases beyond a defined threshold. Similarly, Pravega merges two adjacent Stream Segments (i.e., Scale-down Event) if the number of bytes written to them fall below a defined threshold. Note that, even if the load for a Stream Segment reaches the defined threshold, Pravega does not immediately trigger a Scale-up/down Event. Instead, the load should be satisfying the scaling policy threshold for a sufficient amount of time . Event-based : Similar to the data-based scaling policy, but it uses number of Events instead of bytes. Events, Stream Segments and Auto Scaling \u00b6 As mentioned earlier in this section, that an Event is written into one of the Stream Segments. By considering Auto Scaling, Stream Segments performs bucketing of Events based on Routing Key and time. It is obvious that, at any given time, Events published to a Stream with a given value of Routing Key will appear in the same Stream Segment. It is also worth emphasizing that Events are written only on the active Stream Segments. Stream Segments that are sealed do not accept writes. In the figure above, at time now , only Stream Segments 3 , 6 and 4 are active and the entire key space is covered between those three Stream Segments. Stream Segments and Reader Groups \u00b6 Stream Segments play a major role in understanding the way Reader Groups work. Pravega assigns zero or more Stream Segments to each Reader in a Reader Group. Pravega tries to balances the number of Stream Segments assigned to each Reader. In the figure above, Reader B1 reads from two Stream Segments ( Segment 0 and Segment 3 ), while the other Reader Group ( Reader B2 , Reader B3 ) have only only one Stream Segment to read from. Pravega makes sure that each Stream Segment is read exactly by one Reader in any Reader Group configured with that Stream. Irrespective of Readers being added to the Reader Group or removed from the Reader Group due to crash, Pravega reassigns Stream Segments to maintain balance among the Readers. The number of Stream Segments in a Stream determines the upper bound of parallelism of readers within a Reader Group. If there are more Stream Segments, different Reader Groups and many parallel sets of Readers can effectively consume the Stream. In the above figure, Stream 1 has four Stream Segments. The largest effective Reader Group would contain four Readers. Reader Group B in the above figure is not quite optimal. If one more Reader was added to the Reader Group, each Reader would have one Stream Segment to process, thus maximizing read parallelism. However, the number of Readers in the Reader Group increases beyond 4, at least one of the Readers will not be assigned a Stream Segment. If Stream 1 in the figure above experienced a Scale-Down event, by reducing the number of Stream Segments to three, then the Reader Group B will have an ideal number of Readers. The number of Stream Segments change over time by using the Pravega's Auto Scaling feature as we discussed in the Auto Scaling section. The size of any Stream is determined by the storage capacity of the Pravega cluster. More Streams can be obtained by increasing the storage of the Pravega cluster. Applications can react to changes in the number of Stream Segments in a Stream by adjusting the number of Readers within a Reader Group to maintain optimal read parallelism. As a cool use case, Pravega may allow Flink to increase or decrease the number of task instances that are processing a Stream in parallel. Ordering Guarantees \u00b6 A Stream comprises a set of Segments that can change over time. Segments that overlap in their area of key space have a defined order. An Event written to a Stream is written to a single Stream Segment, and is ordered with respect to the Events of that Stream Segment. The existence and position of an Event within a Stream Segment is strongly consistent. Readers can be assigned multiple parallel Stream Segments (from different parts of key space). A Reader reading from multiple Stream Segments will interleave the Events of the Stream Segments, but the order of Events per Stream Segment is retained. Specifically, if s is a Stream Segment, and s contains two Events i.e., s = { e~1 , e~2 } where e~1 precedes e~2 . Thus, for a Reader reading Stream Segments, it is guaranteed that e~1 will be read before e~2 . This results in the following ordering guarantees: Events with the same Routing Key are consumed in the order they were written. Events with different Routing Keys are sent to a specific Stream Segment and will always be read in the same order even if the Reader performs back ups and re-reads. If an Event has been acknowledged to its Writer or has been read by a Reader, it is guaranteed that it will continue to exist in the same location or position for all subsequent reads until it is deleted. If there are multiple Readers reading a Stream and they all back up to any given point, they will never see any reordering with respect to that point. (It will never be the case that an event that they read before the chosen point now comes after or vice versa). Reader Group Checkpoints \u00b6 Pravega provides the ability for an application to initiate a Checkpoint on a Reader Group. The idea with a Checkpoint is to create a consistent \"point in time\" persistence of the state of each Reader in the Reader Group, by using a specialized Event ( Checkpoint Event ) to signal each Reader to preserve its state. Once a Checkpoint has been completed, the application can use the Checkpoint to reset all the Readers in the Reader Group to the known consistent state represented by the Checkpoint. For more details on working with Reader Groups, Please see Reader Group Basics . Transactions \u00b6 Pravega supports Transactions. The idea of a Transaction is that a Writer can \"batch\" up a bunch of Events and commit them as a unit into a Stream. This is useful, for example, in Flink jobs, using Pravega as a sink. The Flink job can continuously produce results for some data processing and use the Transaction to durably accumulate the results of the processing. For example, at the end of some sort of time window, the Flink job can commit the Transaction and therefore make the results of the processing available for downstream processing, or in the case of an error, the Transaction is aborted and the results disappear. A key difference between Pravega's Transactions and similar approaches (Kafka's producer-side batching) vary with the feature durability. Events added to a Transaction are durable when the Event is acknowledged back to the Writer. However, the Events in the Transaction are not visible to Readers until the Transaction is committed by the Writer. A Transaction is a similar to a Stream and is associated with multiple Stream Segments. When an Event is published into a Transaction, the Event itself is appended to a Stream Segment of the Transaction. For example, a Stream has five Stream Segments, when a Transaction is created on that Stream, conceptually that Transaction also has five Stream Segments. When an Event is published into the Transaction, it is routed and assigned to the same numbered Stream Segment similar to Stream (i.e., Event assigned to Stream Segment 3 in the Stream will be assigned to Segment 3 in the Transaction). Once the Transaction is committed, all the Transaction Segments are automatically appended to their corresponding Stream Segment in the Stream. If the Transaction is aborted, the Transaction, all its Stream Segments and all the Events published into the Transaction are removed from Pravega. Events published into a Transaction are visible to the Reader only after the Transaction is committed. For more details on working with Transactions, please see Working with Pravega: Transactions . State Synchronizers \u00b6 Pravega implements various building blocks to materialize the Stream primitive. One of such building blocks, namely State Synchronizer , is aimed at coordinating processes in a distributed computing environment. Internally, the State Synchronizer uses a Pravega Stream to provide a synchronization mechanism for state shared between multiple processes running in a cluster and making it easier to build distributed applications. With State Synchronizer, an application developer can use Pravega to read and make changes to shared state consistently and perform optimistic locking. State Synchronizer could be used to maintain a single, shared copy of an application's configuration property across all instances of that application in a cloud. State Synchronizer could also be used to store one piece of data or a map with thousands of different key value pairs. In Pravega, managing the state of Reader Groups and distribution of Readers throughout the network is implemented using State Synchronizer. An application developer creates a State Synchronizer on a Stream similar to the creation of a Writer. The State Synchronizer keeps a local copy of the shared state and allows faster access to the data for the application. State Synchronizer keeps track of all the changes happening in the shared state and it is responsible for performing any modification to the shared state in the Stream. Each application instance uses the State Synchronizer, to remain updated with the changes by pulling updates to the shared state and modifying the local copy of the data. Consistency is maintained through a conditional append style of updates to the shared state through the State Synchronizer, making sure that updates are made only to the most recent version of the shared state. The State Synchronizer can occasionally be \"compacted\" by compressing and removing older updates, while retaining only the most recent version of the state in the backing Stream. This feature assures the application developers, that the shared state does not grow unchecked. State Synchronizer works effectively when most updates to shared state are small in comparison to the total data size being stored. This can be achieved by allowing them to be written as small deltas. As with any optimistic concurrency system, State Synchronizer is not at its best when many processes attempt for simultaneous updates on the same piece of data. For more details on working with State Synchronizers, please see Working with Pravega: State Synchronizer . Architecture \u00b6 The following figure depicts the components deployed by Pravega: Pravega is deployed as a distributed system \u2013 a cluster of servers and storage coordinated to run Pravega called a Pravega cluster . Pravega presents a software-defined storage (SDS) architecture formed by Controller instances ( control plane ) and Pravega Servers ( data plane ). The set of Pravega Servers is collectively known as the Segment Store . The set of Controller instances together forms the control plane of Pravega, providing functionality to create, update and delete Streams. Further, it extends the functionality to retrieve information about the Streams, monitor the health of the Pravega cluster, gather metrics, etc. There are usually multiple (recommended at least 3) Controller instances running in a running in a cluster for high availability. The Segment Store implements the Pravega data plane. Pravega Servers provide the API to read and write data in Streams. Data storage is comprised of two tiers: - Tier 1: It provides short term, low-latency data storage, guaranteeing the durability of data written to Streams. Pravega uses Apache Bookkeeper to implement Tier 1 Storage. Tier 1 Storage typically runs within the Pravega cluster. Tier 2: It provides long term storage for Stream data. Pravega uses HDFS, Dell EMC's Isilon or Dell EMC's Elastic Cloud Storage (ECS) to implement Tier 2 Storage. Tier 2 Storage is normally deployed outside the Pravega cluster. Storage tiering allows Pravega to achieve a sweet spot in the latency vs throughput trade-off. This makes Pravega an ideal storage substrate for serving data to both real-time and batch (analytics) applications. Moreover, as data in Tier 1 Storage ages, it is automatically moved into Tier 2 Storage. Thus, Pravega can store vasts amounts of Stream data and applications can read it at any time, while being oblivious to its actual location. Pravega uses Apache Zookeeper as the coordination mechanism for the components in the Pravega cluster. Pravega is a distributed storage system providing the Stream primitive first and foremost. Pravega is carefully designed to take advantage of software-defined storage, so that the amount of data stored in Pravega is limited only by the total storage capacity of the data center. Once an Event is written to Pravega, it is durably stored and replicated so it can survive permanent crashes of datacenter nodes. Pravega provides a Java Client Library , for building client-side applications such as analytics applications using Flink. The Pravega Java Client Library manages the interaction between the application code and Pravega via a custom TCP Wire Protocol. Putting the Concepts Together \u00b6 The concepts in Pravega are depicted in the following figure: Pravega clients are Writers and Readers. Writers write Events into a Stream. Readers read Events from a Stream. Readers are grouped into Reader Groups to read from a Stream in parallel. The Controller is a server-side component that manages the control plane of Pravega. Streams are created, updated and listed using the Controller API. The Pravega Server is a server-side component that implements reads, writes and other data plane operations. Streams are the fundamental storage primitive in Pravega. Streams contain a set of data elements called Events. Events are appended to the \u201ctail\u201d of the Stream by Writers. Readers can read Events from anywhere in the Stream. A Stream is partitioned into a set of Stream Segments. The number of Stream Segments in a Stream can change over time. Events are written into exactly one of the Stream Segments based on Routing Key. For any Reader Group reading a Stream, each Stream Segment is assigned to one Reader in that Reader Group. Each Stream Segment is stored in a combination of Tier 1 and Tier 2 Storage. The tail of the Stream Segment is stored in Tier 1 providing low latency reads and writes. The rest of the Stream Segment is stored in Tier 2, providing high throughput read access with horizontal scalability and low cost. A Note on Tiered Storage \u00b6 To deliver an efficient implementation of Streams, Pravega is based on a tiered storage model. Events are persisted in low latency/high IOPS storage (Tier 1 Storage, write-ahead log) and higher throughput Tier 2 storage (e.g., file system, object store). Writers and Readers are oblivious to the tiered storage model from an API perspective. In Pravega, Tier 1 is based on an append-only Log data structure. As Leigh Stewart observed , there are really three data access mechanisms in a Log: All of the write activity, and much of the read activity happens at the tail of the log. Writes are appended to the log and many clients try to read data immediately as it is written to the log. These two data access mechanisms are dominated by the need for low latency \u2013 low latency writes by Writers and near real-time access to the published data by Readers. Please note that not all Readers read from the tail of the log. Some Readers read by starting at some arbitrary position in the log. These reads are known as catch-up reads . Access to historical data traditionally was done by batch analytics jobs, often using HDFS and Map/Reduce. However with new streaming applications, we can access historical data as well as current data by just accessing the log. One approach would be to store all the historical data in SSDs similar to tail data operations, but that leads to an expensive task and force customers to economize by deleting historical data. Pravega offers a mechanism that allows customers to use cost-effective, highly-scalable, high-throughput storage for the historical part of the log, that way they won\u2019t have to decide on when to delete historical data. Basically, if storage is cheap enough, why not keep all of the history? Tier 1 Storage aids in faster writes to the Streams by assuring durability and makes reading from the tail of a Stream much quicker. Tier 1 Storage is based on the open source Apache BookKeeper Project. Though not essential, we presume that the Tier 1 Storage will be typically implemented on faster SSDs or even non-volatile RAM. Tier 2 Storage provides a highly-scalable, high-throughput cost-effective storage. We expect this Tier 2 to be typically deployed on spinning disks. Pravega asynchronously migrates Events from Tier 1 to Tier 2 to reflect the different access patterns to Stream data. Tier 2 Storage is based on an HDFS model. Stream Retention Policies \u00b6 Pravega allows users to store data in Tier 2 as long as there is storage capacity available. But sometimes, users may not be interested to keep all the historical data related to a Stream. Instead, there are use-cases in which it may be useful to retain just a fraction of a Stream's data. For this reason, Streams can be configured with Retention Policies . Pravega supports two types of Retention Policies: Time-based Retention : It allows the developer to control for how long the data is kept in a Stream before it is deleted. The developer can specify the time limit (milliseconds) in the Stream policy, which is ideal for situations like regulatory compliance that mandate certain retention periods. Size-based Retention : Retains the newest subset of a Stream's data that does not exceed the specified size in bytes.","title":"Pravega Concepts"},{"location":"pravega-concepts/#pravega-concepts","text":"Introduction Streams Events Writers Readers Reader Groups Stream Segments Events in a Stream Segment Stream Segments and Connection Pooling Elastic Streams Auto Scaling Events Stream Segments and Auto scaling Stream Segments and Reader Groups Ordering Guarantees Reader Group Checkpoints Transactions State Synchronizers Architecture Putting the Concepts Together A Note on Tiered Storage Stream Retention Policies","title":"Pravega Concepts"},{"location":"pravega-concepts/#introduction","text":"Pravega is an open source storage system implementing Streams as first-class primitive for storing/serving continuous and unbounded data. Next, we overview the key concepts in Pravega. For a concise definition of key terms of Pravega concepts, please see Terminology .","title":"Introduction"},{"location":"pravega-concepts/#streams","text":"Pravega organizes data into Streams. A Stream is a durable, elastic, append-only, unbounded sequence of bytes having good performance and strong consistency. A Pravega Stream is similar to but more flexible than a \"topic\" in popular message oriented middleware such as RabbitMQ or Apache Kafka . Pravega Streams are based on an append-only log data structure. By using append-only logs, Pravega rapidly ingests data into durable storage. It supports a large variety of application use cases: Stream processing with frameworks like Flink . Publish/subscribe messaging. NoSQL databases like Time Series Database (TSDB). Workflow engines. Event-oriented applications, etc. When a developer creates a Stream in Pravega, s/he gives the Stream a meaningful name such as \"IoTSensorData\" or \"WebApplicationLog20170330\" to inform about the type of data it stores. Moreover, Pravega Stream names are organized within Scopes. A Scope acts as a namespace for Stream names; all Stream names are unique within their Scope. Therefore, a Stream is uniquely identified by the combination of its name and Scope. Developers can also define meaningful Scope names, such as \"FactoryMachines\" or \"HRWebsitelogs\", to effectively organize collections of Streams. For example, Scopes can be used to classify Streams by tenant (in a multi tenant environment), by department in an organization or by geographic location. A Stream is unbounded in size \u2013 Pravega itself does not impose any limits on how many Events (i.e., bytes) are stored in a Stream. Pravega\u2019s design horizontally scales from few machines to a whole datacenter\u2019s capacity. Pravega Streams are divided into Stream Segments to handle a large volume of data within a Stream. A Stream Segment is a shard, or partition of the data within a Stream. For more information, please see Stream Segments section. Applications, such as a Java program reading from an IoT sensor, write data to the tail (front) of the Stream. Analytics applications, such as a Flink or Hadoop jobs, can read from any point in the Stream. Many applications can read and write the same Stream in parallel. Elasticity, scalability, support for large volume of Stream data and applications are the highlights of Pravega's design. More information on read and write operations in the Streams will be discussed in the Readers and Writers section.","title":"Streams"},{"location":"pravega-concepts/#events","text":"Pravega's client API allows applications to write and read data to/from Pravega in the form of Events . An Event is represented as a set of bytes within a Stream. For example, an Event could be as simple as a small number of bytes containing a temperature reading from an IoT sensor composed of a timestamp, a metric identifier and a value. An Event could also be a web log data associated with a user click on a website. Applications make sense of Events using standard Java serializers and deserializers , allowing them to read and write objects in Pravega similarly to reading and writing objects from files. Every Event has a Routing Key . A Routing Key is a string used by developers to group similar Events. A Routing Key is often derived from data naturally occurring in the Event, like \"customer-id\" or \"machine-id\" or a declared/user-defined string. For example, a Routing Key could be a date (to group Events together by time) or it could be a IoT sensor id (to group Events by machine). A Routing Key is important in defining the read and write semantics that Pravega guarantees.","title":"Events"},{"location":"pravega-concepts/#writers-readers-reader-groups","text":"Pravega provides a client library, written in Java, that implements a convenient API for Writer and Reader applications. The Pravega Java Client Library encapsulates the Wire Protocol used to communicate Pravega clients and servers. Writer: An application that creates Events and writes them into a Stream. All data is written by appending to the tail (front) of a Stream. Reader: An application that reads Events from a Stream. Readers can read from any point in the Stream. Many Readers will be reading Events from the tail of the Stream. Tail reads corresponding to recently written Events are immediately delivered to Readers. Some Readers will read from earlier parts of the Stream (called catch-up reads ). The application developer has control over the Reader's start position in the Stream. Position: Abstraction that represents where in a Stream a Reader is currently located. The Position object can be used as a recovery mechanism by replacing the failed Reader by a new Reader starting at the last saved successful read Position. Using this pattern of persisting position objects, applications can be built guaranteeing exactly-once Event processing in the presence of Reader failures. Reader Groups: Readers are organized into Reader Groups. A Reader Group is a named collection of Readers, which together perform parallel reads from a given Stream. When a Reader is created through the Pravega data plane API, the developer includes the name of the Reader Group associated with it. Pravega guarantees that each Event published to a Stream is sent to exactly one Reader within the Reader Group. There could be one or more Readers in the Reader Group and there could be many different Reader Groups simultaneously reading from any given Stream. A Reader Group can be considered as a \"composite Reader\" or \"distributed Reader\", that allows a distributed application to read and process Stream data in parallel. A large amount of Stream data can be consumed by a coordinated group of Readers in a Reader Group. For example, a collection of Flink tasks processing Stream data in parallel using Reader Group. For more details on the basics of working with Pravega Readers and Writers, please see Working with Pravega: Basic Reader and Writer .","title":"Writers, Readers, Reader Groups"},{"location":"pravega-concepts/#stream-segments","text":"A Stream is split into a set of shards or partitions generally referred as Stream Segments .","title":"Stream Segments"},{"location":"pravega-concepts/#events-in-a-stream-segment","text":"The Stream Segments acts as a container for Events within the Stream. When an Event is written into a Stream, it is stored in one of the Stream Segments based on the Event's Routing Key. Pravega uses consistent hashing to assign Events to Stream Segments. Event Routing Keys are hashed to form a \"key space\". The key space is then divided into a number of partitions, corresponding to the number of Stream Segments. Consistent hashing determines of Events to Stream Segments.","title":"Events in a Stream Segment"},{"location":"pravega-concepts/#stream-segments-and-connection-pooling","text":"Event is written to one of the Stream Segments by the Pravega Client based on the Event Routing Key in the Stream. The Stream Segments are managed by the different Segment Store Service instances in the Stream. A new connection to a Segment Store is created even when multiple Segments are owned by the same Segment Store. Every Segment being read by the Pravega client maps to a new connection. The number of connections created increases if the user is writing and reading from multiple Streams. The goal of connection pooling is to ensure a common pool of connections between the client process and the Segment Stores, which does not require a linear growth of the number of connections with the number of Segments.","title":"Stream Segments and Connection pooling"},{"location":"pravega-concepts/#elastic-streams-auto-scaling","text":"A unique feature of Pravega is that the number of parallel Stream Segments in a Stream can automatically grow and shrink over time based on the I/O load it receives. This feature is known as Auto Scaling . Consider the following figure that shows the relationship between Routing Keys and time. A Stream starts at time t0 with a configurable number of Stream Segments. If the rate of data written to the Stream is constant, there will be no change in the number of Stream Segments. At time t1 , the system noted an increase in the ingestion rate and splits Stream Segment 1 into two parts. This process is referred as Scale-Up Event. Before t1 , Events with a Routing Key that hashes to the upper part of the key space (i.e., values ranging from 200-399 ) would be placed in Stream Segment 1 and those that hash into the lower part of the key space (i.e., values ranging from 0-199 ) would be placed in Stream Segment 0 . After t1 , Stream Segment 1 is split into Stream Segment 2 and Stream Segment 3 . The Stream Segment 1 is sealed and stops accepting writes. At this point in time, Events with Routing Key 300 and above are written to Stream Segment 3 and those between 200 and 299 would be written into Stream Segment 2 . Stream Segment 0 continues accepting the same range of Events as before t1 . Another scale-up Event occurs at time t2 , as Stream Segment 0 \u2019s range of Routing Key is split into Stream Segment 5 and Stream Segment 4 . Also at this time, Stream Segment 0 is sealed and allows no further writes. Stream Segments covering a contiguous range of the key space can also be merged. At time t3 , Stream Segment 2 \u2019s range and Stream Segment 5 \u2019s range are merged to Stream Segment 6 to accommodate a decrease in the load on the Stream. When a Stream is created, it is configured with a Scaling Policy that determines how a Stream handles the varying changes in its load. Pravega has three kinds of Scaling Policy: Fixed : The number of Stream Segments does not vary with load. Data-based : Pravega splits a Stream Segment into multiple ones (i.e., Scale-up Event) if the number of bytes per second written to that Stream Segment increases beyond a defined threshold. Similarly, Pravega merges two adjacent Stream Segments (i.e., Scale-down Event) if the number of bytes written to them fall below a defined threshold. Note that, even if the load for a Stream Segment reaches the defined threshold, Pravega does not immediately trigger a Scale-up/down Event. Instead, the load should be satisfying the scaling policy threshold for a sufficient amount of time . Event-based : Similar to the data-based scaling policy, but it uses number of Events instead of bytes.","title":"Elastic Streams: Auto Scaling"},{"location":"pravega-concepts/#events-stream-segments-and-auto-scaling","text":"As mentioned earlier in this section, that an Event is written into one of the Stream Segments. By considering Auto Scaling, Stream Segments performs bucketing of Events based on Routing Key and time. It is obvious that, at any given time, Events published to a Stream with a given value of Routing Key will appear in the same Stream Segment. It is also worth emphasizing that Events are written only on the active Stream Segments. Stream Segments that are sealed do not accept writes. In the figure above, at time now , only Stream Segments 3 , 6 and 4 are active and the entire key space is covered between those three Stream Segments.","title":"Events, Stream Segments and Auto Scaling"},{"location":"pravega-concepts/#stream-segments-and-reader-groups","text":"Stream Segments play a major role in understanding the way Reader Groups work. Pravega assigns zero or more Stream Segments to each Reader in a Reader Group. Pravega tries to balances the number of Stream Segments assigned to each Reader. In the figure above, Reader B1 reads from two Stream Segments ( Segment 0 and Segment 3 ), while the other Reader Group ( Reader B2 , Reader B3 ) have only only one Stream Segment to read from. Pravega makes sure that each Stream Segment is read exactly by one Reader in any Reader Group configured with that Stream. Irrespective of Readers being added to the Reader Group or removed from the Reader Group due to crash, Pravega reassigns Stream Segments to maintain balance among the Readers. The number of Stream Segments in a Stream determines the upper bound of parallelism of readers within a Reader Group. If there are more Stream Segments, different Reader Groups and many parallel sets of Readers can effectively consume the Stream. In the above figure, Stream 1 has four Stream Segments. The largest effective Reader Group would contain four Readers. Reader Group B in the above figure is not quite optimal. If one more Reader was added to the Reader Group, each Reader would have one Stream Segment to process, thus maximizing read parallelism. However, the number of Readers in the Reader Group increases beyond 4, at least one of the Readers will not be assigned a Stream Segment. If Stream 1 in the figure above experienced a Scale-Down event, by reducing the number of Stream Segments to three, then the Reader Group B will have an ideal number of Readers. The number of Stream Segments change over time by using the Pravega's Auto Scaling feature as we discussed in the Auto Scaling section. The size of any Stream is determined by the storage capacity of the Pravega cluster. More Streams can be obtained by increasing the storage of the Pravega cluster. Applications can react to changes in the number of Stream Segments in a Stream by adjusting the number of Readers within a Reader Group to maintain optimal read parallelism. As a cool use case, Pravega may allow Flink to increase or decrease the number of task instances that are processing a Stream in parallel.","title":"Stream Segments and Reader Groups"},{"location":"pravega-concepts/#ordering-guarantees","text":"A Stream comprises a set of Segments that can change over time. Segments that overlap in their area of key space have a defined order. An Event written to a Stream is written to a single Stream Segment, and is ordered with respect to the Events of that Stream Segment. The existence and position of an Event within a Stream Segment is strongly consistent. Readers can be assigned multiple parallel Stream Segments (from different parts of key space). A Reader reading from multiple Stream Segments will interleave the Events of the Stream Segments, but the order of Events per Stream Segment is retained. Specifically, if s is a Stream Segment, and s contains two Events i.e., s = { e~1 , e~2 } where e~1 precedes e~2 . Thus, for a Reader reading Stream Segments, it is guaranteed that e~1 will be read before e~2 . This results in the following ordering guarantees: Events with the same Routing Key are consumed in the order they were written. Events with different Routing Keys are sent to a specific Stream Segment and will always be read in the same order even if the Reader performs back ups and re-reads. If an Event has been acknowledged to its Writer or has been read by a Reader, it is guaranteed that it will continue to exist in the same location or position for all subsequent reads until it is deleted. If there are multiple Readers reading a Stream and they all back up to any given point, they will never see any reordering with respect to that point. (It will never be the case that an event that they read before the chosen point now comes after or vice versa).","title":"Ordering Guarantees"},{"location":"pravega-concepts/#reader-group-checkpoints","text":"Pravega provides the ability for an application to initiate a Checkpoint on a Reader Group. The idea with a Checkpoint is to create a consistent \"point in time\" persistence of the state of each Reader in the Reader Group, by using a specialized Event ( Checkpoint Event ) to signal each Reader to preserve its state. Once a Checkpoint has been completed, the application can use the Checkpoint to reset all the Readers in the Reader Group to the known consistent state represented by the Checkpoint. For more details on working with Reader Groups, Please see Reader Group Basics .","title":"Reader Group Checkpoints"},{"location":"pravega-concepts/#transactions","text":"Pravega supports Transactions. The idea of a Transaction is that a Writer can \"batch\" up a bunch of Events and commit them as a unit into a Stream. This is useful, for example, in Flink jobs, using Pravega as a sink. The Flink job can continuously produce results for some data processing and use the Transaction to durably accumulate the results of the processing. For example, at the end of some sort of time window, the Flink job can commit the Transaction and therefore make the results of the processing available for downstream processing, or in the case of an error, the Transaction is aborted and the results disappear. A key difference between Pravega's Transactions and similar approaches (Kafka's producer-side batching) vary with the feature durability. Events added to a Transaction are durable when the Event is acknowledged back to the Writer. However, the Events in the Transaction are not visible to Readers until the Transaction is committed by the Writer. A Transaction is a similar to a Stream and is associated with multiple Stream Segments. When an Event is published into a Transaction, the Event itself is appended to a Stream Segment of the Transaction. For example, a Stream has five Stream Segments, when a Transaction is created on that Stream, conceptually that Transaction also has five Stream Segments. When an Event is published into the Transaction, it is routed and assigned to the same numbered Stream Segment similar to Stream (i.e., Event assigned to Stream Segment 3 in the Stream will be assigned to Segment 3 in the Transaction). Once the Transaction is committed, all the Transaction Segments are automatically appended to their corresponding Stream Segment in the Stream. If the Transaction is aborted, the Transaction, all its Stream Segments and all the Events published into the Transaction are removed from Pravega. Events published into a Transaction are visible to the Reader only after the Transaction is committed. For more details on working with Transactions, please see Working with Pravega: Transactions .","title":"Transactions"},{"location":"pravega-concepts/#state-synchronizers","text":"Pravega implements various building blocks to materialize the Stream primitive. One of such building blocks, namely State Synchronizer , is aimed at coordinating processes in a distributed computing environment. Internally, the State Synchronizer uses a Pravega Stream to provide a synchronization mechanism for state shared between multiple processes running in a cluster and making it easier to build distributed applications. With State Synchronizer, an application developer can use Pravega to read and make changes to shared state consistently and perform optimistic locking. State Synchronizer could be used to maintain a single, shared copy of an application's configuration property across all instances of that application in a cloud. State Synchronizer could also be used to store one piece of data or a map with thousands of different key value pairs. In Pravega, managing the state of Reader Groups and distribution of Readers throughout the network is implemented using State Synchronizer. An application developer creates a State Synchronizer on a Stream similar to the creation of a Writer. The State Synchronizer keeps a local copy of the shared state and allows faster access to the data for the application. State Synchronizer keeps track of all the changes happening in the shared state and it is responsible for performing any modification to the shared state in the Stream. Each application instance uses the State Synchronizer, to remain updated with the changes by pulling updates to the shared state and modifying the local copy of the data. Consistency is maintained through a conditional append style of updates to the shared state through the State Synchronizer, making sure that updates are made only to the most recent version of the shared state. The State Synchronizer can occasionally be \"compacted\" by compressing and removing older updates, while retaining only the most recent version of the state in the backing Stream. This feature assures the application developers, that the shared state does not grow unchecked. State Synchronizer works effectively when most updates to shared state are small in comparison to the total data size being stored. This can be achieved by allowing them to be written as small deltas. As with any optimistic concurrency system, State Synchronizer is not at its best when many processes attempt for simultaneous updates on the same piece of data. For more details on working with State Synchronizers, please see Working with Pravega: State Synchronizer .","title":"State Synchronizers"},{"location":"pravega-concepts/#architecture","text":"The following figure depicts the components deployed by Pravega: Pravega is deployed as a distributed system \u2013 a cluster of servers and storage coordinated to run Pravega called a Pravega cluster . Pravega presents a software-defined storage (SDS) architecture formed by Controller instances ( control plane ) and Pravega Servers ( data plane ). The set of Pravega Servers is collectively known as the Segment Store . The set of Controller instances together forms the control plane of Pravega, providing functionality to create, update and delete Streams. Further, it extends the functionality to retrieve information about the Streams, monitor the health of the Pravega cluster, gather metrics, etc. There are usually multiple (recommended at least 3) Controller instances running in a running in a cluster for high availability. The Segment Store implements the Pravega data plane. Pravega Servers provide the API to read and write data in Streams. Data storage is comprised of two tiers: - Tier 1: It provides short term, low-latency data storage, guaranteeing the durability of data written to Streams. Pravega uses Apache Bookkeeper to implement Tier 1 Storage. Tier 1 Storage typically runs within the Pravega cluster. Tier 2: It provides long term storage for Stream data. Pravega uses HDFS, Dell EMC's Isilon or Dell EMC's Elastic Cloud Storage (ECS) to implement Tier 2 Storage. Tier 2 Storage is normally deployed outside the Pravega cluster. Storage tiering allows Pravega to achieve a sweet spot in the latency vs throughput trade-off. This makes Pravega an ideal storage substrate for serving data to both real-time and batch (analytics) applications. Moreover, as data in Tier 1 Storage ages, it is automatically moved into Tier 2 Storage. Thus, Pravega can store vasts amounts of Stream data and applications can read it at any time, while being oblivious to its actual location. Pravega uses Apache Zookeeper as the coordination mechanism for the components in the Pravega cluster. Pravega is a distributed storage system providing the Stream primitive first and foremost. Pravega is carefully designed to take advantage of software-defined storage, so that the amount of data stored in Pravega is limited only by the total storage capacity of the data center. Once an Event is written to Pravega, it is durably stored and replicated so it can survive permanent crashes of datacenter nodes. Pravega provides a Java Client Library , for building client-side applications such as analytics applications using Flink. The Pravega Java Client Library manages the interaction between the application code and Pravega via a custom TCP Wire Protocol.","title":"Architecture"},{"location":"pravega-concepts/#putting-the-concepts-together","text":"The concepts in Pravega are depicted in the following figure: Pravega clients are Writers and Readers. Writers write Events into a Stream. Readers read Events from a Stream. Readers are grouped into Reader Groups to read from a Stream in parallel. The Controller is a server-side component that manages the control plane of Pravega. Streams are created, updated and listed using the Controller API. The Pravega Server is a server-side component that implements reads, writes and other data plane operations. Streams are the fundamental storage primitive in Pravega. Streams contain a set of data elements called Events. Events are appended to the \u201ctail\u201d of the Stream by Writers. Readers can read Events from anywhere in the Stream. A Stream is partitioned into a set of Stream Segments. The number of Stream Segments in a Stream can change over time. Events are written into exactly one of the Stream Segments based on Routing Key. For any Reader Group reading a Stream, each Stream Segment is assigned to one Reader in that Reader Group. Each Stream Segment is stored in a combination of Tier 1 and Tier 2 Storage. The tail of the Stream Segment is stored in Tier 1 providing low latency reads and writes. The rest of the Stream Segment is stored in Tier 2, providing high throughput read access with horizontal scalability and low cost.","title":"Putting the Concepts Together"},{"location":"pravega-concepts/#a-note-on-tiered-storage","text":"To deliver an efficient implementation of Streams, Pravega is based on a tiered storage model. Events are persisted in low latency/high IOPS storage (Tier 1 Storage, write-ahead log) and higher throughput Tier 2 storage (e.g., file system, object store). Writers and Readers are oblivious to the tiered storage model from an API perspective. In Pravega, Tier 1 is based on an append-only Log data structure. As Leigh Stewart observed , there are really three data access mechanisms in a Log: All of the write activity, and much of the read activity happens at the tail of the log. Writes are appended to the log and many clients try to read data immediately as it is written to the log. These two data access mechanisms are dominated by the need for low latency \u2013 low latency writes by Writers and near real-time access to the published data by Readers. Please note that not all Readers read from the tail of the log. Some Readers read by starting at some arbitrary position in the log. These reads are known as catch-up reads . Access to historical data traditionally was done by batch analytics jobs, often using HDFS and Map/Reduce. However with new streaming applications, we can access historical data as well as current data by just accessing the log. One approach would be to store all the historical data in SSDs similar to tail data operations, but that leads to an expensive task and force customers to economize by deleting historical data. Pravega offers a mechanism that allows customers to use cost-effective, highly-scalable, high-throughput storage for the historical part of the log, that way they won\u2019t have to decide on when to delete historical data. Basically, if storage is cheap enough, why not keep all of the history? Tier 1 Storage aids in faster writes to the Streams by assuring durability and makes reading from the tail of a Stream much quicker. Tier 1 Storage is based on the open source Apache BookKeeper Project. Though not essential, we presume that the Tier 1 Storage will be typically implemented on faster SSDs or even non-volatile RAM. Tier 2 Storage provides a highly-scalable, high-throughput cost-effective storage. We expect this Tier 2 to be typically deployed on spinning disks. Pravega asynchronously migrates Events from Tier 1 to Tier 2 to reflect the different access patterns to Stream data. Tier 2 Storage is based on an HDFS model.","title":"A Note on Tiered Storage"},{"location":"pravega-concepts/#stream-retention-policies","text":"Pravega allows users to store data in Tier 2 as long as there is storage capacity available. But sometimes, users may not be interested to keep all the historical data related to a Stream. Instead, there are use-cases in which it may be useful to retain just a fraction of a Stream's data. For this reason, Streams can be configured with Retention Policies . Pravega supports two types of Retention Policies: Time-based Retention : It allows the developer to control for how long the data is kept in a Stream before it is deleted. The developer can specify the time limit (milliseconds) in the Stream policy, which is ideal for situations like regulatory compliance that mandate certain retention periods. Size-based Retention : Retains the newest subset of a Stream's data that does not exceed the specified size in bytes.","title":"Stream Retention Policies"},{"location":"reader-group-design/","text":"Reader Groups Design \u00b6 Motivation \u00b6 A set of Readers can be grouped together in order that the set of Events in a Stream can be read in parallel. This grouping of Readers is called a Reader Group. Pravega guarantees that each Event in the Stream is read by exactly one Reader in the Reader Group. Each Reader in a Reader Group is assigned zero or more Stream Segments. The Reader assigned to a Stream Segment is the only Reader within the Reader Group that reads Events from that Stream Segment. This is the fundamental mechanism by which Pravega makes ordering guarantees of Event delivery to a Reader \u2013 a Reader will receive Events in the order they were written into a Stream Segment. There are several challenges associated with this mechanism: How to maintain the mapping of which Reader within a Reader Group is assigned which Stream Segment? How to manage the above mapping when Stream Segments split and merge? How to manage the above mapping when Readers are added to the Reader Group? How to manage the above mapping when Readers leave the Reader Group either by an explicit operation or the Reader becoming unavailable due to network outage or the Reader process failing? To address these challenges, we use State Synchronizer to enable coordination among Readers. Consistent Replicated State \u00b6 A consistent replicated state object representing the Reader Group metadata will be created in each Reader. This Reader Group metadata consists of: A map of online Readers to the Stream Segments they own. A list of positions in Stream Segments that can be taken over. Every time the Readers in a Reader Group change, the state can be updated. The replicated state is updated every time a Reader starts reading a new Stream Segment. Thus every Reader is aware of the Stream Segments owned by all the Readers in the their Reader Group. Given this information: A new Reader can infer which Stream Segments are available to read from. (By virtue of it being absent from the state). Dealing with a Stream Segment being merged becomes easy, because the last reader to reach the end of its pre-merge Stream Segment knows it can freely take ownership of the new Stream Segment. Readers can see their relative load and how they are progressing relative to the other Readers in their group and can decide to transfer Stream Segments if things are out of balance. This allows Readers to take action directly to ensure all the events are read without the need for some external tracker. Reader Group APIs \u00b6 The external APIs to manage Reader Groups could be added to the ReaderGroupManager object. It consist of: ReaderGroup createReaderGroup ( String name , Stream stream , ReaderGroupConfig config ); ReaderGroup getReaderGroup ( String name , Stream stream ); void deleteReaderGroup ( ReaderGroup group ); When a Reader Group is created, it creates a State Synchronizer shared by the Readers. To join a Reader Group, Readers would just specify it in their configuration: ClientFactory clientFactory = ClientFactory . withScope ( scope , controllerURI ); EventStreamReader < T > reader = clientFactory . createReader ( readerId , READER_GROUP_NAME , serializer , readerConfig ); The Readers, while joining the group, access the information stored on the state to determine which Stream Segments to read from. Once when they shut down, they update the state so that other Readers can take over their Stream Segments. Detecting Pravega Reader Failure \u00b6 We still need some effective mechanism to identify, whether Readers are alive or not. The problem is greatly simplified because it need not produce a view of the cluster or manage any state. The component would just need to detect a Reader failure and invoke the void ReaderOffline(String ReaderId, Position lastPosition); API on the Reader Group. For consistency, the \"Failure detector\" should not declare a host as dead that is still processing events. Doing so could violate exactly once processing guarantees. New Reader Added \u00b6 When a Reader joins a group its online status is added to the shared state. Other Readers receive updates to the shared state. When a Reader with more than average number of Stream Segments sees the new Reader, it may give up a Stream Segment by writing its position for that Stream Segment to the shared state. The new Reader can take over a Stream Segment by recording that it is doing so in the shared state. The new Reader can start reading from the position it read from the shared state for the Stream Segment it picked up. There are no races between multiple Readers coming online concurrently because only one of them can successfully claim ownership of any given Stream Segment. Stream Segments Get Merged \u00b6 When a Reader comes to the end of its Stream Segment it records this information in the shared state. When all of the Stream Segments that are getting merged together are completed, a Reader may claim ownership of the following Stream Segment. There is no ambiguity as to who the owner is, because it is stored in the shared state. There is no risk of a Stream Segment being ignored because every Reader can see the available Stream Segments by looking at the shared state and claim them. Reader Offline \u00b6 When a Reader dies, the void ReaderOffline(String ReaderId, Position lastPosition); API method will be invoked either by the Reader itself in a graceful shutdown (internally to the close method) or via a \"liveness detector\". In either case the Reader's last position is written to the state. If a null Position is sent then the last checkpointed position will be written to the state. This is used by the newer Readers when they take ownership of the Stream Segment(s) that were read by the older/offline reader. Any Reader can decide to take over one or more of the Stream Segments owned by the old Reader from where it left off by recording their intention to do so in the state object. Once the state has been updated by the new Reader, it is considered the owner of the Stream Segment and can read from it. Other Considerations on Reader Groups \u00b6 What happens if a Reader does not keep up to date? \u00b6 A Reader with out-of-date state can read from their existing Stream Segments without interference. The only disadvantage to this is that they will not shed load to another Reader should one become available. However, because they have to write to the shared state to start reading from any Stream Segment which they don't already own, they must fetch up-to-date information before moving on to a new Stream Segment. Impact of availability and latency \u00b6 Reading and updating the state object can occur in parallel to reading, so there would likely be no visible latency impact. A stream would be unavailable for reading if Pravega failed in such a way that the Stream Segment containing the Reader Group information went down and remained offline for long enough for the Readers to exhaust all the events in their existing Stream Segments. Of course, if Pravega failed in this way, odds are at least some portion of the stream would also be directly impacted and not be able to read any events. This sort of failure mode would manifest as latency for the Reader, similar to what would happen if they had reached the tail of the stream. This is preferable to using an external system to manage this coordination, as that would require adding new components that can fail in different ways, as opposed to further relying on a small set that we need to make highly available anyway. This is particularly notable in the case of a network partition. If the network is split any Readers that are on the same side of the partition as the Pravega servers can continue working. If we were to utilize an external service, that service could be cut off and Readers might not be able to make progress even if they could talk to Pravega.","title":"Reader Groups"},{"location":"reader-group-design/#reader-groups-design","text":"","title":"Reader Groups Design"},{"location":"reader-group-design/#motivation","text":"A set of Readers can be grouped together in order that the set of Events in a Stream can be read in parallel. This grouping of Readers is called a Reader Group. Pravega guarantees that each Event in the Stream is read by exactly one Reader in the Reader Group. Each Reader in a Reader Group is assigned zero or more Stream Segments. The Reader assigned to a Stream Segment is the only Reader within the Reader Group that reads Events from that Stream Segment. This is the fundamental mechanism by which Pravega makes ordering guarantees of Event delivery to a Reader \u2013 a Reader will receive Events in the order they were written into a Stream Segment. There are several challenges associated with this mechanism: How to maintain the mapping of which Reader within a Reader Group is assigned which Stream Segment? How to manage the above mapping when Stream Segments split and merge? How to manage the above mapping when Readers are added to the Reader Group? How to manage the above mapping when Readers leave the Reader Group either by an explicit operation or the Reader becoming unavailable due to network outage or the Reader process failing? To address these challenges, we use State Synchronizer to enable coordination among Readers.","title":"Motivation"},{"location":"reader-group-design/#consistent-replicated-state","text":"A consistent replicated state object representing the Reader Group metadata will be created in each Reader. This Reader Group metadata consists of: A map of online Readers to the Stream Segments they own. A list of positions in Stream Segments that can be taken over. Every time the Readers in a Reader Group change, the state can be updated. The replicated state is updated every time a Reader starts reading a new Stream Segment. Thus every Reader is aware of the Stream Segments owned by all the Readers in the their Reader Group. Given this information: A new Reader can infer which Stream Segments are available to read from. (By virtue of it being absent from the state). Dealing with a Stream Segment being merged becomes easy, because the last reader to reach the end of its pre-merge Stream Segment knows it can freely take ownership of the new Stream Segment. Readers can see their relative load and how they are progressing relative to the other Readers in their group and can decide to transfer Stream Segments if things are out of balance. This allows Readers to take action directly to ensure all the events are read without the need for some external tracker.","title":"Consistent Replicated State"},{"location":"reader-group-design/#reader-group-apis","text":"The external APIs to manage Reader Groups could be added to the ReaderGroupManager object. It consist of: ReaderGroup createReaderGroup ( String name , Stream stream , ReaderGroupConfig config ); ReaderGroup getReaderGroup ( String name , Stream stream ); void deleteReaderGroup ( ReaderGroup group ); When a Reader Group is created, it creates a State Synchronizer shared by the Readers. To join a Reader Group, Readers would just specify it in their configuration: ClientFactory clientFactory = ClientFactory . withScope ( scope , controllerURI ); EventStreamReader < T > reader = clientFactory . createReader ( readerId , READER_GROUP_NAME , serializer , readerConfig ); The Readers, while joining the group, access the information stored on the state to determine which Stream Segments to read from. Once when they shut down, they update the state so that other Readers can take over their Stream Segments.","title":"Reader Group APIs"},{"location":"reader-group-design/#detecting-pravega-reader-failure","text":"We still need some effective mechanism to identify, whether Readers are alive or not. The problem is greatly simplified because it need not produce a view of the cluster or manage any state. The component would just need to detect a Reader failure and invoke the void ReaderOffline(String ReaderId, Position lastPosition); API on the Reader Group. For consistency, the \"Failure detector\" should not declare a host as dead that is still processing events. Doing so could violate exactly once processing guarantees.","title":"Detecting Pravega Reader Failure"},{"location":"reader-group-design/#new-reader-added","text":"When a Reader joins a group its online status is added to the shared state. Other Readers receive updates to the shared state. When a Reader with more than average number of Stream Segments sees the new Reader, it may give up a Stream Segment by writing its position for that Stream Segment to the shared state. The new Reader can take over a Stream Segment by recording that it is doing so in the shared state. The new Reader can start reading from the position it read from the shared state for the Stream Segment it picked up. There are no races between multiple Readers coming online concurrently because only one of them can successfully claim ownership of any given Stream Segment.","title":"New Reader Added"},{"location":"reader-group-design/#stream-segments-get-merged","text":"When a Reader comes to the end of its Stream Segment it records this information in the shared state. When all of the Stream Segments that are getting merged together are completed, a Reader may claim ownership of the following Stream Segment. There is no ambiguity as to who the owner is, because it is stored in the shared state. There is no risk of a Stream Segment being ignored because every Reader can see the available Stream Segments by looking at the shared state and claim them.","title":"Stream Segments Get Merged"},{"location":"reader-group-design/#reader-offline","text":"When a Reader dies, the void ReaderOffline(String ReaderId, Position lastPosition); API method will be invoked either by the Reader itself in a graceful shutdown (internally to the close method) or via a \"liveness detector\". In either case the Reader's last position is written to the state. If a null Position is sent then the last checkpointed position will be written to the state. This is used by the newer Readers when they take ownership of the Stream Segment(s) that were read by the older/offline reader. Any Reader can decide to take over one or more of the Stream Segments owned by the old Reader from where it left off by recording their intention to do so in the state object. Once the state has been updated by the new Reader, it is considered the owner of the Stream Segment and can read from it.","title":"Reader Offline"},{"location":"reader-group-design/#other-considerations-on-reader-groups","text":"","title":"Other Considerations on Reader Groups"},{"location":"reader-group-design/#what-happens-if-a-reader-does-not-keep-up-to-date","text":"A Reader with out-of-date state can read from their existing Stream Segments without interference. The only disadvantage to this is that they will not shed load to another Reader should one become available. However, because they have to write to the shared state to start reading from any Stream Segment which they don't already own, they must fetch up-to-date information before moving on to a new Stream Segment.","title":"What happens if a Reader does not keep up to date?"},{"location":"reader-group-design/#impact-of-availability-and-latency","text":"Reading and updating the state object can occur in parallel to reading, so there would likely be no visible latency impact. A stream would be unavailable for reading if Pravega failed in such a way that the Stream Segment containing the Reader Group information went down and remained offline for long enough for the Readers to exhaust all the events in their existing Stream Segments. Of course, if Pravega failed in this way, odds are at least some portion of the stream would also be directly impacted and not be able to read any events. This sort of failure mode would manifest as latency for the Reader, similar to what would happen if they had reached the tail of the stream. This is preferable to using an external system to manage this coordination, as that would require adding new components that can fail in different ways, as opposed to further relying on a small set that we need to make highly available anyway. This is particularly notable in the case of a network partition. If the network is split any Readers that are on the same side of the partition as the Pravega servers can continue working. If we were to utilize an external service, that service could be cut off and Readers might not be able to make progress even if they could talk to Pravega.","title":"Impact of availability and latency"},{"location":"reader-group-notifications/","text":"Working with Pravega: ReaderGroup Notifications \u00b6 The ReaderGroup api supports different types of notifications. Currently, we have two types implemented, but we plan to add more over time. The types we currently support are the following: Segment Notification A segment notification is triggered when the total number of segments managed by the reader group changes. During a scale operation segments can be split into multiple or merged into some other segment causing the total number of segments to change. The total number of segments can also change when the configuration of the reader group changes, for example, when it adds or removes a stream. The method for subscribing to segment notifications is shown below @Cleanup ReaderGroupManager groupManager = new ReaderGroupManagerImpl ( SCOPE , controller , clientFactory , connectionFactory ); groupManager . createReaderGroup ( GROUP_NAME , ReaderGroupConfig . builder (). . stream ( Stream . of ( SCOPE , STREAM )) . build ()); groupManager . getReaderGroup ( GROUP_NAME ). getSegmentNotifier ( executor ). registerListener ( segmentNotification -> { int numOfReaders = segmentNotification . getNumOfReaders (); int segments = segmentNotification . getNumOfSegments (); if ( numOfReaders < segments ) { //Scale up number of readers based on application capacity } else { //More readers available time to shut down some } }); The application can register a listener to be notified of SegmentNotification using the registerListener api. This api takes io.pravega.client.stream.notifications.Listener as a parameter. Here the application can add custom logic to change the set of online readers according to the number of segments. For example, if the number of segments increases, then application might consider increasing the number of online readers. If the number of segments instead decreases according to a segment notification, then the application might want to change the set of online readers accordingly. EndOfData Notification An end of data notifier is triggered when the readers have read all the data of the stream(s) managed by the reader group. This is useful to process the stream data with a batch job where the application wants to read data of sealed stream(s). The method for subscribing to end of data notifications is shown below @Cleanup ReaderGroupManager groupManager = new ReaderGroupManagerImpl ( SCOPE , controller , clientFactory , connectionFactory ); groupManager . createReaderGroup ( GROUP_NAME , ReaderGroupConfig . builder () . stream ( Stream . of ( SCOPE , SEALED_STREAM )) . build ()); groupManager . getReaderGroup ( GROUP_NAME ). getEndOfDataNotifier ( executor ). registerListener ( notification -> { //custom action e.g: close all readers }); The application can register a listener to be notified of EndOfDataNotification using the registerListener api. This api takes io.pravega.client.stream.notifications.Listener as a parameter. Here the application can add custom logic that can be invoked once all the data of the sealed streams are read.","title":"Working with Reader Group notifications"},{"location":"reader-group-notifications/#working-with-pravega-readergroup-notifications","text":"The ReaderGroup api supports different types of notifications. Currently, we have two types implemented, but we plan to add more over time. The types we currently support are the following: Segment Notification A segment notification is triggered when the total number of segments managed by the reader group changes. During a scale operation segments can be split into multiple or merged into some other segment causing the total number of segments to change. The total number of segments can also change when the configuration of the reader group changes, for example, when it adds or removes a stream. The method for subscribing to segment notifications is shown below @Cleanup ReaderGroupManager groupManager = new ReaderGroupManagerImpl ( SCOPE , controller , clientFactory , connectionFactory ); groupManager . createReaderGroup ( GROUP_NAME , ReaderGroupConfig . builder (). . stream ( Stream . of ( SCOPE , STREAM )) . build ()); groupManager . getReaderGroup ( GROUP_NAME ). getSegmentNotifier ( executor ). registerListener ( segmentNotification -> { int numOfReaders = segmentNotification . getNumOfReaders (); int segments = segmentNotification . getNumOfSegments (); if ( numOfReaders < segments ) { //Scale up number of readers based on application capacity } else { //More readers available time to shut down some } }); The application can register a listener to be notified of SegmentNotification using the registerListener api. This api takes io.pravega.client.stream.notifications.Listener as a parameter. Here the application can add custom logic to change the set of online readers according to the number of segments. For example, if the number of segments increases, then application might consider increasing the number of online readers. If the number of segments instead decreases according to a segment notification, then the application might want to change the set of online readers accordingly. EndOfData Notification An end of data notifier is triggered when the readers have read all the data of the stream(s) managed by the reader group. This is useful to process the stream data with a batch job where the application wants to read data of sealed stream(s). The method for subscribing to end of data notifications is shown below @Cleanup ReaderGroupManager groupManager = new ReaderGroupManagerImpl ( SCOPE , controller , clientFactory , connectionFactory ); groupManager . createReaderGroup ( GROUP_NAME , ReaderGroupConfig . builder () . stream ( Stream . of ( SCOPE , SEALED_STREAM )) . build ()); groupManager . getReaderGroup ( GROUP_NAME ). getEndOfDataNotifier ( executor ). registerListener ( notification -> { //custom action e.g: close all readers }); The application can register a listener to be notified of EndOfDataNotification using the registerListener api. This api takes io.pravega.client.stream.notifications.Listener as a parameter. Here the application can add custom logic that can be invoked once all the data of the sealed streams are read.","title":"Working with Pravega: ReaderGroup Notifications"},{"location":"roadmap/","text":"Pravega Roadmap \u00b6 Version 0.3 \u00b6 The following will be the primary feature focus areas for our upcoming release. Retention Policy Implementation \u00b6 Retention policies allow an operator to define a specific Stream size or data age. Any data beyond this threshold will be automatically purged. Transactions API \u00b6 The current transactions API is functional, however it is cumbersome and requires detailed knowledge of Pravega to configure appropriate values such as timeouts. This work will simplify the API and automate as many timeouts as possible. Exactly Once Guarantees \u00b6 Focus is on testing and strengthening exactly once guarantees and correctness under failure conditions. Low-level Reader API \u00b6 This will expose a new low-level reader API that provides access the low level byte stream as opposed to the event semantics offered by the Java Client API. This can also be leveraged in the future to build different flavors of reader groups. Security \u00b6 Security for this release will focus on support for securing Pravega external interfaces along with basic access controls on stream access and administration. - Access Control on Stream operations. - Auth between Clients and Controller/SegmentStore. - Auth between SegmentStore and Tier 2 Storage. Pravega Connectors \u00b6 Pravega ecosystem interconnectivity will be augmented with the following: - Expanded Flink connector support (batch & table API support). - Logstash connector. - Others also under consideration (Interested in writing a connector? Talk to us on Slack ) Future Items \u00b6 The following items are new features that we wish to build in upcoming Pravega releases, however many active work is currently underway. Please reach out on the Pravega channels if you're interested in picking one of these up. Operational Features Non-disruptive and rolling upgrades for Pravega Provide default Failure Detector Exposing information for administration purposes Ability to define throughput quotas and other QoS guarantees Pravega Connectors / Integration Kafka API Compatibility (Producer and Consumer APIs) Spark connectors (source/sink) REST Proxy for Reader/Writer (REST proxy for Admin operations is already there) Stream Management Stream aliasing Ability to logically group multiple Streams Ability to assign arbitrary Key-Value pairs to streams - Tagging Tiering Support Policy driven tiering of Streams from Streaming Storage to Long-term storage Support for additional Tier 2 Storage backends","title":"Pravega Roadmap"},{"location":"roadmap/#pravega-roadmap","text":"","title":"Pravega Roadmap"},{"location":"roadmap/#version-03","text":"The following will be the primary feature focus areas for our upcoming release.","title":"Version 0.3"},{"location":"roadmap/#retention-policy-implementation","text":"Retention policies allow an operator to define a specific Stream size or data age. Any data beyond this threshold will be automatically purged.","title":"Retention Policy Implementation"},{"location":"roadmap/#transactions-api","text":"The current transactions API is functional, however it is cumbersome and requires detailed knowledge of Pravega to configure appropriate values such as timeouts. This work will simplify the API and automate as many timeouts as possible.","title":"Transactions API"},{"location":"roadmap/#exactly-once-guarantees","text":"Focus is on testing and strengthening exactly once guarantees and correctness under failure conditions.","title":"Exactly Once Guarantees"},{"location":"roadmap/#low-level-reader-api","text":"This will expose a new low-level reader API that provides access the low level byte stream as opposed to the event semantics offered by the Java Client API. This can also be leveraged in the future to build different flavors of reader groups.","title":"Low-level Reader API"},{"location":"roadmap/#security","text":"Security for this release will focus on support for securing Pravega external interfaces along with basic access controls on stream access and administration. - Access Control on Stream operations. - Auth between Clients and Controller/SegmentStore. - Auth between SegmentStore and Tier 2 Storage.","title":"Security"},{"location":"roadmap/#pravega-connectors","text":"Pravega ecosystem interconnectivity will be augmented with the following: - Expanded Flink connector support (batch & table API support). - Logstash connector. - Others also under consideration (Interested in writing a connector? Talk to us on Slack )","title":"Pravega Connectors"},{"location":"roadmap/#future-items","text":"The following items are new features that we wish to build in upcoming Pravega releases, however many active work is currently underway. Please reach out on the Pravega channels if you're interested in picking one of these up. Operational Features Non-disruptive and rolling upgrades for Pravega Provide default Failure Detector Exposing information for administration purposes Ability to define throughput quotas and other QoS guarantees Pravega Connectors / Integration Kafka API Compatibility (Producer and Consumer APIs) Spark connectors (source/sink) REST Proxy for Reader/Writer (REST proxy for Admin operations is already there) Stream Management Stream aliasing Ability to logically group multiple Streams Ability to assign arbitrary Key-Value pairs to streams - Tagging Tiering Support Policy driven tiering of Streams from Streaming Storage to Long-term storage Support for additional Tier 2 Storage backends","title":"Future Items"},{"location":"segment-containers/","text":"Segment Containers in a Pravega Cluster \u00b6 This document describes the high level design of how we are managing the lifecyle of Segment Containers in a Pravega Cluster. Segment Containers \u00b6 In this document we refer to a Segment Container as Container. The total number of Containers is fixed for a given deployment. Each Container can be owned by only one Pravega host and all Containers in the cluster should be running at any given point in time. Pravega Host \u00b6 A Pravega host is an instance of a Pravega service which owns and executes a set of Containers. Detecting Active Pravega Hosts \u00b6 Every Pravega host on startup will register itself with Zookeeper using ephemeral nodes. The ephemeral nodes are present in Zookeeper as long as Zookeeper receives appropriate signals from the Pravega host. These ephemeral nodes are used to detect the active Pravega hosts in the cluster. Monitoring the Pravega Cluster \u00b6 Each Pravega Controller runs a service which monitors the ephemeral nodes on the Zookeeper and detects all active Pravega hosts in the cluster. If any changes are detected to the cluster membership, then all Containers are verified and re-mapped to the available set of Pravega hosts. This information is persisted in the HostStore atomically. This is stored as a single blob and contains a map of Host to set of Containers that a host owns. We use Zookeeper to ensure only one Pravega Controller is monitoring the cluster to avoid multiple simultaneous Writers to the HostStore. This will avoid race conditions and allow the state to converge faster. Rebalance Frequency \u00b6 When a Pravega Host is added or removed from the cluster, rebalancing of the Container ownership happens. As it is an expensive operation, Pravega maintains a configured minimum time interval between any two rebalance operations. It ends up in proportionally increasing more time for performing ownership change in the cluster, if the rebalance operation is delayed due to some reason. Ownership Change Notification \u00b6 Every Pravega host has a long running Segment Manager Service. This constantly polls/watches the HostStore for any changes to the Container ownership. On detecting any ownership changes for itself (identified by the host key in the Map) the Segment Manager triggers addition and removal of Containers accordingly.","title":"Segment Containers"},{"location":"segment-containers/#segment-containers-in-a-pravega-cluster","text":"This document describes the high level design of how we are managing the lifecyle of Segment Containers in a Pravega Cluster.","title":"Segment Containers in a Pravega Cluster"},{"location":"segment-containers/#segment-containers","text":"In this document we refer to a Segment Container as Container. The total number of Containers is fixed for a given deployment. Each Container can be owned by only one Pravega host and all Containers in the cluster should be running at any given point in time.","title":"Segment Containers"},{"location":"segment-containers/#pravega-host","text":"A Pravega host is an instance of a Pravega service which owns and executes a set of Containers.","title":"Pravega Host"},{"location":"segment-containers/#detecting-active-pravega-hosts","text":"Every Pravega host on startup will register itself with Zookeeper using ephemeral nodes. The ephemeral nodes are present in Zookeeper as long as Zookeeper receives appropriate signals from the Pravega host. These ephemeral nodes are used to detect the active Pravega hosts in the cluster.","title":"Detecting Active Pravega Hosts"},{"location":"segment-containers/#monitoring-the-pravega-cluster","text":"Each Pravega Controller runs a service which monitors the ephemeral nodes on the Zookeeper and detects all active Pravega hosts in the cluster. If any changes are detected to the cluster membership, then all Containers are verified and re-mapped to the available set of Pravega hosts. This information is persisted in the HostStore atomically. This is stored as a single blob and contains a map of Host to set of Containers that a host owns. We use Zookeeper to ensure only one Pravega Controller is monitoring the cluster to avoid multiple simultaneous Writers to the HostStore. This will avoid race conditions and allow the state to converge faster.","title":"Monitoring the Pravega Cluster"},{"location":"segment-containers/#rebalance-frequency","text":"When a Pravega Host is added or removed from the cluster, rebalancing of the Container ownership happens. As it is an expensive operation, Pravega maintains a configured minimum time interval between any two rebalance operations. It ends up in proportionally increasing more time for performing ownership change in the cluster, if the rebalance operation is delayed due to some reason.","title":"Rebalance Frequency"},{"location":"segment-containers/#ownership-change-notification","text":"Every Pravega host has a long running Segment Manager Service. This constantly polls/watches the HostStore for any changes to the Container ownership. On detecting any ownership changes for itself (identified by the host key in the Map) the Segment Manager triggers addition and removal of Containers accordingly.","title":"Ownership Change Notification"},{"location":"segment-store-service/","text":"Pravega Segment Store Service \u00b6 Introduction Terminology Architecture System diagram Components Segment Containers Segment Container Metatdata Container Metadata Segment Metadata Log Operations Durable Log Information Flow Truncation Operation Processor Operation Metadata Updater Durable Data Log In-Memory Operation Log Read Index Cache Storage Writer Integration with Controller Segment Container Manager Storage Abstraction Data Flow Appends Reads Synchronization with Tier 2 Container Startup Introduction \u00b6 The Pravega Segment Store Service is a subsystem that lies at the heart of the entire Pravega deployment. It is the main access point for managing Stream Segments , providing the ability to create , delete and modify/access their contents. The Pravega client communicates with the Pravega Stream Controller to figure out which Stream Segments need to be used (for a Stream), and both the Stream Controller and the client deal with the Segment Store Service to operate on them. The basic idea behind the Segment Store Service is that it buffers the incoming data in a very fast and durable append-only medium (Tier 1), and syncs it to a high-throughput (but not necessarily low latency) system (Tier 2) in the background, while aggregating multiple (smaller) operations to a Stream Segment into a fewer (but larger) ones. The Pravega Segment Store Service can provide the following guarantees: Stream Segments that are unlimited in length, with append-only semantics, yet supporting arbitrary-offset reads. No throughput degradation when performing small appends, regardless of the performance of the underlying Tier 2 Storage system. Multiple concurrent writers to the same Stream Segment. the order is guaranteed within the context of a single Writer, but appends from multiple concurrent Writers will be added in the order in which they were received (appends are atomic without interleaving their contents). Writing to and reading from a Stream Segment concurrently with relatively low latency between writing and reading. Terminology \u00b6 The following terminologies are used throughout the document: Stream Segment or Segment : A contiguous sequence of bytes, similar to a file of unbounded size. This is a part of a Stream, limited both temporally and laterally (by key). The scope of Streams and mapping Stream Segments to such Streams is beyond the purpose of this document. Tier 2 Storage or Permanent Storage : The final resting place of the data. Tier 1 Storage : Fast append storage, used for durable buffering of incoming appends before distributing to Tier 2 Storage. Cache : A key-value local cache with no expectation of durability. Pravega Segment Store Service or Segment Store : The Service that this document describes. Transaction : A sequence of appends that are related to a Segment, which, if persisted, would make up a contiguous range of bytes within it. This is used for ingesting very large records or for accumulating data that may or may not be persisted into the Segment (but its fate cannot be determined until later in the future). Note: At the Pravega level, a Transaction applies to an entire Stream. In this document, a Transaction applies to a single Segment. Architecture \u00b6 The Segment Store is made up of the following components: Pravega Node : A host running a Pravega Process. Stream Segment Container (or Segment Container ): A logical grouping of Stream Segments. The mapping of Segments to Containers is deterministic and does not require any persistent store; Segments are mapped to Containers via a hash function (based on the Segment's name). Durable Data Log Adapter (or Durable Data Log ): An abstraction layer for Tier 1 Storage. Storage Adapter : An abstraction layer for Tier 2 Storage. Cache : An abstraction layer for append data caching. Streaming Client : An API that can be used to communicate with the Pravega Segment Store. Segment Container Manager : A component that can be used to determine the lifecycle of Segment Containers on a Pravega Node. This is used to start or stop Segment Containers based on an external coordination service (such as the Pravega Controller). The Segment Store handles writes by first writing them to a log ( Durable Data Log ) on a fast storage (SSDs preferably) and immediately acking back to the client after they have been persisted there. Subsequently, those writes are then aggregated into larger chunks and written in the background to Tier 2 Storage. Data for appends that have been acknowledged (and are in Tier 1) but not yet in Tier 2 is stored in the Cache (in addition to Tier 1). Once such data has been written to Tier 2 Storage, it may or may not be kept in the Cache, depending on a number of factors, such as Cache utilization/pressure and access patterns. More details about each component described above can be found in the Components section. System Diagram \u00b6 In the above diagram, the major components of the Segment Store are shown. But for simplicity, only one Segment Container is depicted. All Container components and major links between them (how they interact with each other) are shown. The Container Metadata component is not shown, because every other component communicates with it in one form or another and adding it would only clutter the diagram. More detailed diagrams can be found under the Data Flow section. Components \u00b6 Segment Containers \u00b6 Segment Containers are a logical grouping of Segments and are responsible for all operations on those Segments within their span. A Segment Container is made of multiple sub-components: Segment Container Metadata : A collection of Segment-specific metadata that describes the current state of each Segment (how much data in Tier 2, how much in Tier 1, whether it is sealed, etc.), as well as other miscellaneous info about each Container. Durable Log : The Container writes every operation it receives to this log and acks back only when the log says it has been accepted and durably persisted. Read Index : An in-memory index of where data can be read from. The Container delegates all read requests to it, and it is responsible for fetching the data from wherever it is currently located (Cache, Tier 1 Storage or Tier 2 Storage). Cache : Used to store data for appends that exist in Tier 1 only (not yet in Tier 2), as well as blocks of data that support reads. Storage Writer : Processes the durable log operations and applies them to Tier 2 Storage (in the order in which they were received). This component is also the one that coalesces multiple operations together, for better back-end throughput. Segment Container Metadata \u00b6 The Segment Container Metadata is critical to the good functioning and synchronization of its components. This metadata is shared across all components and it comes at two levels: \"Container-wide metadata\" and \"per-Segment metadata\". Each serves a different purpose and is described below. Container Metadata \u00b6 Each Segment Container needs to keep some general-purpose metadata that affects all operations inside the container: Operation Sequence Number : The largest sequence number assigned by the Durable Log . Every time a new operation is received and successfully processed by the Durable Log , this number is incremented (its value will never decrease or otherwise rollback, even if an operation failed to be persisted). The operation sequence number is guaranteed to be strict-monotonic increasing (no two operations have the same value, and an operation will always have a larger sequence number than all operations before it). Epoch : A number that is incremented every time a successful recovery (Container Start) happens. This value is durably incremented and stored as part of recovery and can be used for a number of cases (a good use is Tier 2 fencing for HDFS, which doesn't provide a good, native mechanism for that). Active Segment Metadata : Keeps information about each active Stream Segment. A Segment is active if it has had activity (read or write) recently and is currently loaded in memory. If a Stream Segment is idle for a while, or if there are many Stream Segments currently active, a Stream Segment becomes inactive by having its outstanding metadata flushed to Tier 2 Storage and evicted from memory. Tier 1 Metadata : Various pieces of information that can be used to accurately truncate the Tier 1 Storage Log once all operations prior to that point have been durably stored to Tier 2. Checkpoints : Container metadata is periodically Checkpointed by having its entire snapshot (including Active Segments) serialized to Tier 1. A Checkpoint serves as a Truncation Point for Tier 1, as it contains all the updates that have been made to the Container via all the processed operations before it, so we no longer need those operations in order to reconstruct the metadata. If we truncate Tier 1 on a Checkpoint, then we can use information from Tier 2 and this Checkpoint to reconstruct by using the previously available metadata, without relying on any operation prior to it in Tier 1. Segment Metadata \u00b6 Each Segment Container needs to keep per-segment metadata, which it uses to keep track of the state of each segment as it processes operations for it. The metadata can be volatile (it can be fully rebuilt upon recovery) and contains the following properties for each segment that is currently in use: Name : The name of the Stream Segment. Id : Internally assigned unique Stream Segment ID. This is used to refer to Stream Segments, which is preferred to the Name. This ID is used for the entire lifetime of the Stream Segment, which means that even if the Stream Segment becomes inactive, a future reactivation will have it mapped to the same ID. StartOffset (also known as TruncationOffset ): The lowest offset of the data that is available for reading. A non-truncated Stream Segment will have Start Offset equal to 0 , while subsequent Truncate operations will increase (but never decreases) this number. StorageLength : The highest offset of the data that exists in Tier 2 Storage. Length : The highest offset of the committed data in Tier 1 Storage. LastModified : The timestamp of the last processed (and acknowledged) append. IsSealed : Whether the Stream Segment is closed for appends (this value may not have been applied to Tier 2 Storage yet). IsSealedInStorage : Whether the Stream Segment is closed for appends (and this has been persisted in Tier 2 Storage). IsMerged : Whether the Stream Segment has been merged into another one (but this has not yet been persisted in Tier 2 Storage). This only applies for Transactions. Once the merge is persisted into Tier 2, the Transaction Segment does not exist anymore (so IsDeleted will become true). IsDeleted : Whether the Stream Segment is deleted or has recently been merged into another Stream Segment. This only applies for recently deleted Stream Segments, and not for Stream Segments that never existed. The following are always true for any Stream Segment: StorageLength <= Length StartOffset <= Length Log Operations \u00b6 The Log Operation is a basic unit that is enqueued in the Durable Log . It does not represent an action, per se, but is the base for several serializable operations (we can serialize multiple types of operations, not just Appends). Each operation is the result of an external action (which denote the alteration of a Stream Segment), or an internal trigger, such as metadata maintenance operations. Every Log operation has the following elements: SequenceNumber : The unique sequence number assigned to this entry (see more under Container Metadata ) section. The following are the various types of Log operations: Storage Operations : Represent operations that need to be applied to the underlying Tier 2 Storage: StreamSegmentAppendOperation : Represents an append to a particular Stream Segment. CachedStreamSegmentAppendOperation : Same as StreamSegmentAppendOperation , but this is for internal use (instead of having an actual data payload, it points to a location in the Cache from where the data can be retrieved). StreamSegmentSealOperation : When processed, it sets a flag in the in-memory metadata that no more appends can be received. When the Storage Writer processes it, it marks the Stream Segment as read-only in Tier 2 Storage. StreamSegmentTruncateOperation : Truncates a Stream Segment at a particular offset. This causes the Stream Segment's StartOffset to change. MergeTransactionOperation : Indicates that a Transaction is to be merged into its parent Stream Segment. Metadata Operations are auxiliary operations that indicate a change to the Container metadata. They can be the result of an external operation (we received a request for a Stream Segment we never knew about before, so we must assign a \"unique ID\" to it) or to snapshot the entire metadata (which helps with recovery and cleaning up Tier 1 Storage). The purpose of the metadata operations is to reduce the amount of time needed for failover recovery (when needed). StreamSegmentMapOperation : Maps an ID to a Stream Segment Name. TransactionMapOperation : Maps an ID to a Transaction and to its Parent Segment. UpdateAttributesOperation : Updates any attributes on a Stream Segment. MetadataCheckpoint : Includes an entire snapshot of the metadata. This can be useful during recovery. This contains all metadata up to this point, which is a sufficient base for all operations after it. Durable Log \u00b6 The Durable Log is the central component that handles all Log operations. All operations (which are created by the Container) are added to the Durable Log , which processes them in the order in which they were received. It is made up of a few other components, all of which are working towards a single goal of processing all incoming operations as quickly as possible, without compromising data integrity. Information Flow in the Durable Log \u00b6 All received operations are added to an Operation Queue (the caller receives a Future which will be completed when the operation is durably persisted). The Operation Processor picks all operations currently available in the queue (if the queue is empty, it will wait until at least one operation is added). The Operation Processor runs as a continuous loop (in a background thread), and executes the following steps. Dequeue all outstanding operations from the operation Queue (described above). Pre-process the operations (validate that they are correct and would not cause undesired behavior, assign offsets (where needed), assign sequence numbers, etc.) Write the operations to a Data Frame Builder , which serializes and packs the operations in Data Frames . Once a Data Frame is complete (full or no more operations to add), the Data Frame Builder sends the Data Frame to the Durable Data Log . Note that, an operation may span multiple DataFrames , but the goal is to make the best use of the Durable Data Log throughput capacity by making writes as large as possible considering the maximum size limit per write. When a Data Frame has been durably persisted in the Durable Data Log , the operation Processor post-processes all operations that were fully written so far. It adds them to in-memory structures, updates indices, etc., and completes the Futures associated with them. The Operation Processor works asynchronously, by not waiting for a particular Data Frame to be written before starting another one and sending it to the Durable Data Log . Likewise, multiple Data Frames may be in flight by maintaining a specific order. The operation Processor relies on certain ordering guarantees from the Durable Data Log , if a particular Data Frame was acked, it assures that all the prior Data Frames to it were also committed successfully, in the right order. Note: The operation Processor does not do any write throttling. It leaves that to the Durable Data Log implementation, but it controls the size of the Data Frames that get sent to it. Truncation \u00b6 Based on supplied configuration, the Durable Log auto-adds a special kind of operation, named MetadataCheckpointOperation . This operation, when processed by the operation Processor, collects a snapshot of the entire Container metadata and serializes it to the Durable Data Log . This special operation marks a Truncation Point - a place in the Stream of Log operations where we can issue Truncate operations. It is very important that after every truncation, the first operation to be found in the log is a MetadataCheckpointOperation , because without the prior operations to reconstruct metadata, this is the only way to be able to process subsequent operations. Note: Durable Data Log (Tier 1) truncation should not be confused with Segment Truncation. They serve different purposes and are applied to different targets. Operation Processor \u00b6 The Operation Processor is a sub-component of the Durable Log that deals with incoming Log operations. Its purpose is to validate, persist , and update metadata and other internal structures based on the contents of each operation. Operation Metadata Updater \u00b6 The Operation Metadata Updater is a sub-component of the Durable Log that is responsible with validating operations based on the current state of the metadata, as well as update the metadata after a successful commit of an operation. Internally it has various mechanisms to handle failures, and it can rollback certain changes in failure situations. Durable Data Log \u00b6 The Durable Data Log is an abstraction layer to an external component that provides append-only semantics. It is supposed to be a system which provides very fast appends to a log, that guarantees the durability and consistency of the written data. The read performance is not so much a factor, because we do not read directly from this component. Read is performed on it when we need to recover the contents of the Durable Log . As explained above, Log operations are serialized into Data Frames (with a single operation able to span multiple such Frames if needed), and these Data Frames are then serialized as entries into this Durable Data Log . This is used only as a fail-safe, and we only need to read these Frames back if we need to perform recovery (in which case we need to deserialize all Log operations contained in them, in the same order in which they were received). In-Memory Operation Log \u00b6 The In-Memory Operation Log contains committed (and replicated) Log operations in the exact same order as they were added to the Durable Data Log . While the Durable Data Log contains a sequence of Data Frames (which contain serializations of operations), the Memory Log contains the actual operations, which can be used throughout the Durable Log and the Storage Writer. The Memory Log is essentially a chain of Log operations ordered by the time when the operation was received. We always add at one end, and we remove from the other. When we truncate the Durable Data Log the Memory Log is also truncated at the same location. Read Index \u00b6 The Read Index helps the Segment Container perform reads from Streams at arbitrary offsets. While the Durable Log records (and can only replay) data in the order in which it is received, the Read Index can access the data in a random fashion. The Read Index is made of multiple Segment Read Indices (one per live Segment). The Segment Read Index is a data structure that is used to serve reads from memory, as well as pull data from Tier 2 Storage and provides Future Reads (tail reads) when data is not yet available. When a read request is received, the Segment Read Index returns a read iterator that will return data as long as the read request parameters have not yet been satisfied. The iterator will either fetch data that is immediately available in memory, or request data from Tier 2 storage (and bring it to memory) or, if it reached the current end of the Segment, return a Future that will be completed when new data is added (thus providing tailing or future reads). At the heart of the Segment Read Index lies a sorted index of entries (indexed by their start offsets) which is used to locate the requested data when needed. The index itself is implemented by a custom balanced binary search tree (AVL Tree to be more precise) with a goal of minimizing memory usage while not sacrificing insert or access performance. The entries themselves do not contain data, rather some small amount of metadata that is used to locate the data in the Cache and to determine usage patterns (good for cache evictions). Cache \u00b6 The Cache is a component where all data (whether from new appends or that was pulled from Tier 2 storage) is stored. It is a direct memory store store entirely managed by the Read Index. Storage Writer \u00b6 Pravega is by no means the final resting place of the data, nor it is meant to be a storage service. The Tier 2 Storage is where we want data to be in the long term and Pravega is only used to store a very short tail-end of it (using Tier 1 Storage), enough to make appends fast and aggregate them into bigger chunks for committal to Tier 2 Storage. To perform this, it needs another component ( Storage Writer ) that reads data from the Durable Log in the order in which it was received, aggregates it, and sends it to Tier 2 Storage. Just like the Durable Log , there is one Storage Writer per Segment Container. Each Writer reads Log operations from the in-memory operation Log (exposed via the read() method in the Durable Log ) in the order they were processed. It keeps track of the last read item by means of its sequence number. This state is not persisted, and upon recovery, it can just start from the beginning of the available Durable Log . The Storage Writer can process any Storage operation ( Append, Seal, Merge ), and as Pravega being the sole actor it modifies such data in Tier 2 and applies them without any constraints. It has several mechanisms to detect and recover from possible data loss or external actors modifying data concurrently. Integration with Controller \u00b6 Methods for mapping Segment Containers to hosts and rules used for moving from one to another are beyond the scope of this document. Here, we just describe how the Segment Store Service interacts with the Controller and how it manages the lifecycle of Segment Containers based on external events. Segment Container Manager \u00b6 Each instance of a Segment Store Service needs a Segment Container Manager . The role of this component is to manage the lifecycle of the Segment Containers that are assigned to that node (service). It performs the following duties: Connects to the Controller Service-Side Client (i.e., a client that deals only with Segment Container events, and not with the management of Streams and listens to all changes that pertain to Containers that pertain to its own instance. When it receives a notification that it needs to start a Segment Container for a particular Container Id, it initiates the process of bootstrapping such an object. It does not notify the requesting client of success until the operation completes without error. When it receives a notification that it needs to stop a Segment Container for a particular Container Id, it initiates the process of shutting it down. It does not notify the requesting client of success until the operation completes without error. If the Segment Container shuts down unexpectedly (whether during Start or during its normal operation), it will not attempt to restart it locally; instead it will notify the Controller. Storage Abstractions \u00b6 The Segment Store was not designed with particular implementations for Tier 1 or Tier 2. Instead, all these components have been abstracted out in simple, well-defined interfaces, which can be implemented against any standard file system (Tier 2) or append-only log system (Tier 1). Possible candidates for Tier 1 Storage: Apache BookKeeper (preferred, adapter is fully implemented as part of Pravega) Non-durable, non-replicated solutions: In-Memory (Single node deployment only - Pravega becomes a volatile buffer for Tier 2 Storage; data loss is unavoidable and unrecoverable from in the case of process crash or system restart). This is used for unit test only. Local File System (Single node deployment only - Pravega becomes a semi-durable buffer for Tier 2 Storage; data loss is unavoidable and unrecoverable from in the case of complete node failure) Possible candidates for Tier 2 Storage: HDFS (Implementation available) Extended S3 (Implementation available) NFS (general FileSystem ) (Implementation available) In-Memory (Single node deployment only - limited usefulness; data loss is unavoidable and unrecoverable from in the case of process crash or system restart) This is used for unit test only. A note about Tier 2 Truncation : The Segment Store supports Segment truncation at a particular offset, which means that, once that request is complete, no offset below that one will be available for reading. The above is a metadata update operation, however this also needs to be supported by Tier 2 so that the truncated data is physically deleted from it. If a Tier 2 implementation does not natively support truncation from the beginning of a file with offset preservation (i.e., a Segment of length 100 is truncated at offset 50, then offsets 0..49 are deleted, but offsets 50-99 are available and are not shifted down), then the Segment Store provides a wrapper on top of a generic Tier 2 implementation that can do that. The RollingStorage Tier 2 wrapper splits a Segment into multiple Segment Chunks and exposes them as a single Segment to the upper layers. Segment Chunks that have been truncated out, are deleted automatically. This is not a very precise application (since it relies heavily on a rollover policy dictating granularity), but it is a practical solution for those cases when the real Tier 2 implementation does not provide the features that we need. Data Flow \u00b6 Here are a few examples of how data flows inside the Pravega Segment Store Service. Appends \u00b6 The diagram above depicts these steps (note the step numbers may not match, but the order is the same): Segment Store receives append request with params: Segment Name, Payload and Attribute Updates. Segment Store determines the Container ID for the given Segment and verifies that the Segment Container is registered locally. If not, it returns an appropriate error code. Segment Store delegates request to the appropriate Segment Container instance. Segment Container verifies that the Segment belongs to the Segment Container and that the Segment actually exists. If not, it returns an appropriate error code. During this process, it also gets an existing Segment ID or assigns a new one (by using the Segment Mapper component). Segment Container creates a StreamSegmentAppendOperation with the input data and sends it to the Durable Log . Durable Log takes the Append operation and processes it according to the algorithm described in the Durable Log section. Puts it in its operation Queue. Operation Processor pulls all operations off the Queue. Operation Processor uses the Data Frame Builder to construct Data Frames with the operations it has. Data Frame Builder asynchronously writes the Data Frame to the Durable Data Log . Upon completion, the following are done in parallel: Metadata is updated. The operation is added to the Memory Operation Log and Read Index . A call that triggered the operation is acked. The above process is asynchronous, which means the Operation Processor will have multiple Data Frames in flight (not illustrated). It will keep track of each one's changes and apply or roll them back as needed. This process applies for every single operation that the Segment Store supports. All modify operations go through the Operation Processor and have a similar path. Reads \u00b6 The diagram above depicts these steps (note the step numbers may not match, but the order is the same): Segment Store receives read request with params: Segment Name, Read Offset, Max-Length. Segment Store determines the Container ID for the given Segment and verifies if it is Leader for given Segment Container . If not, it returns an appropriate error code. Segment Store delegates request to the Segment Container instance. Segment Container verifies that the Segment belongs to that Container and that it actually exists. If not, it returns an appropriate error code to the client. During this process, it also gets an existing Segment ID or assigns a new one (by using the Segment Mapper component). Segment Container delegates the request to its Read Index , which processes the read as described in the Read Index section, by issuing Reads from Storage (for data that is not in the Cache ), and querying/updating the Cache as needed. Synchronization with Tier 2 (Storage Writer) \u00b6 The diagram above depicts these steps (note the step numbers may not match, but the order is the same): The Storage Writer 's main loop is the sub-component that triggers all these operations. Read next operation from the Durable Log (in between each loop, the Writer remembers what the sequence number of the last processed operation was). All operations are processed and added to the internal Segment Aggregators (one Aggregator per Segment). Eligible Segment Aggregators are flushed to Storage (eligibility depends on the amount of data collected in each aggregator, and whether there are any Seal, Merge or Truncate operations queued up). Each time an Append operation is encountered, a trip to the Read Index may be required in order to get the contents of the append. After every successful modification ( write/seal/concat/truncate ) to Storage , the Container Metadata is updated to reflect the changes. The Durable Log is truncated (if eligible). Container Startup (Normal/Recovery) \u00b6 The diagram above depicts these steps (note the step numbers may not match, but the order is the same): The Container Manager receives a request to start a Container in this instance of the Segment Store Service . It creates, registers, and starts the Container. The Container starts the Durable Log component. Durable Log initiates the recovery process (coordinated by the Recovery Executor ). Recovery Executor reads all Data Frames from Durable Data Log . Deserialized operations from the read Data Frames are added to the Memory Operation Log . The Container Metadata is updated by means of the Operation Metadata Updater (same as the one used inside Operation Processor). The Read Index is populated with the contents of those operations that apply to it. The Container Starts the Storage Writer . The Storage Writer 's Main Loop starts processing operations from the Durable Log , and upon first encountering a new Segment, it reconciles its content (and metadata) with the reality that exists in Storage . After both the Durable Log and the Storage Writer have started, the Container is ready to start accepting new external requests.","title":"Segment Store Service"},{"location":"segment-store-service/#pravega-segment-store-service","text":"Introduction Terminology Architecture System diagram Components Segment Containers Segment Container Metatdata Container Metadata Segment Metadata Log Operations Durable Log Information Flow Truncation Operation Processor Operation Metadata Updater Durable Data Log In-Memory Operation Log Read Index Cache Storage Writer Integration with Controller Segment Container Manager Storage Abstraction Data Flow Appends Reads Synchronization with Tier 2 Container Startup","title":"Pravega Segment Store Service"},{"location":"segment-store-service/#introduction","text":"The Pravega Segment Store Service is a subsystem that lies at the heart of the entire Pravega deployment. It is the main access point for managing Stream Segments , providing the ability to create , delete and modify/access their contents. The Pravega client communicates with the Pravega Stream Controller to figure out which Stream Segments need to be used (for a Stream), and both the Stream Controller and the client deal with the Segment Store Service to operate on them. The basic idea behind the Segment Store Service is that it buffers the incoming data in a very fast and durable append-only medium (Tier 1), and syncs it to a high-throughput (but not necessarily low latency) system (Tier 2) in the background, while aggregating multiple (smaller) operations to a Stream Segment into a fewer (but larger) ones. The Pravega Segment Store Service can provide the following guarantees: Stream Segments that are unlimited in length, with append-only semantics, yet supporting arbitrary-offset reads. No throughput degradation when performing small appends, regardless of the performance of the underlying Tier 2 Storage system. Multiple concurrent writers to the same Stream Segment. the order is guaranteed within the context of a single Writer, but appends from multiple concurrent Writers will be added in the order in which they were received (appends are atomic without interleaving their contents). Writing to and reading from a Stream Segment concurrently with relatively low latency between writing and reading.","title":"Introduction"},{"location":"segment-store-service/#terminology","text":"The following terminologies are used throughout the document: Stream Segment or Segment : A contiguous sequence of bytes, similar to a file of unbounded size. This is a part of a Stream, limited both temporally and laterally (by key). The scope of Streams and mapping Stream Segments to such Streams is beyond the purpose of this document. Tier 2 Storage or Permanent Storage : The final resting place of the data. Tier 1 Storage : Fast append storage, used for durable buffering of incoming appends before distributing to Tier 2 Storage. Cache : A key-value local cache with no expectation of durability. Pravega Segment Store Service or Segment Store : The Service that this document describes. Transaction : A sequence of appends that are related to a Segment, which, if persisted, would make up a contiguous range of bytes within it. This is used for ingesting very large records or for accumulating data that may or may not be persisted into the Segment (but its fate cannot be determined until later in the future). Note: At the Pravega level, a Transaction applies to an entire Stream. In this document, a Transaction applies to a single Segment.","title":"Terminology"},{"location":"segment-store-service/#architecture","text":"The Segment Store is made up of the following components: Pravega Node : A host running a Pravega Process. Stream Segment Container (or Segment Container ): A logical grouping of Stream Segments. The mapping of Segments to Containers is deterministic and does not require any persistent store; Segments are mapped to Containers via a hash function (based on the Segment's name). Durable Data Log Adapter (or Durable Data Log ): An abstraction layer for Tier 1 Storage. Storage Adapter : An abstraction layer for Tier 2 Storage. Cache : An abstraction layer for append data caching. Streaming Client : An API that can be used to communicate with the Pravega Segment Store. Segment Container Manager : A component that can be used to determine the lifecycle of Segment Containers on a Pravega Node. This is used to start or stop Segment Containers based on an external coordination service (such as the Pravega Controller). The Segment Store handles writes by first writing them to a log ( Durable Data Log ) on a fast storage (SSDs preferably) and immediately acking back to the client after they have been persisted there. Subsequently, those writes are then aggregated into larger chunks and written in the background to Tier 2 Storage. Data for appends that have been acknowledged (and are in Tier 1) but not yet in Tier 2 is stored in the Cache (in addition to Tier 1). Once such data has been written to Tier 2 Storage, it may or may not be kept in the Cache, depending on a number of factors, such as Cache utilization/pressure and access patterns. More details about each component described above can be found in the Components section.","title":"Architecture"},{"location":"segment-store-service/#system-diagram","text":"In the above diagram, the major components of the Segment Store are shown. But for simplicity, only one Segment Container is depicted. All Container components and major links between them (how they interact with each other) are shown. The Container Metadata component is not shown, because every other component communicates with it in one form or another and adding it would only clutter the diagram. More detailed diagrams can be found under the Data Flow section.","title":"System Diagram"},{"location":"segment-store-service/#components","text":"","title":"Components"},{"location":"segment-store-service/#segment-containers","text":"Segment Containers are a logical grouping of Segments and are responsible for all operations on those Segments within their span. A Segment Container is made of multiple sub-components: Segment Container Metadata : A collection of Segment-specific metadata that describes the current state of each Segment (how much data in Tier 2, how much in Tier 1, whether it is sealed, etc.), as well as other miscellaneous info about each Container. Durable Log : The Container writes every operation it receives to this log and acks back only when the log says it has been accepted and durably persisted. Read Index : An in-memory index of where data can be read from. The Container delegates all read requests to it, and it is responsible for fetching the data from wherever it is currently located (Cache, Tier 1 Storage or Tier 2 Storage). Cache : Used to store data for appends that exist in Tier 1 only (not yet in Tier 2), as well as blocks of data that support reads. Storage Writer : Processes the durable log operations and applies them to Tier 2 Storage (in the order in which they were received). This component is also the one that coalesces multiple operations together, for better back-end throughput.","title":"Segment Containers"},{"location":"segment-store-service/#segment-container-metadata","text":"The Segment Container Metadata is critical to the good functioning and synchronization of its components. This metadata is shared across all components and it comes at two levels: \"Container-wide metadata\" and \"per-Segment metadata\". Each serves a different purpose and is described below.","title":"Segment Container Metadata"},{"location":"segment-store-service/#container-metadata","text":"Each Segment Container needs to keep some general-purpose metadata that affects all operations inside the container: Operation Sequence Number : The largest sequence number assigned by the Durable Log . Every time a new operation is received and successfully processed by the Durable Log , this number is incremented (its value will never decrease or otherwise rollback, even if an operation failed to be persisted). The operation sequence number is guaranteed to be strict-monotonic increasing (no two operations have the same value, and an operation will always have a larger sequence number than all operations before it). Epoch : A number that is incremented every time a successful recovery (Container Start) happens. This value is durably incremented and stored as part of recovery and can be used for a number of cases (a good use is Tier 2 fencing for HDFS, which doesn't provide a good, native mechanism for that). Active Segment Metadata : Keeps information about each active Stream Segment. A Segment is active if it has had activity (read or write) recently and is currently loaded in memory. If a Stream Segment is idle for a while, or if there are many Stream Segments currently active, a Stream Segment becomes inactive by having its outstanding metadata flushed to Tier 2 Storage and evicted from memory. Tier 1 Metadata : Various pieces of information that can be used to accurately truncate the Tier 1 Storage Log once all operations prior to that point have been durably stored to Tier 2. Checkpoints : Container metadata is periodically Checkpointed by having its entire snapshot (including Active Segments) serialized to Tier 1. A Checkpoint serves as a Truncation Point for Tier 1, as it contains all the updates that have been made to the Container via all the processed operations before it, so we no longer need those operations in order to reconstruct the metadata. If we truncate Tier 1 on a Checkpoint, then we can use information from Tier 2 and this Checkpoint to reconstruct by using the previously available metadata, without relying on any operation prior to it in Tier 1.","title":"Container Metadata"},{"location":"segment-store-service/#segment-metadata","text":"Each Segment Container needs to keep per-segment metadata, which it uses to keep track of the state of each segment as it processes operations for it. The metadata can be volatile (it can be fully rebuilt upon recovery) and contains the following properties for each segment that is currently in use: Name : The name of the Stream Segment. Id : Internally assigned unique Stream Segment ID. This is used to refer to Stream Segments, which is preferred to the Name. This ID is used for the entire lifetime of the Stream Segment, which means that even if the Stream Segment becomes inactive, a future reactivation will have it mapped to the same ID. StartOffset (also known as TruncationOffset ): The lowest offset of the data that is available for reading. A non-truncated Stream Segment will have Start Offset equal to 0 , while subsequent Truncate operations will increase (but never decreases) this number. StorageLength : The highest offset of the data that exists in Tier 2 Storage. Length : The highest offset of the committed data in Tier 1 Storage. LastModified : The timestamp of the last processed (and acknowledged) append. IsSealed : Whether the Stream Segment is closed for appends (this value may not have been applied to Tier 2 Storage yet). IsSealedInStorage : Whether the Stream Segment is closed for appends (and this has been persisted in Tier 2 Storage). IsMerged : Whether the Stream Segment has been merged into another one (but this has not yet been persisted in Tier 2 Storage). This only applies for Transactions. Once the merge is persisted into Tier 2, the Transaction Segment does not exist anymore (so IsDeleted will become true). IsDeleted : Whether the Stream Segment is deleted or has recently been merged into another Stream Segment. This only applies for recently deleted Stream Segments, and not for Stream Segments that never existed. The following are always true for any Stream Segment: StorageLength <= Length StartOffset <= Length","title":"Segment Metadata"},{"location":"segment-store-service/#log-operations","text":"The Log Operation is a basic unit that is enqueued in the Durable Log . It does not represent an action, per se, but is the base for several serializable operations (we can serialize multiple types of operations, not just Appends). Each operation is the result of an external action (which denote the alteration of a Stream Segment), or an internal trigger, such as metadata maintenance operations. Every Log operation has the following elements: SequenceNumber : The unique sequence number assigned to this entry (see more under Container Metadata ) section. The following are the various types of Log operations: Storage Operations : Represent operations that need to be applied to the underlying Tier 2 Storage: StreamSegmentAppendOperation : Represents an append to a particular Stream Segment. CachedStreamSegmentAppendOperation : Same as StreamSegmentAppendOperation , but this is for internal use (instead of having an actual data payload, it points to a location in the Cache from where the data can be retrieved). StreamSegmentSealOperation : When processed, it sets a flag in the in-memory metadata that no more appends can be received. When the Storage Writer processes it, it marks the Stream Segment as read-only in Tier 2 Storage. StreamSegmentTruncateOperation : Truncates a Stream Segment at a particular offset. This causes the Stream Segment's StartOffset to change. MergeTransactionOperation : Indicates that a Transaction is to be merged into its parent Stream Segment. Metadata Operations are auxiliary operations that indicate a change to the Container metadata. They can be the result of an external operation (we received a request for a Stream Segment we never knew about before, so we must assign a \"unique ID\" to it) or to snapshot the entire metadata (which helps with recovery and cleaning up Tier 1 Storage). The purpose of the metadata operations is to reduce the amount of time needed for failover recovery (when needed). StreamSegmentMapOperation : Maps an ID to a Stream Segment Name. TransactionMapOperation : Maps an ID to a Transaction and to its Parent Segment. UpdateAttributesOperation : Updates any attributes on a Stream Segment. MetadataCheckpoint : Includes an entire snapshot of the metadata. This can be useful during recovery. This contains all metadata up to this point, which is a sufficient base for all operations after it.","title":"Log Operations"},{"location":"segment-store-service/#durable-log","text":"The Durable Log is the central component that handles all Log operations. All operations (which are created by the Container) are added to the Durable Log , which processes them in the order in which they were received. It is made up of a few other components, all of which are working towards a single goal of processing all incoming operations as quickly as possible, without compromising data integrity.","title":"Durable Log"},{"location":"segment-store-service/#information-flow-in-the-durable-log","text":"All received operations are added to an Operation Queue (the caller receives a Future which will be completed when the operation is durably persisted). The Operation Processor picks all operations currently available in the queue (if the queue is empty, it will wait until at least one operation is added). The Operation Processor runs as a continuous loop (in a background thread), and executes the following steps. Dequeue all outstanding operations from the operation Queue (described above). Pre-process the operations (validate that they are correct and would not cause undesired behavior, assign offsets (where needed), assign sequence numbers, etc.) Write the operations to a Data Frame Builder , which serializes and packs the operations in Data Frames . Once a Data Frame is complete (full or no more operations to add), the Data Frame Builder sends the Data Frame to the Durable Data Log . Note that, an operation may span multiple DataFrames , but the goal is to make the best use of the Durable Data Log throughput capacity by making writes as large as possible considering the maximum size limit per write. When a Data Frame has been durably persisted in the Durable Data Log , the operation Processor post-processes all operations that were fully written so far. It adds them to in-memory structures, updates indices, etc., and completes the Futures associated with them. The Operation Processor works asynchronously, by not waiting for a particular Data Frame to be written before starting another one and sending it to the Durable Data Log . Likewise, multiple Data Frames may be in flight by maintaining a specific order. The operation Processor relies on certain ordering guarantees from the Durable Data Log , if a particular Data Frame was acked, it assures that all the prior Data Frames to it were also committed successfully, in the right order. Note: The operation Processor does not do any write throttling. It leaves that to the Durable Data Log implementation, but it controls the size of the Data Frames that get sent to it.","title":"Information Flow in the Durable Log"},{"location":"segment-store-service/#truncation","text":"Based on supplied configuration, the Durable Log auto-adds a special kind of operation, named MetadataCheckpointOperation . This operation, when processed by the operation Processor, collects a snapshot of the entire Container metadata and serializes it to the Durable Data Log . This special operation marks a Truncation Point - a place in the Stream of Log operations where we can issue Truncate operations. It is very important that after every truncation, the first operation to be found in the log is a MetadataCheckpointOperation , because without the prior operations to reconstruct metadata, this is the only way to be able to process subsequent operations. Note: Durable Data Log (Tier 1) truncation should not be confused with Segment Truncation. They serve different purposes and are applied to different targets.","title":"Truncation"},{"location":"segment-store-service/#operation-processor","text":"The Operation Processor is a sub-component of the Durable Log that deals with incoming Log operations. Its purpose is to validate, persist , and update metadata and other internal structures based on the contents of each operation.","title":"Operation Processor"},{"location":"segment-store-service/#operation-metadata-updater","text":"The Operation Metadata Updater is a sub-component of the Durable Log that is responsible with validating operations based on the current state of the metadata, as well as update the metadata after a successful commit of an operation. Internally it has various mechanisms to handle failures, and it can rollback certain changes in failure situations.","title":"Operation Metadata Updater"},{"location":"segment-store-service/#durable-data-log","text":"The Durable Data Log is an abstraction layer to an external component that provides append-only semantics. It is supposed to be a system which provides very fast appends to a log, that guarantees the durability and consistency of the written data. The read performance is not so much a factor, because we do not read directly from this component. Read is performed on it when we need to recover the contents of the Durable Log . As explained above, Log operations are serialized into Data Frames (with a single operation able to span multiple such Frames if needed), and these Data Frames are then serialized as entries into this Durable Data Log . This is used only as a fail-safe, and we only need to read these Frames back if we need to perform recovery (in which case we need to deserialize all Log operations contained in them, in the same order in which they were received).","title":"Durable Data Log"},{"location":"segment-store-service/#in-memory-operation-log","text":"The In-Memory Operation Log contains committed (and replicated) Log operations in the exact same order as they were added to the Durable Data Log . While the Durable Data Log contains a sequence of Data Frames (which contain serializations of operations), the Memory Log contains the actual operations, which can be used throughout the Durable Log and the Storage Writer. The Memory Log is essentially a chain of Log operations ordered by the time when the operation was received. We always add at one end, and we remove from the other. When we truncate the Durable Data Log the Memory Log is also truncated at the same location.","title":"In-Memory Operation Log"},{"location":"segment-store-service/#read-index","text":"The Read Index helps the Segment Container perform reads from Streams at arbitrary offsets. While the Durable Log records (and can only replay) data in the order in which it is received, the Read Index can access the data in a random fashion. The Read Index is made of multiple Segment Read Indices (one per live Segment). The Segment Read Index is a data structure that is used to serve reads from memory, as well as pull data from Tier 2 Storage and provides Future Reads (tail reads) when data is not yet available. When a read request is received, the Segment Read Index returns a read iterator that will return data as long as the read request parameters have not yet been satisfied. The iterator will either fetch data that is immediately available in memory, or request data from Tier 2 storage (and bring it to memory) or, if it reached the current end of the Segment, return a Future that will be completed when new data is added (thus providing tailing or future reads). At the heart of the Segment Read Index lies a sorted index of entries (indexed by their start offsets) which is used to locate the requested data when needed. The index itself is implemented by a custom balanced binary search tree (AVL Tree to be more precise) with a goal of minimizing memory usage while not sacrificing insert or access performance. The entries themselves do not contain data, rather some small amount of metadata that is used to locate the data in the Cache and to determine usage patterns (good for cache evictions).","title":"Read Index"},{"location":"segment-store-service/#cache","text":"The Cache is a component where all data (whether from new appends or that was pulled from Tier 2 storage) is stored. It is a direct memory store store entirely managed by the Read Index.","title":"Cache"},{"location":"segment-store-service/#storage-writer","text":"Pravega is by no means the final resting place of the data, nor it is meant to be a storage service. The Tier 2 Storage is where we want data to be in the long term and Pravega is only used to store a very short tail-end of it (using Tier 1 Storage), enough to make appends fast and aggregate them into bigger chunks for committal to Tier 2 Storage. To perform this, it needs another component ( Storage Writer ) that reads data from the Durable Log in the order in which it was received, aggregates it, and sends it to Tier 2 Storage. Just like the Durable Log , there is one Storage Writer per Segment Container. Each Writer reads Log operations from the in-memory operation Log (exposed via the read() method in the Durable Log ) in the order they were processed. It keeps track of the last read item by means of its sequence number. This state is not persisted, and upon recovery, it can just start from the beginning of the available Durable Log . The Storage Writer can process any Storage operation ( Append, Seal, Merge ), and as Pravega being the sole actor it modifies such data in Tier 2 and applies them without any constraints. It has several mechanisms to detect and recover from possible data loss or external actors modifying data concurrently.","title":"Storage Writer"},{"location":"segment-store-service/#integration-with-controller","text":"Methods for mapping Segment Containers to hosts and rules used for moving from one to another are beyond the scope of this document. Here, we just describe how the Segment Store Service interacts with the Controller and how it manages the lifecycle of Segment Containers based on external events.","title":"Integration with Controller"},{"location":"segment-store-service/#segment-container-manager","text":"Each instance of a Segment Store Service needs a Segment Container Manager . The role of this component is to manage the lifecycle of the Segment Containers that are assigned to that node (service). It performs the following duties: Connects to the Controller Service-Side Client (i.e., a client that deals only with Segment Container events, and not with the management of Streams and listens to all changes that pertain to Containers that pertain to its own instance. When it receives a notification that it needs to start a Segment Container for a particular Container Id, it initiates the process of bootstrapping such an object. It does not notify the requesting client of success until the operation completes without error. When it receives a notification that it needs to stop a Segment Container for a particular Container Id, it initiates the process of shutting it down. It does not notify the requesting client of success until the operation completes without error. If the Segment Container shuts down unexpectedly (whether during Start or during its normal operation), it will not attempt to restart it locally; instead it will notify the Controller.","title":"Segment Container Manager"},{"location":"segment-store-service/#storage-abstractions","text":"The Segment Store was not designed with particular implementations for Tier 1 or Tier 2. Instead, all these components have been abstracted out in simple, well-defined interfaces, which can be implemented against any standard file system (Tier 2) or append-only log system (Tier 1). Possible candidates for Tier 1 Storage: Apache BookKeeper (preferred, adapter is fully implemented as part of Pravega) Non-durable, non-replicated solutions: In-Memory (Single node deployment only - Pravega becomes a volatile buffer for Tier 2 Storage; data loss is unavoidable and unrecoverable from in the case of process crash or system restart). This is used for unit test only. Local File System (Single node deployment only - Pravega becomes a semi-durable buffer for Tier 2 Storage; data loss is unavoidable and unrecoverable from in the case of complete node failure) Possible candidates for Tier 2 Storage: HDFS (Implementation available) Extended S3 (Implementation available) NFS (general FileSystem ) (Implementation available) In-Memory (Single node deployment only - limited usefulness; data loss is unavoidable and unrecoverable from in the case of process crash or system restart) This is used for unit test only. A note about Tier 2 Truncation : The Segment Store supports Segment truncation at a particular offset, which means that, once that request is complete, no offset below that one will be available for reading. The above is a metadata update operation, however this also needs to be supported by Tier 2 so that the truncated data is physically deleted from it. If a Tier 2 implementation does not natively support truncation from the beginning of a file with offset preservation (i.e., a Segment of length 100 is truncated at offset 50, then offsets 0..49 are deleted, but offsets 50-99 are available and are not shifted down), then the Segment Store provides a wrapper on top of a generic Tier 2 implementation that can do that. The RollingStorage Tier 2 wrapper splits a Segment into multiple Segment Chunks and exposes them as a single Segment to the upper layers. Segment Chunks that have been truncated out, are deleted automatically. This is not a very precise application (since it relies heavily on a rollover policy dictating granularity), but it is a practical solution for those cases when the real Tier 2 implementation does not provide the features that we need.","title":"Storage Abstractions"},{"location":"segment-store-service/#data-flow","text":"Here are a few examples of how data flows inside the Pravega Segment Store Service.","title":"Data Flow"},{"location":"segment-store-service/#appends","text":"The diagram above depicts these steps (note the step numbers may not match, but the order is the same): Segment Store receives append request with params: Segment Name, Payload and Attribute Updates. Segment Store determines the Container ID for the given Segment and verifies that the Segment Container is registered locally. If not, it returns an appropriate error code. Segment Store delegates request to the appropriate Segment Container instance. Segment Container verifies that the Segment belongs to the Segment Container and that the Segment actually exists. If not, it returns an appropriate error code. During this process, it also gets an existing Segment ID or assigns a new one (by using the Segment Mapper component). Segment Container creates a StreamSegmentAppendOperation with the input data and sends it to the Durable Log . Durable Log takes the Append operation and processes it according to the algorithm described in the Durable Log section. Puts it in its operation Queue. Operation Processor pulls all operations off the Queue. Operation Processor uses the Data Frame Builder to construct Data Frames with the operations it has. Data Frame Builder asynchronously writes the Data Frame to the Durable Data Log . Upon completion, the following are done in parallel: Metadata is updated. The operation is added to the Memory Operation Log and Read Index . A call that triggered the operation is acked. The above process is asynchronous, which means the Operation Processor will have multiple Data Frames in flight (not illustrated). It will keep track of each one's changes and apply or roll them back as needed. This process applies for every single operation that the Segment Store supports. All modify operations go through the Operation Processor and have a similar path.","title":"Appends"},{"location":"segment-store-service/#reads","text":"The diagram above depicts these steps (note the step numbers may not match, but the order is the same): Segment Store receives read request with params: Segment Name, Read Offset, Max-Length. Segment Store determines the Container ID for the given Segment and verifies if it is Leader for given Segment Container . If not, it returns an appropriate error code. Segment Store delegates request to the Segment Container instance. Segment Container verifies that the Segment belongs to that Container and that it actually exists. If not, it returns an appropriate error code to the client. During this process, it also gets an existing Segment ID or assigns a new one (by using the Segment Mapper component). Segment Container delegates the request to its Read Index , which processes the read as described in the Read Index section, by issuing Reads from Storage (for data that is not in the Cache ), and querying/updating the Cache as needed.","title":"Reads"},{"location":"segment-store-service/#synchronization-with-tier-2-storage-writer","text":"The diagram above depicts these steps (note the step numbers may not match, but the order is the same): The Storage Writer 's main loop is the sub-component that triggers all these operations. Read next operation from the Durable Log (in between each loop, the Writer remembers what the sequence number of the last processed operation was). All operations are processed and added to the internal Segment Aggregators (one Aggregator per Segment). Eligible Segment Aggregators are flushed to Storage (eligibility depends on the amount of data collected in each aggregator, and whether there are any Seal, Merge or Truncate operations queued up). Each time an Append operation is encountered, a trip to the Read Index may be required in order to get the contents of the append. After every successful modification ( write/seal/concat/truncate ) to Storage , the Container Metadata is updated to reflect the changes. The Durable Log is truncated (if eligible).","title":"Synchronization with Tier 2 (Storage Writer)"},{"location":"segment-store-service/#container-startup-normalrecovery","text":"The diagram above depicts these steps (note the step numbers may not match, but the order is the same): The Container Manager receives a request to start a Container in this instance of the Segment Store Service . It creates, registers, and starts the Container. The Container starts the Durable Log component. Durable Log initiates the recovery process (coordinated by the Recovery Executor ). Recovery Executor reads all Data Frames from Durable Data Log . Deserialized operations from the read Data Frames are added to the Memory Operation Log . The Container Metadata is updated by means of the Operation Metadata Updater (same as the one used inside Operation Processor). The Read Index is populated with the contents of those operations that apply to it. The Container Starts the Storage Writer . The Storage Writer 's Main Loop starts processing operations from the Durable Log , and upon first encountering a new Segment, it reconciles its content (and metadata) with the reality that exists in Storage . After both the Durable Log and the Storage Writer have started, the Container is ready to start accepting new external requests.","title":"Container Startup (Normal/Recovery)"},{"location":"state-synchronizer-design/","text":"State Synchronizer Design \u00b6 In a State Synchronizer data can be written and read by multiple processes, and the consistency is guaranteed using optimistic checks. State Synchronizer provides the abstraction of a user defined Java Object which is kept in-sync consistently across the multiple machines. All the hosts would see the same object even as it is modified. The State Synchronizer API can be used to perform updates to the state Object . This can be used to implement replicated state machines, schema distribution, and leader election. State Synchronizer works by storing a consistent history of updates to the state Object . The updates are stored in a Pravega Stream. Pravega ensures that every process that is performing an update on the latest version of that Object . Thus the Object is coordinated across a fleet and everyone sees the same sequence of updates on the same Object . The idea is to use a Stream to persist a sequence of changes for a shared state. And allow various applications to use the Pravega Java Client Library to concurrently read and write the shared state in a consistent fashion. This works by having each process keep a copy of the data. All the updates are written through the State Synchronizer which appends them to the Pravega Stream Segment. Latest updates can be incorporated to the data by consuming from the Stream Segment. To provide consistency a conditional append is used. This ensures that the updates can only proceed if the process performing them has the most recent data. To avoid the unbounded data in the Stream Segment, a compact operation is involved which re-writes the latest data and truncates the old data. In Pravega Stream, a Segment is always owned by a single server. This allows it to provide atomic compare-and-set operation on the Segments. This primitive is used to build a higher level abstraction at the application layer while maintaining strong consistency. This model works well when most of the updates are small in comparison to the total data size being stored, as they can be written as small deltas. As with any optimistic concurrency system it would work worst when many processes contend and try to update the same information at the same time. Example \u00b6 A concrete example of synchronizing the contents of a Set is provided. We also have an example that is synchronizing membership of a set of hosts . Imagine you want many processes to share a Map. This can be done by creating the State Synchronizer, it will aid in coordinating the changes to the Map. Each client has its own copy of the Map in memory and can apply updates by passing a generator to the State Synchronizer. Every time an update is made, the update is recorded to the Stream Segment. Updates are successful when the Map passed into the update method is consistent with all of the updates that have been recorded to the Stream Segment. If this occurs the generator is called with the latest state to try again. Thus the order of updates is defined by the order in which they are written to the Stream Segment. Implementation \u00b6 For the implementation, two features of the Pravega Segment Store Service are used. Conditional Append \u00b6 The conditional append call in the Pravega Segment Store is the cornerstone for the implementation of the State Synchronizer semantics. That is, when a client updates a piece of data via State Synchronizer, a conditional append is internally used against the Segment Store. In a conditional append, the client specifies the Offset in which the append is expected to be located. If the Offset provided by the client does match the actual Offset of the append in the Stream Segment, the operation is aborted and an error is returned to the client. This mechanism is used in the State Synchronizer to provide optimistic locks on data updates. Truncate Segment \u00b6 Truncate Segment deletes all data before a given Offset . This operation does not affect the existing Offset s. Any reads for the Offset s lower than this value will fail. Any data stored below this Offset can be removed. Truncation is performed following compaction, so that the Segment does not need to hold onto old data.","title":"StateSynchronizer"},{"location":"state-synchronizer-design/#state-synchronizer-design","text":"In a State Synchronizer data can be written and read by multiple processes, and the consistency is guaranteed using optimistic checks. State Synchronizer provides the abstraction of a user defined Java Object which is kept in-sync consistently across the multiple machines. All the hosts would see the same object even as it is modified. The State Synchronizer API can be used to perform updates to the state Object . This can be used to implement replicated state machines, schema distribution, and leader election. State Synchronizer works by storing a consistent history of updates to the state Object . The updates are stored in a Pravega Stream. Pravega ensures that every process that is performing an update on the latest version of that Object . Thus the Object is coordinated across a fleet and everyone sees the same sequence of updates on the same Object . The idea is to use a Stream to persist a sequence of changes for a shared state. And allow various applications to use the Pravega Java Client Library to concurrently read and write the shared state in a consistent fashion. This works by having each process keep a copy of the data. All the updates are written through the State Synchronizer which appends them to the Pravega Stream Segment. Latest updates can be incorporated to the data by consuming from the Stream Segment. To provide consistency a conditional append is used. This ensures that the updates can only proceed if the process performing them has the most recent data. To avoid the unbounded data in the Stream Segment, a compact operation is involved which re-writes the latest data and truncates the old data. In Pravega Stream, a Segment is always owned by a single server. This allows it to provide atomic compare-and-set operation on the Segments. This primitive is used to build a higher level abstraction at the application layer while maintaining strong consistency. This model works well when most of the updates are small in comparison to the total data size being stored, as they can be written as small deltas. As with any optimistic concurrency system it would work worst when many processes contend and try to update the same information at the same time.","title":"State Synchronizer Design"},{"location":"state-synchronizer-design/#example","text":"A concrete example of synchronizing the contents of a Set is provided. We also have an example that is synchronizing membership of a set of hosts . Imagine you want many processes to share a Map. This can be done by creating the State Synchronizer, it will aid in coordinating the changes to the Map. Each client has its own copy of the Map in memory and can apply updates by passing a generator to the State Synchronizer. Every time an update is made, the update is recorded to the Stream Segment. Updates are successful when the Map passed into the update method is consistent with all of the updates that have been recorded to the Stream Segment. If this occurs the generator is called with the latest state to try again. Thus the order of updates is defined by the order in which they are written to the Stream Segment.","title":"Example"},{"location":"state-synchronizer-design/#implementation","text":"For the implementation, two features of the Pravega Segment Store Service are used.","title":"Implementation"},{"location":"state-synchronizer-design/#conditional-append","text":"The conditional append call in the Pravega Segment Store is the cornerstone for the implementation of the State Synchronizer semantics. That is, when a client updates a piece of data via State Synchronizer, a conditional append is internally used against the Segment Store. In a conditional append, the client specifies the Offset in which the append is expected to be located. If the Offset provided by the client does match the actual Offset of the append in the Stream Segment, the operation is aborted and an error is returned to the client. This mechanism is used in the State Synchronizer to provide optimistic locks on data updates.","title":"Conditional Append"},{"location":"state-synchronizer-design/#truncate-segment","text":"Truncate Segment deletes all data before a given Offset . This operation does not affect the existing Offset s. Any reads for the Offset s lower than this value will fail. Any data stored below this Offset can be removed. Truncation is performed following compaction, so that the Segment does not need to hold onto old data.","title":"Truncate Segment"},{"location":"state-synchronizer/","text":"Working with Pravega: State Synchronizer \u00b6 You can think about Pravega as a streaming storage primitive, because it is a great way to durably persist data. You can think about Pravega as a great pub-sub messaging system, because with Readers, Writers and ReaderGroups it is a great way to do messaging at scale. But you can also think about Pravega as a way to implement shared state in a consistent fashion across multiple cooperating processes distributed in a cluster. It is this latter category that we explore with this document. Instructions for running the sample applications can be found in the Pravega Samples readme . You really should be familiar with Pravega Concepts (see Pravega Concepts ) before continuing reading this page. In particular, you should be somewhat familiar with the State Synchronizer concept. Shared State and Pravega \u00b6 State Synchronizer is a facility provided by the Pravega programming model to make it easy for developers to use Pravega to coordinate shared state between processes. The idea is that a Stream is used to persist a sequence of changes to shared state and that various applications use their Pravega Java Client Library to concurrently read and write the shared state in a consistent fashion. SharedStateMap and Shared Configuration Example \u00b6 Before we dive into the details about how to use State Synchronizer, let's take a quick look at an example application that uses State Synchronizer. We have provided a simple yet illustrative example of using State Synchronizer here. The example uses State Synchronizer to build an implementation of Java's Map data structure called SharedMap. We use that primitive SharedMap data structure to build a Shared Config, that allows a set of processes to consistently read/write a shared, configuration object of key/value pair properties. Also as part of that example, we provide a simple command line-based application that allows you to play around with the SharedConfig app. Here is a menu of the available commands in the SharedConfigCLI application: Enter one of the following commands at the command line prompt: GET_ALL - prints out all of the properties in the Shared Config. GET {key} - print out the configuration property for the given key. PUT {key} , {value} - update the Shared Config with the given key/value pair. Print out previous value (if it existed). PUT_IF_ABSENT {key} , {value} - update the Shared Config with the given key/value pair, only if the property is not already defined. REMOVE {key} [ , {currentValue}] - remove the given property from the Shared Config. If {currentValue} is given, remove only if the property's current value matches {currentValue}.. REPLACE {key} , {newValue} [ , {currentValue}] - update the value of the property. If {currentValue} is given, update only if the property's current value matches {cuurentValue}. CLEAR - remove all the keys from the Shared Config. REFRESH - force an update from the Synchronized State. HELP - print out a list of commands. QUIT - terminate the program. Install the Pravega-Samples and launch two instances of the SharedConfigCLI using the same scope and stream name. This will simulate how two different processes can coordinate their local copy of the SharedConfig with one shared state object. You can follow these steps to get a feel for how the SharedConfig is coordinated: # Process 1 Process 2 Discussion 1 GET_ALL GET_ALL Shows that both processes see an empty SharedConfig 2 PUT p1,v1 Process 1 adds a property named p1 3 GET p1 GET p1 Process 1 sees value v1 for the property Process 2 does not have a property named p1. Why? Because it has not refreshed its state with the shared state 4 REFRESH Re-synchronize Process 2's state with the shared state 5 GET p1 Now Process 2 sees the change Process 1 made in step 2 6 REPLACE p1, newVal, v1 Process 2 attempts to change the value of p1, but uses a conditional replace, meaning the change should be made only if the old value of p1 is v1 (which it is at this point) 7 GET p1 Sure enough, the value of p1 was changed to newVal 8 REPLACE p1, anotherVal, v1 Process 1 tries to change the value of p1 in the same way Process 2 did in step 6. This will fail because the value of p1 in shared state is no longer v1 9 GET p1 The failed replace operation in step 8 caused Process 1's copy of the shared state to be updated, its value is now newVal because of step 6. You can repeat with a similar sequence to explore the semantics of PUT_IF_ABSENT and other operations that modify shared state. The idea is that modifications to the SharedConfig succeed only if they operate on the latest value. We use optimistic concurrency to implement efficient consistency across multiple consumers of the SharedConfig object. You can have multiple different SharedConfig state objects running simultaneously, each separate SharedConfig uses State Synchronizer objects based on a different Pravega Stream. Of course if you launch two applications using State Synchronizer objects backed by the same Stream, you get two processes concurrently accessing the shared state. This is exactly the situation we illustrated above. Using State Synchronizer to Build the SharedMap \u00b6 We used the State Synchronizer to build the SharedMap object in Pravega-Samples. State Synchronizer can be used to build a shared version of almost any data structure. Maybe your app needs to share just a simple integer count of something; we can use State Synchronizer to build a simple shared counter. Maybe the data you are sharing is a Set of currently running servers in a cluster; we can use State Synchronizer to build a shared Set. The possibilities are many. Let's explore how to build shared objects using State Synchronizer by examining how we built Shared Map. State Synchronizer \u00b6 State Synchronizer is a type of Pravega client, similar to an EventStreamReader or EventStreamWriter. A State Synchronizer is created via a ClientFactory object. Each State Synchronizer has a unique name within a Scope. A SynchronizerConfig object is used to tailor the behavior of a StateSynchronizer (although currently, there are no properties on a State Synchronizer that are configurable). State Synchronizer uses Java generic types to allow a developer to specify a type specific State Synchronizer. All of these things are done in a fashion similar to how EventStreamReaders and EventStreamWriters are used. StateT \u00b6 When designing an application that uses State Synchronizer, the developer needs to decide what type of state is going to be synchronized (shared). Are we sharing a Map? A Set? A Pojo? What is the data structure that is being shared. This defines the core \"type\" of the State Synchronizer (the StateT generic type in the State Synchronizer interface). The StateT object can be any Java object that implements the Revisioned interface defined by Pravega. Revisioned is a simple interface that allows Pravega to ensure it can properly compare two different StateT objects. In our example, the SharedMap is the State Synchronizer application. It defines a simple Map object presenting the typical get(key), set (key, value) etc. operations you would expect from a key-value pair map object. It implements the Revisioned interface, as required to use the State Synchronizer, and uses a simple ConcurrentHashMap as its internal implementation of the Map. So in our example, StateT corresponds to SharedStateMap\\<K,V>. UpdateT and InitialUpdateT \u00b6 In addition to StateT, there are two other generic types that need to be defined by a StateSynchronizer app: an Update type and an InitialUpdate type). The UpdateType represents the \"delta\" or change objects that are persisted on the Pravega Stream. The InitialUpdateType is a special update object used to to start the State Synchronizer off. Both UpdateType and InitialUpdateType are defined in terms of StateT. The StateSynchronizer uses a single Segment on a Stream to store updates (changes) to the shared state object. Changes, in the form of Initial or Update type objects, are written to the Stream based on whether the update is relative to the most current copy of the state in the Stream. If an update is presented that is based on an older version of the state, the update is not made. The StateSynchronizer object itself keeps a local in memory copy of the state, it also keeps version metadata about that copy of the state. Local state can be retrieved using the getState() operation. The local in memory copy could be stale, and it can be refreshed by an application using the fetchUpdates() operation, that retrieves all the changes made to the given version of the state. Most changes from the application are made through the updateState() operation. The updateState() operation takes a Function as parameter. The Function is invoked with the latest state object, and computes the updates to be applied. In our example, InitialUpdateT is implemented as: /** * Create a Map. This is used by StateSynchronizer to initialize shared state. */ private static class CreateState < K , V > implements InitialUpdate < SharedStateMap < K , V >>, Serializable { private static final long serialVersionUID = 1L ; private final ConcurrentHashMap < K , V > impl ; public CreateState ( ConcurrentHashMap < K , V > impl ) { this . impl = impl ; } @Override public SharedStateMap < K , V > create ( String scopedStreamName , Revision revision ) { return new SharedStateMap < K , V >( scopedStreamName , impl , revision ); } } In this case, the CreateState class is used to initialize the shared state in the Stream by creating a new, empty SharedStateMap object. You could imagine other examples of InitialUpdate that would set a counter to 1, or perhaps initialize a Set to a fixed initial set of members. It may seem a bit odd that functions like \"initialize\" and \"update\" are expressed as classes, but when you think about it, that makes sense. The changes, like initialize and update, need to be stored in Pravega, therefore they need to be serializable objects. It must be possible for client applications to be able to start at any time, compute the current state and then keep up as changes are written to the Stream. If we just stored \"the latest state value\" in the Stream, there would be no way to consistently provide concurrent update and read using optimistic concurrency. UpdateT is a bit more tricky. There isn't just one kind of update to a Map, but rather there are all sorts of updates: put of a key/value pair, put of a collection of key/value pairs, removing a key/value pair and clearing all of the key/value pairs, Each of these \"kinds\" of updates are represented by their own Class. We define an abstract class, called StateUpdate, from which all of these \"operational\" update classes inherit. StateUpdate abstract class /** * A base class for all updates to the shared state. This allows for several different types of updates. */ private static abstract class StateUpdate < K , V > implements Update < SharedStateMap < K , V >>, Serializable { private static final long serialVersionUID = 1L ; @Override public SharedStateMap < K , V > applyTo ( SharedStateMap < K , V > oldState , Revision newRevision ) { ConcurrentHashMap < K , V > newState = new ConcurrentHashMap < K , V >( oldState . impl ); process ( newState ); return new SharedStateMap < K , V >( oldState . getScopedStreamName (), newState , newRevision ); } public abstract void process ( ConcurrentHashMap < K , V > updatableList ); } By defining an abstract class, we can define UpdateT in terms of the abstract StateUpdate class. The abstract class implements the \"applyTo\" method that is invoked by the StateSynchronizer to apply the update to the current state object and return an updated state object. The actual work is done on a copy of the old state's underlying Map (impl) object, a \"process\" operation is applied (specific to each subclass) to the impl object and a new version of the SharedState, using the post-processed impl as the internal state. The abstract class defines a process() method that actually does the work of whatever update needs to be applied. This method is implemented by the various concrete classes that represent Put, PutAll etc. operations on the shared map. Here, for example, is the way we implement the Put(key,value) operation on the SharedMap object: Put as an Update Object /** * Add a key/value pair to the State. */ private static class Put < K , V > extends StateUpdate < K , V > { private static final long serialVersionUID = 1L ; private final K key ; private final V value ; public Put ( K key , V value ) { this . key = key ; this . value = value ; } @Override public void process ( ConcurrentHashMap < K , V > impl ) { impl . put ( key , value ); } } Here, the process() operation is to add a key/value pair to the map, or if the key already exists, change the value. Each of the \"operations\" on the SharedMap is implemented in terms of creating instances of the various subclasses of StateUpdate. Executing Operations on SharedMap \u00b6 SharedMap demonstrates the typical operations on a StateSynchronizer. SharedMap presents an API, very similar to Java's Map\\<K,V> interface. It implements the Map operations in terms of manipulating the StateSynchronizer, using the various subclasses of StateUpdate to perform state change (write) operations. Create/Initialize \u00b6 Creating a SharedMap /** * Creates the shared state using a synchronizer based on the given stream name. * * @param clientFactory - the Pravega ClientFactory to use to create the StateSynchronizer. * @param streamManager - the Pravega StreamManager to use to create the Scope and the Stream used by the StateSynchronizer * @param scope - the Scope to use to create the Stream used by the StateSynchronizer. * @param name - the name of the Stream to be used by the StateSynchronizer. */ public SharedMap ( ClientFactory clientFactory , StreamManager streamManager , String scope , String name ){ streamManager . createScope ( scope ); StreamConfiguration streamConfig = StreamConfiguration . builder (). scope ( scope ). streamName ( name ) . scalingPolicy ( ScalingPolicy . fixed ( 1 )) . build (); streamManager . createStream ( scope , name , streamConfig ); this . stateSynchronizer = clientFactory . createStateSynchronizer ( name , new JavaSerializer < StateUpdate < K , V >>(), new JavaSerializer < CreateState < K , V >>(), SynchronizerConfig . builder (). build ()); stateSynchronizer . initialize ( new CreateState < K , V >( new ConcurrentHashMap < K , V >())); } A SharedMap object is created by defining the scope and stream (almost always the case, the scope and stream probably already exist, so the steps in lines 10-16 are usually no-ops. The StateSynchronizer object itself is constructed in lines 18-21 using the ClientFactory in a fashion similar to the way a Pravega Reader or Writer would be created. Note that the UpdateT object and InitialUpdateT object can have separate Java serializers specified. Currently, the SynchronizerConfig object is pretty dull; there are no configuration items currently available on the StateSynchronizer. The StateSynchronizer provides an initialize() API that takes an InitialUpdate object. This is called in the SharedMap constructor to make sure the SharedState is properly initialized. Note, in many cases, the SharedMap object will be created on a stream that already contains shared state for the SharedMap. Even in this case, it is ok to call initialize() because initialize() won't modify the shared state in the Stream. Read Operations \u00b6 The read operations, operations that do not alter shared state, like get(key) containsValue(value) etc., work against the local copy of the StateSynchronizer. All of these operations retrieve the current local state using getState() and then do the read operation from that state. The local state of the StateSynchronizer might be stale. In these cases, the SharedMap client would use refresh() to force the StateSynchronizer to refresh its state from shared state using the fetchUpdates() operation on the StateSynchronizer object. Note, this is a design decision to trade off staleness for responsiveness. We could easily have implemented the read operations to instead always do a refresh before doing the read against local state. That would be a very efficient strategy if the developer expected that there will be frequent updates to the shared state. In our case, we had imagined that the SharedMap would be read frequently but updated relatively infrequently, and therefore chose to read against local state. Write (update) Operations \u00b6 Each write operation is implemented in terms of the various concrete StateUpdate objects we discussed earlier. The clear() operation uses the Clear subclass of StateUpdate to remove all the key/value pairs, put() uses the Put class, etc. Lets dive into the implementation of the put() operation to discuss StateSynchronizer programming in a bit more detail: Implementing put(key,value) /** * Associates the specified value with the specified key in this map. * * @param key - the key at which the value should be found. * @param value - the value to be entered into the map. * @return - the previous value (if it existed) for the given key or null if the key did not exist before this operation. */ public V put ( K key , V value ){ final AtomicReference < V > oldValue = new AtomicReference < V >( null ); stateSynchronizer . updateState (( state , updates ) -> { oldValue . set ( state . get ( key )); updates . add ( new Put < K , V >( key , value )); }); return oldValue . get (); } It is important to note that the function provided to the StateSynchronizer's updateState() will be called potentially multiple times. The result of applying the function to the old state is written only when it is applied against the most current revision of the state. If there was a race and the optimistic concurrency check fails, it will be called again. Most of the time there will only be a small number of invocations. In some cases, the developer may choose to use fetchUpdates() to synchronize the StateSynchronizer with the latest copy of shared state from the stream before running updateState(). This is a matter of optimizing the tradeoff between how frequent updates are expected and how efficient you want the update to be. If you expect a lot of updates, call fetchUpdates() before calling updateState(). In our case, we didn't expect a lot of updates and therefore we process potentially several invocations of the function each time put() is called. Delete Operations \u00b6 We chose to implement the delete (remove) operations to also leverage the compact() feature of StateSynchronizer. We have a policy that after every 5 remove operations, and after every clear() operation, we do a compact operation. Now, we could have chosen to do a compact() operation after every 5 update operations, but we wanted to isolate the illustration of using compact() to just delete operations. You can think of compact() as a form of \"garbage collection\" in StateSynchronizer. After a certain number of changes have been written to SharedState, it might be efficient to write out a new initial state, an accumulated representation of all the changes, to the Stream. That way data older than the compact operation can be ignored and eventually removed from the Stream. As a result of the compact() operation, a new initial sate (Initial2) is written to the stream. Now, all the data from Change3 and older is no longer relevant and can be garbage collected out of the Stream.","title":"Working with State Synchronizer"},{"location":"state-synchronizer/#working-with-pravega-state-synchronizer","text":"You can think about Pravega as a streaming storage primitive, because it is a great way to durably persist data. You can think about Pravega as a great pub-sub messaging system, because with Readers, Writers and ReaderGroups it is a great way to do messaging at scale. But you can also think about Pravega as a way to implement shared state in a consistent fashion across multiple cooperating processes distributed in a cluster. It is this latter category that we explore with this document. Instructions for running the sample applications can be found in the Pravega Samples readme . You really should be familiar with Pravega Concepts (see Pravega Concepts ) before continuing reading this page. In particular, you should be somewhat familiar with the State Synchronizer concept.","title":"Working with Pravega: State Synchronizer"},{"location":"state-synchronizer/#shared-state-and-pravega","text":"State Synchronizer is a facility provided by the Pravega programming model to make it easy for developers to use Pravega to coordinate shared state between processes. The idea is that a Stream is used to persist a sequence of changes to shared state and that various applications use their Pravega Java Client Library to concurrently read and write the shared state in a consistent fashion.","title":"Shared State and Pravega"},{"location":"state-synchronizer/#sharedstatemap-and-shared-configuration-example","text":"Before we dive into the details about how to use State Synchronizer, let's take a quick look at an example application that uses State Synchronizer. We have provided a simple yet illustrative example of using State Synchronizer here. The example uses State Synchronizer to build an implementation of Java's Map data structure called SharedMap. We use that primitive SharedMap data structure to build a Shared Config, that allows a set of processes to consistently read/write a shared, configuration object of key/value pair properties. Also as part of that example, we provide a simple command line-based application that allows you to play around with the SharedConfig app. Here is a menu of the available commands in the SharedConfigCLI application: Enter one of the following commands at the command line prompt: GET_ALL - prints out all of the properties in the Shared Config. GET {key} - print out the configuration property for the given key. PUT {key} , {value} - update the Shared Config with the given key/value pair. Print out previous value (if it existed). PUT_IF_ABSENT {key} , {value} - update the Shared Config with the given key/value pair, only if the property is not already defined. REMOVE {key} [ , {currentValue}] - remove the given property from the Shared Config. If {currentValue} is given, remove only if the property's current value matches {currentValue}.. REPLACE {key} , {newValue} [ , {currentValue}] - update the value of the property. If {currentValue} is given, update only if the property's current value matches {cuurentValue}. CLEAR - remove all the keys from the Shared Config. REFRESH - force an update from the Synchronized State. HELP - print out a list of commands. QUIT - terminate the program. Install the Pravega-Samples and launch two instances of the SharedConfigCLI using the same scope and stream name. This will simulate how two different processes can coordinate their local copy of the SharedConfig with one shared state object. You can follow these steps to get a feel for how the SharedConfig is coordinated: # Process 1 Process 2 Discussion 1 GET_ALL GET_ALL Shows that both processes see an empty SharedConfig 2 PUT p1,v1 Process 1 adds a property named p1 3 GET p1 GET p1 Process 1 sees value v1 for the property Process 2 does not have a property named p1. Why? Because it has not refreshed its state with the shared state 4 REFRESH Re-synchronize Process 2's state with the shared state 5 GET p1 Now Process 2 sees the change Process 1 made in step 2 6 REPLACE p1, newVal, v1 Process 2 attempts to change the value of p1, but uses a conditional replace, meaning the change should be made only if the old value of p1 is v1 (which it is at this point) 7 GET p1 Sure enough, the value of p1 was changed to newVal 8 REPLACE p1, anotherVal, v1 Process 1 tries to change the value of p1 in the same way Process 2 did in step 6. This will fail because the value of p1 in shared state is no longer v1 9 GET p1 The failed replace operation in step 8 caused Process 1's copy of the shared state to be updated, its value is now newVal because of step 6. You can repeat with a similar sequence to explore the semantics of PUT_IF_ABSENT and other operations that modify shared state. The idea is that modifications to the SharedConfig succeed only if they operate on the latest value. We use optimistic concurrency to implement efficient consistency across multiple consumers of the SharedConfig object. You can have multiple different SharedConfig state objects running simultaneously, each separate SharedConfig uses State Synchronizer objects based on a different Pravega Stream. Of course if you launch two applications using State Synchronizer objects backed by the same Stream, you get two processes concurrently accessing the shared state. This is exactly the situation we illustrated above.","title":"SharedStateMap and Shared Configuration Example"},{"location":"state-synchronizer/#using-state-synchronizer-to-build-the-sharedmap","text":"We used the State Synchronizer to build the SharedMap object in Pravega-Samples. State Synchronizer can be used to build a shared version of almost any data structure. Maybe your app needs to share just a simple integer count of something; we can use State Synchronizer to build a simple shared counter. Maybe the data you are sharing is a Set of currently running servers in a cluster; we can use State Synchronizer to build a shared Set. The possibilities are many. Let's explore how to build shared objects using State Synchronizer by examining how we built Shared Map.","title":"Using State Synchronizer to Build the SharedMap"},{"location":"state-synchronizer/#state-synchronizer","text":"State Synchronizer is a type of Pravega client, similar to an EventStreamReader or EventStreamWriter. A State Synchronizer is created via a ClientFactory object. Each State Synchronizer has a unique name within a Scope. A SynchronizerConfig object is used to tailor the behavior of a StateSynchronizer (although currently, there are no properties on a State Synchronizer that are configurable). State Synchronizer uses Java generic types to allow a developer to specify a type specific State Synchronizer. All of these things are done in a fashion similar to how EventStreamReaders and EventStreamWriters are used.","title":"State Synchronizer"},{"location":"state-synchronizer/#statet","text":"When designing an application that uses State Synchronizer, the developer needs to decide what type of state is going to be synchronized (shared). Are we sharing a Map? A Set? A Pojo? What is the data structure that is being shared. This defines the core \"type\" of the State Synchronizer (the StateT generic type in the State Synchronizer interface). The StateT object can be any Java object that implements the Revisioned interface defined by Pravega. Revisioned is a simple interface that allows Pravega to ensure it can properly compare two different StateT objects. In our example, the SharedMap is the State Synchronizer application. It defines a simple Map object presenting the typical get(key), set (key, value) etc. operations you would expect from a key-value pair map object. It implements the Revisioned interface, as required to use the State Synchronizer, and uses a simple ConcurrentHashMap as its internal implementation of the Map. So in our example, StateT corresponds to SharedStateMap\\<K,V>.","title":"StateT"},{"location":"state-synchronizer/#updatet-and-initialupdatet","text":"In addition to StateT, there are two other generic types that need to be defined by a StateSynchronizer app: an Update type and an InitialUpdate type). The UpdateType represents the \"delta\" or change objects that are persisted on the Pravega Stream. The InitialUpdateType is a special update object used to to start the State Synchronizer off. Both UpdateType and InitialUpdateType are defined in terms of StateT. The StateSynchronizer uses a single Segment on a Stream to store updates (changes) to the shared state object. Changes, in the form of Initial or Update type objects, are written to the Stream based on whether the update is relative to the most current copy of the state in the Stream. If an update is presented that is based on an older version of the state, the update is not made. The StateSynchronizer object itself keeps a local in memory copy of the state, it also keeps version metadata about that copy of the state. Local state can be retrieved using the getState() operation. The local in memory copy could be stale, and it can be refreshed by an application using the fetchUpdates() operation, that retrieves all the changes made to the given version of the state. Most changes from the application are made through the updateState() operation. The updateState() operation takes a Function as parameter. The Function is invoked with the latest state object, and computes the updates to be applied. In our example, InitialUpdateT is implemented as: /** * Create a Map. This is used by StateSynchronizer to initialize shared state. */ private static class CreateState < K , V > implements InitialUpdate < SharedStateMap < K , V >>, Serializable { private static final long serialVersionUID = 1L ; private final ConcurrentHashMap < K , V > impl ; public CreateState ( ConcurrentHashMap < K , V > impl ) { this . impl = impl ; } @Override public SharedStateMap < K , V > create ( String scopedStreamName , Revision revision ) { return new SharedStateMap < K , V >( scopedStreamName , impl , revision ); } } In this case, the CreateState class is used to initialize the shared state in the Stream by creating a new, empty SharedStateMap object. You could imagine other examples of InitialUpdate that would set a counter to 1, or perhaps initialize a Set to a fixed initial set of members. It may seem a bit odd that functions like \"initialize\" and \"update\" are expressed as classes, but when you think about it, that makes sense. The changes, like initialize and update, need to be stored in Pravega, therefore they need to be serializable objects. It must be possible for client applications to be able to start at any time, compute the current state and then keep up as changes are written to the Stream. If we just stored \"the latest state value\" in the Stream, there would be no way to consistently provide concurrent update and read using optimistic concurrency. UpdateT is a bit more tricky. There isn't just one kind of update to a Map, but rather there are all sorts of updates: put of a key/value pair, put of a collection of key/value pairs, removing a key/value pair and clearing all of the key/value pairs, Each of these \"kinds\" of updates are represented by their own Class. We define an abstract class, called StateUpdate, from which all of these \"operational\" update classes inherit. StateUpdate abstract class /** * A base class for all updates to the shared state. This allows for several different types of updates. */ private static abstract class StateUpdate < K , V > implements Update < SharedStateMap < K , V >>, Serializable { private static final long serialVersionUID = 1L ; @Override public SharedStateMap < K , V > applyTo ( SharedStateMap < K , V > oldState , Revision newRevision ) { ConcurrentHashMap < K , V > newState = new ConcurrentHashMap < K , V >( oldState . impl ); process ( newState ); return new SharedStateMap < K , V >( oldState . getScopedStreamName (), newState , newRevision ); } public abstract void process ( ConcurrentHashMap < K , V > updatableList ); } By defining an abstract class, we can define UpdateT in terms of the abstract StateUpdate class. The abstract class implements the \"applyTo\" method that is invoked by the StateSynchronizer to apply the update to the current state object and return an updated state object. The actual work is done on a copy of the old state's underlying Map (impl) object, a \"process\" operation is applied (specific to each subclass) to the impl object and a new version of the SharedState, using the post-processed impl as the internal state. The abstract class defines a process() method that actually does the work of whatever update needs to be applied. This method is implemented by the various concrete classes that represent Put, PutAll etc. operations on the shared map. Here, for example, is the way we implement the Put(key,value) operation on the SharedMap object: Put as an Update Object /** * Add a key/value pair to the State. */ private static class Put < K , V > extends StateUpdate < K , V > { private static final long serialVersionUID = 1L ; private final K key ; private final V value ; public Put ( K key , V value ) { this . key = key ; this . value = value ; } @Override public void process ( ConcurrentHashMap < K , V > impl ) { impl . put ( key , value ); } } Here, the process() operation is to add a key/value pair to the map, or if the key already exists, change the value. Each of the \"operations\" on the SharedMap is implemented in terms of creating instances of the various subclasses of StateUpdate.","title":"UpdateT and InitialUpdateT"},{"location":"state-synchronizer/#executing-operations-on-sharedmap","text":"SharedMap demonstrates the typical operations on a StateSynchronizer. SharedMap presents an API, very similar to Java's Map\\<K,V> interface. It implements the Map operations in terms of manipulating the StateSynchronizer, using the various subclasses of StateUpdate to perform state change (write) operations.","title":"Executing Operations on SharedMap"},{"location":"state-synchronizer/#createinitialize","text":"Creating a SharedMap /** * Creates the shared state using a synchronizer based on the given stream name. * * @param clientFactory - the Pravega ClientFactory to use to create the StateSynchronizer. * @param streamManager - the Pravega StreamManager to use to create the Scope and the Stream used by the StateSynchronizer * @param scope - the Scope to use to create the Stream used by the StateSynchronizer. * @param name - the name of the Stream to be used by the StateSynchronizer. */ public SharedMap ( ClientFactory clientFactory , StreamManager streamManager , String scope , String name ){ streamManager . createScope ( scope ); StreamConfiguration streamConfig = StreamConfiguration . builder (). scope ( scope ). streamName ( name ) . scalingPolicy ( ScalingPolicy . fixed ( 1 )) . build (); streamManager . createStream ( scope , name , streamConfig ); this . stateSynchronizer = clientFactory . createStateSynchronizer ( name , new JavaSerializer < StateUpdate < K , V >>(), new JavaSerializer < CreateState < K , V >>(), SynchronizerConfig . builder (). build ()); stateSynchronizer . initialize ( new CreateState < K , V >( new ConcurrentHashMap < K , V >())); } A SharedMap object is created by defining the scope and stream (almost always the case, the scope and stream probably already exist, so the steps in lines 10-16 are usually no-ops. The StateSynchronizer object itself is constructed in lines 18-21 using the ClientFactory in a fashion similar to the way a Pravega Reader or Writer would be created. Note that the UpdateT object and InitialUpdateT object can have separate Java serializers specified. Currently, the SynchronizerConfig object is pretty dull; there are no configuration items currently available on the StateSynchronizer. The StateSynchronizer provides an initialize() API that takes an InitialUpdate object. This is called in the SharedMap constructor to make sure the SharedState is properly initialized. Note, in many cases, the SharedMap object will be created on a stream that already contains shared state for the SharedMap. Even in this case, it is ok to call initialize() because initialize() won't modify the shared state in the Stream.","title":"Create/Initialize"},{"location":"state-synchronizer/#read-operations","text":"The read operations, operations that do not alter shared state, like get(key) containsValue(value) etc., work against the local copy of the StateSynchronizer. All of these operations retrieve the current local state using getState() and then do the read operation from that state. The local state of the StateSynchronizer might be stale. In these cases, the SharedMap client would use refresh() to force the StateSynchronizer to refresh its state from shared state using the fetchUpdates() operation on the StateSynchronizer object. Note, this is a design decision to trade off staleness for responsiveness. We could easily have implemented the read operations to instead always do a refresh before doing the read against local state. That would be a very efficient strategy if the developer expected that there will be frequent updates to the shared state. In our case, we had imagined that the SharedMap would be read frequently but updated relatively infrequently, and therefore chose to read against local state.","title":"Read Operations"},{"location":"state-synchronizer/#write-update-operations","text":"Each write operation is implemented in terms of the various concrete StateUpdate objects we discussed earlier. The clear() operation uses the Clear subclass of StateUpdate to remove all the key/value pairs, put() uses the Put class, etc. Lets dive into the implementation of the put() operation to discuss StateSynchronizer programming in a bit more detail: Implementing put(key,value) /** * Associates the specified value with the specified key in this map. * * @param key - the key at which the value should be found. * @param value - the value to be entered into the map. * @return - the previous value (if it existed) for the given key or null if the key did not exist before this operation. */ public V put ( K key , V value ){ final AtomicReference < V > oldValue = new AtomicReference < V >( null ); stateSynchronizer . updateState (( state , updates ) -> { oldValue . set ( state . get ( key )); updates . add ( new Put < K , V >( key , value )); }); return oldValue . get (); } It is important to note that the function provided to the StateSynchronizer's updateState() will be called potentially multiple times. The result of applying the function to the old state is written only when it is applied against the most current revision of the state. If there was a race and the optimistic concurrency check fails, it will be called again. Most of the time there will only be a small number of invocations. In some cases, the developer may choose to use fetchUpdates() to synchronize the StateSynchronizer with the latest copy of shared state from the stream before running updateState(). This is a matter of optimizing the tradeoff between how frequent updates are expected and how efficient you want the update to be. If you expect a lot of updates, call fetchUpdates() before calling updateState(). In our case, we didn't expect a lot of updates and therefore we process potentially several invocations of the function each time put() is called.","title":"Write (update) Operations"},{"location":"state-synchronizer/#delete-operations","text":"We chose to implement the delete (remove) operations to also leverage the compact() feature of StateSynchronizer. We have a policy that after every 5 remove operations, and after every clear() operation, we do a compact operation. Now, we could have chosen to do a compact() operation after every 5 update operations, but we wanted to isolate the illustration of using compact() to just delete operations. You can think of compact() as a form of \"garbage collection\" in StateSynchronizer. After a certain number of changes have been written to SharedState, it might be efficient to write out a new initial state, an accumulated representation of all the changes, to the Stream. That way data older than the compact operation can be ignored and eventually removed from the Stream. As a result of the compact() operation, a new initial sate (Initial2) is written to the stream. Now, all the data from Change3 and older is no longer relevant and can be garbage collected out of the Stream.","title":"Delete Operations"},{"location":"streamcuts/","text":"Working with Pravega: StreamCuts \u00b6 This section describes StreamCut s and its usage with streaming clients and batch clients. Pre-requisites \u00b6 Familiarity with Pravega Concepts . Definition \u00b6 A Pravega Stream is formed by one or multiple parallel Stream Segments for storing/reading events. A Pravega Stream is elastic, as it handles the changes in the number of parallel Stream Segments along time to accommodate fluctuating workloads. A StreamCut represents a consistent position in the stream. It contains a set of Stream Segments and offset pairs for a single stream which represents the complete keyspace at a given point in time. The offset always points to the event boundary and hence there will be no offset pointing to an incomplete event. The StreamCut representing the tail of the stream (with the newest event) is an ever changing one since events can be continuously added to the stream and the StreamCut pointing to the tail of the stream with newer events would have a different value. Similarly the StreamCut representing the head of the stream (with the oldest event) is an ever changing one as the stream retention policy could truncate the stream and the StreamCut pointing to the head of the stream post truncation would have a different value. StreamCut.UNBOUNDED is used to represent such a position in the stream and the user can use it to specify this ever changing stream position (both head and tail of the stream). It should be noted that StreamCut s obtained using the streaming client and batch client can be used interchangeably. StreamCut with Reader \u00b6 A Reader Group is a named collection of Readers that together, in parallel, read Events from a given stream. Every Reader is always associated with a Reader Group. StreamCut (s) can be obtained from a Reader Group using the following APIs: - getStreamCuts() : The API io.pravega.client.stream.ReaderGroup.getStreamCuts returns a Map<Stream, StreamCut> which represents the last known Position of the Readers for all the streams managed by the Reader Group. generateStreamCuts() : The API io.pravega.client.stream.ReaderGroup.generateStreamCuts , generates a StreamCut after co-ordinating with all the Readers using io.pravega.client.state.StateSynchronizer . A StreamCut is generated by using the latest Stream Segment read offsets returned by the Readers along with unassigned segments (if any). The configuration ReaderGroupConfig.getGroupRefreshTimeMillis() decides the maximum delay by which the Readers return the latest read offsets of their assigned segments. The StreamCut generated by this API can be used by the application as a reference to a Position in the stream. This is guaranteed to be greater than or equal to the position of the Readers at the point of invocation of the API. A StreamCut can be used to configure a Reader Group to enable bounded processing of a Stream. The start and/or end StreamCut of a Stream can be passed as part of the Reader Group configuration. The below example shows the different ways to use StreamCut s as part of the Reader Group configuration. /* * The below ReaderGroup configuration ensures that the readers belonging to * the ReaderGroup read events from * - Stream \"s1\" from startStreamCut1 (representing the oldest event) upto endStreamCut1 (representing the newest event) * - Stream \"s2\" from startStreamCut2 upto the tail of the stream, this is similar to using StreamCut.UNBOUNDED * for endStreamCut. * - Stream \"s3\" from the current head of the stream upto endStreamCut2 * - Stream \"s4\" from the current head of the stream upto the tail of the stream. */ ReaderGroupConfig . builder () . stream ( \"scope/s1\" , startStreamCut1 , endStreamCut1 ) . stream ( \"scope/s2\" , startStreamCut2 ) . stream ( \"scope/s3\" , StreamCut . UNBOUNDED , endStreamCut2 ) . stream ( \"scope/s4\" ) . build (); The below API can be used to reset an existing Reader Group with a new Reader Group configuration instead creating a Reader Group. /* * ReaderGroup API used to reset a ReaderGroup to a newer ReaderGroup configuration. */ io . pravega . client . stream . ReaderGroup . resetReaderGroup ( ReaderGroupConfig config ) StreamCut with Stream Manager \u00b6 StreamCut representing the current head and current tail of a stream can be obtained using the StreamManager API getStreamInfo(String scopeName, String streamName) . /** * Get information about a given Stream, {@link StreamInfo}. * This includes {@link StreamCut}s pointing to the current HEAD and TAIL of the Stream. * * @param scopeName The scope of the stream. * @param streamName The stream name. * @return stream information. */ StreamInfo getStreamInfo ( String scopeName , String streamName ); StreamCut with BatchClient \u00b6 BatchClient can be used to perform bounded processing of the stream given the start and end StreamCut s. BatchClient API io.pravega.client.batch.BatchClient.getSegments(stream, startStreamCut, endStreamCut) is used to fetch segments which reside between the given startStreamCut and endStreamCut . With the retrieved segment information, the user can consume all the events in parallel without adhering to time ordering of events. It must be noted that passing StreamCut.UNBOUNDED to startStreamCut and endStreamCut will result in using the current head of stream and the current tail of the stream, respectively. We have provided a simple yet illustrative example of using StreamCut here .","title":"Working with StreamCuts"},{"location":"streamcuts/#working-with-pravega-streamcuts","text":"This section describes StreamCut s and its usage with streaming clients and batch clients.","title":"Working with Pravega: StreamCuts"},{"location":"streamcuts/#pre-requisites","text":"Familiarity with Pravega Concepts .","title":"Pre-requisites"},{"location":"streamcuts/#definition","text":"A Pravega Stream is formed by one or multiple parallel Stream Segments for storing/reading events. A Pravega Stream is elastic, as it handles the changes in the number of parallel Stream Segments along time to accommodate fluctuating workloads. A StreamCut represents a consistent position in the stream. It contains a set of Stream Segments and offset pairs for a single stream which represents the complete keyspace at a given point in time. The offset always points to the event boundary and hence there will be no offset pointing to an incomplete event. The StreamCut representing the tail of the stream (with the newest event) is an ever changing one since events can be continuously added to the stream and the StreamCut pointing to the tail of the stream with newer events would have a different value. Similarly the StreamCut representing the head of the stream (with the oldest event) is an ever changing one as the stream retention policy could truncate the stream and the StreamCut pointing to the head of the stream post truncation would have a different value. StreamCut.UNBOUNDED is used to represent such a position in the stream and the user can use it to specify this ever changing stream position (both head and tail of the stream). It should be noted that StreamCut s obtained using the streaming client and batch client can be used interchangeably.","title":"Definition"},{"location":"streamcuts/#streamcut-with-reader","text":"A Reader Group is a named collection of Readers that together, in parallel, read Events from a given stream. Every Reader is always associated with a Reader Group. StreamCut (s) can be obtained from a Reader Group using the following APIs: - getStreamCuts() : The API io.pravega.client.stream.ReaderGroup.getStreamCuts returns a Map<Stream, StreamCut> which represents the last known Position of the Readers for all the streams managed by the Reader Group. generateStreamCuts() : The API io.pravega.client.stream.ReaderGroup.generateStreamCuts , generates a StreamCut after co-ordinating with all the Readers using io.pravega.client.state.StateSynchronizer . A StreamCut is generated by using the latest Stream Segment read offsets returned by the Readers along with unassigned segments (if any). The configuration ReaderGroupConfig.getGroupRefreshTimeMillis() decides the maximum delay by which the Readers return the latest read offsets of their assigned segments. The StreamCut generated by this API can be used by the application as a reference to a Position in the stream. This is guaranteed to be greater than or equal to the position of the Readers at the point of invocation of the API. A StreamCut can be used to configure a Reader Group to enable bounded processing of a Stream. The start and/or end StreamCut of a Stream can be passed as part of the Reader Group configuration. The below example shows the different ways to use StreamCut s as part of the Reader Group configuration. /* * The below ReaderGroup configuration ensures that the readers belonging to * the ReaderGroup read events from * - Stream \"s1\" from startStreamCut1 (representing the oldest event) upto endStreamCut1 (representing the newest event) * - Stream \"s2\" from startStreamCut2 upto the tail of the stream, this is similar to using StreamCut.UNBOUNDED * for endStreamCut. * - Stream \"s3\" from the current head of the stream upto endStreamCut2 * - Stream \"s4\" from the current head of the stream upto the tail of the stream. */ ReaderGroupConfig . builder () . stream ( \"scope/s1\" , startStreamCut1 , endStreamCut1 ) . stream ( \"scope/s2\" , startStreamCut2 ) . stream ( \"scope/s3\" , StreamCut . UNBOUNDED , endStreamCut2 ) . stream ( \"scope/s4\" ) . build (); The below API can be used to reset an existing Reader Group with a new Reader Group configuration instead creating a Reader Group. /* * ReaderGroup API used to reset a ReaderGroup to a newer ReaderGroup configuration. */ io . pravega . client . stream . ReaderGroup . resetReaderGroup ( ReaderGroupConfig config )","title":"StreamCut with Reader"},{"location":"streamcuts/#streamcut-with-stream-manager","text":"StreamCut representing the current head and current tail of a stream can be obtained using the StreamManager API getStreamInfo(String scopeName, String streamName) . /** * Get information about a given Stream, {@link StreamInfo}. * This includes {@link StreamCut}s pointing to the current HEAD and TAIL of the Stream. * * @param scopeName The scope of the stream. * @param streamName The stream name. * @return stream information. */ StreamInfo getStreamInfo ( String scopeName , String streamName );","title":"StreamCut with Stream Manager"},{"location":"streamcuts/#streamcut-with-batchclient","text":"BatchClient can be used to perform bounded processing of the stream given the start and end StreamCut s. BatchClient API io.pravega.client.batch.BatchClient.getSegments(stream, startStreamCut, endStreamCut) is used to fetch segments which reside between the given startStreamCut and endStreamCut . With the retrieved segment information, the user can consume all the events in parallel without adhering to time ordering of events. It must be noted that passing StreamCut.UNBOUNDED to startStreamCut and endStreamCut will result in using the current head of stream and the current tail of the stream, respectively. We have provided a simple yet illustrative example of using StreamCut here .","title":"StreamCut with BatchClient"},{"location":"terminology/","text":"Terminology \u00b6 The glossary of terms related to Pravega is given below: Term Definition Pravega Pravega is an open source storage system that exposes stream as the main primitive for continuous and unbounded data. Stream A durable, elastic, append-only, unbounded sequence of bytes that has good performance and strong consistency. A Stream is identified by a Stream name and a Scope . A Stream is comprised of one or more Stream Segments. Stream Segment A shard of a Stream . The number of Stream Segments in a Stream might vary over time according to load and Scaling Policy . In the absence of a Scale Event , Events written to a Stream with the same Routing Key are stored in the same Stream Segment and are ordered. When a Scale Event occurs, the set of Stream Segments of a Stream changes and Events written with a given Routing Key K before the Scaling Event are stored in a different Stream Segment compared to Events written with the same Routing Key K after the event. In conjunction with Reader Groups , the number of Stream Segments is the maximum amount of read parallelism of a Stream . Scope A namespace for Stream names. A Stream name must be unique within a Scope . Event A collection of bytes within a Stream. An Event is associated with a Routing Key. Routing Key A property of an Event used to route messages to Readers. Two Events with the same Routing Key will be read by Readers in exactly the same order they were written. Reader A software application that reads data from one or more Streams . Writer A software application that writes data to one or more Streams. Pravega Java Client Library A Java library used by applications to interface with Pravega Reader Group A named collection of one or more Readers that read from a Stream in parallel. Pravega assigns Stream Segments to the Readers ensuring that all Stream Segments are assigned to at least one Reader and that they are balanced across the Readers . Position An offset within a Stream , representing a type of recovery point for a Reader . If a Reader crashes, a Position can be used to initialize the failed Reader 's replacement so that the replaced Reader resumes processing the Stream from where the failed Reader left off. Tier 1 Storage Short term, low-latency, data storage that guarantees the durability of data written to Streams . The current implementation of Tier 1 uses Apache Bookkeeper . Tier 1 storage keeps the most recent appends to streams in Pravega. As data in Tier 1 ages, it is moved out of Tier 1 into Tier 2. Tier 2 Storage A portion of Pravega storage based on cheap and deep persistent storage technology such as HDFS , DellEMC's Isilon or DellEMC's Elastic Cloud Storage . Pravega Server A component of Pravega that implements the Pravega data plane API for operations such as reading from and writing to Streams . The data plane of Pravega, also called the Segment Store, is composed of one or more Pravega Server instances. Segment Store A collection of Pravega Servers that in aggregate form the data plane of a Pravega cluster. Controller A component of Pravega that implements the Pravega control plane API for operations such as creating and retrieving information about Streams . The control plane of Pravega is composed of one or more Controller instances coordinated by Zookeeper . Auto Scaling A Pravega concept that allows the number of Stream Segments in a Stream to change over time, based on Scaling Policy. Scaling Policy A configuration item of a Stream that determines how the number of Stream Segments in the Stream should change over time. There are three kinds of Scaling Policy , a Stream has exactly one of the following at any given time. - Fixed number of Stream Segments - Change the number of Stream Segments based on the number of bytes per second written to the Stream (Size- based) - Change the number of Stream Segments based on the number of Events per second written to the Stream (Event-based) Scale Event There are two types of Scale Event : Scale-Up Event and Scale-Down Event. A Scale Event triggers Auto Scaling . A Scale-Up Event occurs when there is an increase in load, the number of Stream Segments are increased by splitting one or more Stream Segments in the Stream . A Scale-Down Event occurs when there is a decrease in load, the number of Stream Segments are reduced by merging one or more Stream Segments in the Stream . Transaction A collection of Stream write operations that are applied atomically to the Stream . Either all of the bytes in a Transaction are written to the Stream or none of them are. State Synchronizer An abstraction built on top of Pravega to enable the implementation of replicated state using a Pravega segment to back up the state transformations. A State Synchronizer allows a piece of data to be shared between multiple processes with strong consistency and optimistic concurrency. Checkpoint A kind of Event that signals all Readers within a Reader Group to persist their state. StreamCut A StreamCut represents a consistent position in the Stream . It contains a set of Segment and offset pairs for a single Stream which represents the complete keyspace at a given point in time. The offset always points to the event boundary and hence there will be no offset pointing to an incomplete Event .","title":"Terminology"},{"location":"terminology/#terminology","text":"The glossary of terms related to Pravega is given below: Term Definition Pravega Pravega is an open source storage system that exposes stream as the main primitive for continuous and unbounded data. Stream A durable, elastic, append-only, unbounded sequence of bytes that has good performance and strong consistency. A Stream is identified by a Stream name and a Scope . A Stream is comprised of one or more Stream Segments. Stream Segment A shard of a Stream . The number of Stream Segments in a Stream might vary over time according to load and Scaling Policy . In the absence of a Scale Event , Events written to a Stream with the same Routing Key are stored in the same Stream Segment and are ordered. When a Scale Event occurs, the set of Stream Segments of a Stream changes and Events written with a given Routing Key K before the Scaling Event are stored in a different Stream Segment compared to Events written with the same Routing Key K after the event. In conjunction with Reader Groups , the number of Stream Segments is the maximum amount of read parallelism of a Stream . Scope A namespace for Stream names. A Stream name must be unique within a Scope . Event A collection of bytes within a Stream. An Event is associated with a Routing Key. Routing Key A property of an Event used to route messages to Readers. Two Events with the same Routing Key will be read by Readers in exactly the same order they were written. Reader A software application that reads data from one or more Streams . Writer A software application that writes data to one or more Streams. Pravega Java Client Library A Java library used by applications to interface with Pravega Reader Group A named collection of one or more Readers that read from a Stream in parallel. Pravega assigns Stream Segments to the Readers ensuring that all Stream Segments are assigned to at least one Reader and that they are balanced across the Readers . Position An offset within a Stream , representing a type of recovery point for a Reader . If a Reader crashes, a Position can be used to initialize the failed Reader 's replacement so that the replaced Reader resumes processing the Stream from where the failed Reader left off. Tier 1 Storage Short term, low-latency, data storage that guarantees the durability of data written to Streams . The current implementation of Tier 1 uses Apache Bookkeeper . Tier 1 storage keeps the most recent appends to streams in Pravega. As data in Tier 1 ages, it is moved out of Tier 1 into Tier 2. Tier 2 Storage A portion of Pravega storage based on cheap and deep persistent storage technology such as HDFS , DellEMC's Isilon or DellEMC's Elastic Cloud Storage . Pravega Server A component of Pravega that implements the Pravega data plane API for operations such as reading from and writing to Streams . The data plane of Pravega, also called the Segment Store, is composed of one or more Pravega Server instances. Segment Store A collection of Pravega Servers that in aggregate form the data plane of a Pravega cluster. Controller A component of Pravega that implements the Pravega control plane API for operations such as creating and retrieving information about Streams . The control plane of Pravega is composed of one or more Controller instances coordinated by Zookeeper . Auto Scaling A Pravega concept that allows the number of Stream Segments in a Stream to change over time, based on Scaling Policy. Scaling Policy A configuration item of a Stream that determines how the number of Stream Segments in the Stream should change over time. There are three kinds of Scaling Policy , a Stream has exactly one of the following at any given time. - Fixed number of Stream Segments - Change the number of Stream Segments based on the number of bytes per second written to the Stream (Size- based) - Change the number of Stream Segments based on the number of Events per second written to the Stream (Event-based) Scale Event There are two types of Scale Event : Scale-Up Event and Scale-Down Event. A Scale Event triggers Auto Scaling . A Scale-Up Event occurs when there is an increase in load, the number of Stream Segments are increased by splitting one or more Stream Segments in the Stream . A Scale-Down Event occurs when there is a decrease in load, the number of Stream Segments are reduced by merging one or more Stream Segments in the Stream . Transaction A collection of Stream write operations that are applied atomically to the Stream . Either all of the bytes in a Transaction are written to the Stream or none of them are. State Synchronizer An abstraction built on top of Pravega to enable the implementation of replicated state using a Pravega segment to back up the state transformations. A State Synchronizer allows a piece of data to be shared between multiple processes with strong consistency and optimistic concurrency. Checkpoint A kind of Event that signals all Readers within a Reader Group to persist their state. StreamCut A StreamCut represents a consistent position in the Stream . It contains a set of Segment and offset pairs for a single Stream which represents the complete keyspace at a given point in time. The offset always points to the event boundary and hence there will be no offset pointing to an incomplete Event .","title":"Terminology"},{"location":"transactions/","text":"Working with Pravega: Transactions \u00b6 This article explores how to write a set of Events to a Stream atomically using Pravega Transactions. Instructions for running the sample applications can be found in the Pravega Samples readme . You really should be familiar with Pravega Concepts (see Pravega Concepts ) before continuing reading this page. Pravega Transactions and the Console Writer and Console Reader Apps \u00b6 We have written a couple of applications, ConsoleReader and ConsoleWriter that help illustrate reading and writing data with Pravega and in particular to illustrate the Transaction facility in the Pravega programming model. You can find those applications here . ConsoleReader \u00b6 The ConsoleReader app is very simple. It uses the Pravega Java Client Library to read from a Stream and output each Event onto the console. It runs indefinitely, so you have to kill the process to terminate the program. ConsoleWriter \u00b6 The ConsoleWriter app is a bit more sophisticated. It uses the Pravega Java Client Library to write Events to a Stream, including Events written in the context of a Pravega Transaction. To make manipulating Transactions a bit easier, we provide a console-based CLI. The help text for the CLI is shown below: ConsoleWriter Help text Enter one of the following commands at the command line prompt: If no command is entered, the line is treated as a parameter to the WRITE_EVENT command. WRITE_EVENT {event} - write the {event} out to the Stream or the current Transaction. WRITE_EVENT_RK <<{routingKey}>> , {event} - write the {event} out to the Stream or the current Transaction using {routingKey}. Note << and >> around {routingKey}. BEGIN - begin a Transaction. Only one Transaction at a time is supported by the CLI. GET_TXN_ID - output the current Transaction's Id (if a Transaction is running) FLUSH - flush the current Transaction (if a Transaction is running) COMMIT - commit the Transaction (if a Transaction is running) ABORT - abort the Transaction (if a Transaction is running) STATUS - check the status of the Transaction(if a Transaction is running) HELP - print out a list of commands. QUIT - terminate the program. examples/someStream > So writing a single Event is simple, just type some text (you don't even have to type the WRITE_EVENT command if you don't want to). But we really want to talk about Pravega Transactions, so lets dive into that. Pravega Transactions \u00b6 The idea with a Pravega Transaction is that it allows an application to prepare a set of Events that can be written \"all at once\" to a Stream. This allows an application to \"commit\" a bunch of Events Atomically. This is done by writing them into the Transaction and calling commit to append them to the Stream. An application might want to do this in cases where it wants the Events to be durably stored and later decided whether or not those Events should be appended to the Stream. This allows the application to control when the set of Events are made visible to Readers. A Transaction is created via an EventStreamWriter. Recall that an EventStreamWriter itself is created through a ClientFactory and is constructed to operate against a Stream. Transactions are therefore bound to a Stream. Once a Transaction is created, it acts a lot like a Writer. Applications Write Events to the Transaction and once acknowledged, the data is considered durably persisted in the Transaction. Note that the data written to a Transaction will not be visible to Readers until the Transaction is committed. In addition to writeEvent and writeEvent using a routing key, there are several Transaction specific operations provided: Operation Discussion getTxnId() Retrieve the unique identifier for the Transaction. Pravega generates a unique UUID for each Transaction. flush() Ensure that all Writes have been persisted. ping() Extend the duration of a Transaction. Note that after a certain amount of idle time, the Transaction will automatically abort. This is to handle the case where the client has crashed and it is no longer appropriate to keep resources associated with the Transaction. checkStatus() Return the state of the Transaction. The Transaction can be in one of the following states: Open, Committing, Committed, Aborting or Aborted. commit() Append all of the Events written to the Transaction into the Stream. Either all of the Event data will be appended to the Stream or none of it will be. abort() Terminate the Transaction, the data written to the Transaction will be deleted. Using the ConsoleWriter to Begin and Commit a Transaction \u00b6 All of the Transaction API is reflected in the ConsoleWriter's CLI command set. To begin a transaction, type BEGIN: Begin Transaction examples/someStream >begin 346d8561-3fd8-40b6-8c15-9343eeea2992 > When a Transaction is created, it returns a Transaction object parameterized to the type of Event supported by the Stream. In the case of the ConsoleWriter, the type of Event is a Java String. The command prompt changes to show the Transaction's id. Now any of the Transaction related commands can be issued (GET_TXN_ID, FLUSH, PING, COMMIT, ABORT and STATUS). Note that the BEGIN command won't work because the ConsoleWriter supports only one Transaction at a time (this is a limitation of the app, not a limitation of Pravega). When the ConsoleWriter is in a Transactional context, the WRITE_EVENT (remember if you don't type a command, ConsoleWriter assumes you want to write the text as an Event) or the WRITE_EVENT_RK will be written to the Transaction: Write Events to a Transaction 346d8561-3fd8-40b6-8c15-9343eeea2992 >m1 **** Wrote 'm1' 346d8561-3fd8-40b6-8c15-9343eeea2992 >m2 **** Wrote 'm2' 346d8561-3fd8-40b6-8c15-9343eeea2992 >m3 **** Wrote 'm3' At this point, if you look at the Stream (by invoking the ConsoleReader app on the Stream, for example), you won't see those Events written to the Stream. Events not Written to the Stream (yet) $ bin/consoleReader ... ******** Reading events from examples/someStream But when a COMMIT command is given, causing the Transaction to commit: Do the Commit 346d8561-3fd8-40b6-8c15-9343eeea2992 >commit **** Transaction commit completed. those Events are appended to the Stream and are now all available: After commit, the Events are Visible ******** Reading events from examples/someStream 'm1' 'm2' 'm3' More on Begin Transaction \u00b6 The Begin Transaction (beginTxn()) operation takes three parameters (ConsoleWriter chooses some reasonable defaults so in the CLI these are optional): Param Discussion transactionTimeout The amount of time a transaction should be allowed to run before it is automatically aborted by Pravega. This is also referred to as a \"lease\". maxExecutionTime The amount of time allowed between ping operations.","title":"Working with Transactions"},{"location":"transactions/#working-with-pravega-transactions","text":"This article explores how to write a set of Events to a Stream atomically using Pravega Transactions. Instructions for running the sample applications can be found in the Pravega Samples readme . You really should be familiar with Pravega Concepts (see Pravega Concepts ) before continuing reading this page.","title":"Working with Pravega: Transactions"},{"location":"transactions/#pravega-transactions-and-the-console-writer-and-console-reader-apps","text":"We have written a couple of applications, ConsoleReader and ConsoleWriter that help illustrate reading and writing data with Pravega and in particular to illustrate the Transaction facility in the Pravega programming model. You can find those applications here .","title":"Pravega Transactions and the Console Writer and Console Reader Apps"},{"location":"transactions/#consolereader","text":"The ConsoleReader app is very simple. It uses the Pravega Java Client Library to read from a Stream and output each Event onto the console. It runs indefinitely, so you have to kill the process to terminate the program.","title":"ConsoleReader"},{"location":"transactions/#consolewriter","text":"The ConsoleWriter app is a bit more sophisticated. It uses the Pravega Java Client Library to write Events to a Stream, including Events written in the context of a Pravega Transaction. To make manipulating Transactions a bit easier, we provide a console-based CLI. The help text for the CLI is shown below: ConsoleWriter Help text Enter one of the following commands at the command line prompt: If no command is entered, the line is treated as a parameter to the WRITE_EVENT command. WRITE_EVENT {event} - write the {event} out to the Stream or the current Transaction. WRITE_EVENT_RK <<{routingKey}>> , {event} - write the {event} out to the Stream or the current Transaction using {routingKey}. Note << and >> around {routingKey}. BEGIN - begin a Transaction. Only one Transaction at a time is supported by the CLI. GET_TXN_ID - output the current Transaction's Id (if a Transaction is running) FLUSH - flush the current Transaction (if a Transaction is running) COMMIT - commit the Transaction (if a Transaction is running) ABORT - abort the Transaction (if a Transaction is running) STATUS - check the status of the Transaction(if a Transaction is running) HELP - print out a list of commands. QUIT - terminate the program. examples/someStream > So writing a single Event is simple, just type some text (you don't even have to type the WRITE_EVENT command if you don't want to). But we really want to talk about Pravega Transactions, so lets dive into that.","title":"ConsoleWriter"},{"location":"transactions/#pravega-transactions","text":"The idea with a Pravega Transaction is that it allows an application to prepare a set of Events that can be written \"all at once\" to a Stream. This allows an application to \"commit\" a bunch of Events Atomically. This is done by writing them into the Transaction and calling commit to append them to the Stream. An application might want to do this in cases where it wants the Events to be durably stored and later decided whether or not those Events should be appended to the Stream. This allows the application to control when the set of Events are made visible to Readers. A Transaction is created via an EventStreamWriter. Recall that an EventStreamWriter itself is created through a ClientFactory and is constructed to operate against a Stream. Transactions are therefore bound to a Stream. Once a Transaction is created, it acts a lot like a Writer. Applications Write Events to the Transaction and once acknowledged, the data is considered durably persisted in the Transaction. Note that the data written to a Transaction will not be visible to Readers until the Transaction is committed. In addition to writeEvent and writeEvent using a routing key, there are several Transaction specific operations provided: Operation Discussion getTxnId() Retrieve the unique identifier for the Transaction. Pravega generates a unique UUID for each Transaction. flush() Ensure that all Writes have been persisted. ping() Extend the duration of a Transaction. Note that after a certain amount of idle time, the Transaction will automatically abort. This is to handle the case where the client has crashed and it is no longer appropriate to keep resources associated with the Transaction. checkStatus() Return the state of the Transaction. The Transaction can be in one of the following states: Open, Committing, Committed, Aborting or Aborted. commit() Append all of the Events written to the Transaction into the Stream. Either all of the Event data will be appended to the Stream or none of it will be. abort() Terminate the Transaction, the data written to the Transaction will be deleted.","title":"Pravega Transactions"},{"location":"transactions/#using-the-consolewriter-to-begin-and-commit-a-transaction","text":"All of the Transaction API is reflected in the ConsoleWriter's CLI command set. To begin a transaction, type BEGIN: Begin Transaction examples/someStream >begin 346d8561-3fd8-40b6-8c15-9343eeea2992 > When a Transaction is created, it returns a Transaction object parameterized to the type of Event supported by the Stream. In the case of the ConsoleWriter, the type of Event is a Java String. The command prompt changes to show the Transaction's id. Now any of the Transaction related commands can be issued (GET_TXN_ID, FLUSH, PING, COMMIT, ABORT and STATUS). Note that the BEGIN command won't work because the ConsoleWriter supports only one Transaction at a time (this is a limitation of the app, not a limitation of Pravega). When the ConsoleWriter is in a Transactional context, the WRITE_EVENT (remember if you don't type a command, ConsoleWriter assumes you want to write the text as an Event) or the WRITE_EVENT_RK will be written to the Transaction: Write Events to a Transaction 346d8561-3fd8-40b6-8c15-9343eeea2992 >m1 **** Wrote 'm1' 346d8561-3fd8-40b6-8c15-9343eeea2992 >m2 **** Wrote 'm2' 346d8561-3fd8-40b6-8c15-9343eeea2992 >m3 **** Wrote 'm3' At this point, if you look at the Stream (by invoking the ConsoleReader app on the Stream, for example), you won't see those Events written to the Stream. Events not Written to the Stream (yet) $ bin/consoleReader ... ******** Reading events from examples/someStream But when a COMMIT command is given, causing the Transaction to commit: Do the Commit 346d8561-3fd8-40b6-8c15-9343eeea2992 >commit **** Transaction commit completed. those Events are appended to the Stream and are now all available: After commit, the Events are Visible ******** Reading events from examples/someStream 'm1' 'm2' 'm3'","title":"Using the ConsoleWriter to Begin and Commit a Transaction"},{"location":"transactions/#more-on-begin-transaction","text":"The Begin Transaction (beginTxn()) operation takes three parameters (ConsoleWriter chooses some reasonable defaults so in the CLI these are optional): Param Discussion transactionTimeout The amount of time a transaction should be allowed to run before it is automatically aborted by Pravega. This is also referred to as a \"lease\". maxExecutionTime The amount of time allowed between ping operations.","title":"More on Begin Transaction"},{"location":"wire-protocol/","text":"Pravega Streaming Service Wire Protocol \u00b6 This page describes the proposed Wire Protocol for the Streaming Service. See Pravega Concepts for more information. Protocol \u00b6 Data is sent over the wire in self-contained \"messages\" that are either \"requests\" (messages sent from the client to the server) or \"replies\" (responses sent from the server to the client). All the requests and replies have 8 byte headers with two fields (all data is written in BigEndian format). Message Type : An Integer (4 bytes) identifies the message type and determines the subsequent fields. (Note that the protocol can be extended by adding new types.) Length : An Integer (4 bytes) (Messages should be less than 2 24 , but the upper bits remain zero). Payload size of the message (possibly zero, indicating there is no data). The remainder of the fields are specific to the type of the message. A few important messages are listed below. Protocol Primitive Types \u00b6 The protocol is built out of the following primitive types. Type Description BOOLEAN (1 byte) Values 0 and 1 are used to represent False and True respectively. When reading a boolean value, any non-zero value is considered true. STRING (2 bytes) A sequence of characters. The first 2 bytes are used to indicate the byte length of the UTF-8 encoded character sequence, which is non-negative. This is followed by the UTF-8 encoding of the string. VARLONG (8 bytes) An Integer between -2 63 and 2 63 -1 inclusive. Encoding follows the variable-length zig-zag encoding from Google Protocol Buffers . INT (4 bytes) An Integer between -2 31 and 2 31 -1 inclusive. UUID (16 bytes) Universally Unique Identifiers (UUID) as defined by RFC 4122, ISO/IEC 9834-8:2005, and related standards. It can be used as a global unique 128-bit identifier. Reading \u00b6 Read Segment - Request \u00b6 Field Datatype Description Segment String The Stream Segment that was read. Offset Long The Offset in the Stream Segment to read from. suggestedLength of Reply Integer The clients can request for the required length to the server (but the server may allot a different number of bytes. delegationToken String This was added to perform auth . It is an opaque-to-the-client token provided by the Controller that says it's allowed to make this call. RequestId Long The client-generated ID that identifies a client request. More information on Segment Request messages like MergeSegment , SealSegment , TruncateSegment and DeleteSegment , can be found here . Segment Read - Reply \u00b6 Field Datatype Description Segment String This Segment indicates the Stream Segment that was read. Offset Long The Offset in the Stream Segment to read from. Tail Boolean If the read reached the tail of the Stream Segment. EndOfSegment Boolean If the read reached the end of the Stream Segment. Data Binary Remaining length in the message. RequestId Long The client-generated ID that identifies a client request. The client requests to read from a particular Segment at a particular Offset . It then receives one or more replies in the form of SegmentRead messages. These contain the data they requested (assuming it exists). The server may decide transferring to the client more or less data than it was asked for, splitting that data in a suitable number of reply messages. More information on Segment Reply messages like SegmentIsSealed , SegmentIsTruncated , SegmentAlreadyExists , NoSuchSegment and TableSegmentNotEmpty , can be found here . Appending \u00b6 Setup Append - Request \u00b6 Field Datatype Description RequestId Long The client-generated ID that identifies a client request. writerId UUID Identifies the requesting appender. Segment String This Segment indicates the Stream Segment that was read. delegationToken String This was added to perform auth . It is an opaque-to-the-client token provided by the Controller that says it's allowed to make this call. Append Setup - Reply \u00b6 Field Datatype Description RequestId Long The client-generated ID that identifies a client request. Segment String This Segment indicates the Stream Segment to append to. writerId UUID Identifies the requesting appender. This ID is used to identify the Segment for which an AppendBlock is destined. lastEventNumber Long Specifies the last event number in the Stream. AppendBlock - Request \u00b6 Field Datatype Description writerId UUID Identifies the requesting appender. Data Binary This holds the contents of the block. RequestId Long The client-generated ID that identifies a client request. AppendBlockEnd - Request \u00b6 Field Datatype Description writerId UUID Identifies the requesting appender. sizeOfWholeEvents Integer The total number of bytes in this block (starting from the beginning) that is composed of whole (meaning non-partial) events. Data Binary This holds the contents of the block. numEvents Integer Specifies the current number of events. lastEventNumber Long Specifies the value of last event number in the Stream. RequestId Long The client-generated ID that identifies a client request. The ApppendBlockEnd has a sizeOfWholeEvents to allow the append block to be less than full. This allows the client to begin writing a block before it has a large number of events. This avoids the need to buffer up events in the client and allows for lower latency. Partial Event - Request/Reply \u00b6 Data : A Partial Event is an Event at the end of an Append block that did not fully fit in the Append block. The remainder of the Event will be available in the AppendBlockEnd . Event - Request \u00b6 Field Description Data Specifies the Event's data (only valid inside the block). Data Appended - Reply \u00b6 Field Datatype Description writerId UUID Identifies the requesting appender. eventNumber Long This matches the lastEventNumber in the append block. previousEventNumber Long This is the previous value of eventNumber that was returned in the last DataAppeneded . RequestId Long The client-generated ID that identifies a client request. When appending a client: Establishes a connection to the host chosen by it. Sends a \"Setup Append\" request. Waits for the \"Append Setup\" reply. After receiving the \"Append Setup\" reply, it performs the following: Send a AppendBlock request. Send as many Events that can fit in the block. Send an AppendBlockEnd request. While this is happening, the server will be periodically sending it DataAppended replies acking messages. Note that there can be multiple \"Appends Setup\" for a given TCP connection. This allows a client to share a connection when producing to multiple Segments. A client can optimize its appending by specifying a large value in it's AppendBlock message, as the events inside of the block do not need to be processed individually. Segment Attribute \u00b6 GetSegmentAttribute - Request \u00b6 Field Datatype Description RequestId Long The client-generated ID that identifies a client request. SegmentName String The Segment to retrieve the attribute from. attributeId UUID The attribute to retrieve. delegationToken String This was added to perform auth . It is an opaque-to-the-client token provided by the Controller that says it's allowed to make this call. SegmentAtrribute - Reply \u00b6 Field Datatype Description RequestId Long The client-generated ID that identifies a client request. Value Long The value of the attribute. More information on SegmentAttribute Request message like updateSegmentAttribute and Reply message like SegmentAttributeUpdate can be found here . TableSegment \u00b6 ReadTable - Request \u00b6 Field Datatype Description RequestId Long The client-generated ID that identifies a client request. Segment String The Stream Segment that was read. delegationToken String This was added to perform auth . It is an opaque-to-the-client token provided by the Controller that says it's allowed to make this call. keys List The version of the key is always set to io.pravega.segmentstore.contracts.tables.TableKey.NO_VERSION . More information on TableSegments Request messages like MergeTableSegments , SealTableSegment , DeleteTableSegment , UpdateTableEntries , RemoveTableKeys , ReadTableKeys and ReadTableEntries can be found here . TableRead - Reply \u00b6 Field Datatype Description RequestId Long The client-generated ID that identifies a client request. Segment String The Stream Segment that was read. Entries TableEntries The entries of the Table that was read. More information on TableSegments Reply messages like TableEntriesUpdated , TableKeysRemoved , TableKeysRead and TableEntriesRead can be found here .","title":"Wire Protocol"},{"location":"wire-protocol/#pravega-streaming-service-wire-protocol","text":"This page describes the proposed Wire Protocol for the Streaming Service. See Pravega Concepts for more information.","title":"Pravega Streaming Service Wire Protocol"},{"location":"wire-protocol/#protocol","text":"Data is sent over the wire in self-contained \"messages\" that are either \"requests\" (messages sent from the client to the server) or \"replies\" (responses sent from the server to the client). All the requests and replies have 8 byte headers with two fields (all data is written in BigEndian format). Message Type : An Integer (4 bytes) identifies the message type and determines the subsequent fields. (Note that the protocol can be extended by adding new types.) Length : An Integer (4 bytes) (Messages should be less than 2 24 , but the upper bits remain zero). Payload size of the message (possibly zero, indicating there is no data). The remainder of the fields are specific to the type of the message. A few important messages are listed below.","title":"Protocol"},{"location":"wire-protocol/#protocol-primitive-types","text":"The protocol is built out of the following primitive types. Type Description BOOLEAN (1 byte) Values 0 and 1 are used to represent False and True respectively. When reading a boolean value, any non-zero value is considered true. STRING (2 bytes) A sequence of characters. The first 2 bytes are used to indicate the byte length of the UTF-8 encoded character sequence, which is non-negative. This is followed by the UTF-8 encoding of the string. VARLONG (8 bytes) An Integer between -2 63 and 2 63 -1 inclusive. Encoding follows the variable-length zig-zag encoding from Google Protocol Buffers . INT (4 bytes) An Integer between -2 31 and 2 31 -1 inclusive. UUID (16 bytes) Universally Unique Identifiers (UUID) as defined by RFC 4122, ISO/IEC 9834-8:2005, and related standards. It can be used as a global unique 128-bit identifier.","title":"Protocol Primitive Types"},{"location":"wire-protocol/#reading","text":"","title":"Reading"},{"location":"wire-protocol/#read-segment-request","text":"Field Datatype Description Segment String The Stream Segment that was read. Offset Long The Offset in the Stream Segment to read from. suggestedLength of Reply Integer The clients can request for the required length to the server (but the server may allot a different number of bytes. delegationToken String This was added to perform auth . It is an opaque-to-the-client token provided by the Controller that says it's allowed to make this call. RequestId Long The client-generated ID that identifies a client request. More information on Segment Request messages like MergeSegment , SealSegment , TruncateSegment and DeleteSegment , can be found here .","title":"Read Segment - Request"},{"location":"wire-protocol/#segment-read-reply","text":"Field Datatype Description Segment String This Segment indicates the Stream Segment that was read. Offset Long The Offset in the Stream Segment to read from. Tail Boolean If the read reached the tail of the Stream Segment. EndOfSegment Boolean If the read reached the end of the Stream Segment. Data Binary Remaining length in the message. RequestId Long The client-generated ID that identifies a client request. The client requests to read from a particular Segment at a particular Offset . It then receives one or more replies in the form of SegmentRead messages. These contain the data they requested (assuming it exists). The server may decide transferring to the client more or less data than it was asked for, splitting that data in a suitable number of reply messages. More information on Segment Reply messages like SegmentIsSealed , SegmentIsTruncated , SegmentAlreadyExists , NoSuchSegment and TableSegmentNotEmpty , can be found here .","title":"Segment Read - Reply"},{"location":"wire-protocol/#appending","text":"","title":"Appending"},{"location":"wire-protocol/#setup-append-request","text":"Field Datatype Description RequestId Long The client-generated ID that identifies a client request. writerId UUID Identifies the requesting appender. Segment String This Segment indicates the Stream Segment that was read. delegationToken String This was added to perform auth . It is an opaque-to-the-client token provided by the Controller that says it's allowed to make this call.","title":"Setup Append - Request"},{"location":"wire-protocol/#append-setup-reply","text":"Field Datatype Description RequestId Long The client-generated ID that identifies a client request. Segment String This Segment indicates the Stream Segment to append to. writerId UUID Identifies the requesting appender. This ID is used to identify the Segment for which an AppendBlock is destined. lastEventNumber Long Specifies the last event number in the Stream.","title":"Append\u00a0Setup\u00a0- Reply"},{"location":"wire-protocol/#appendblock-request","text":"Field Datatype Description writerId UUID Identifies the requesting appender. Data Binary This holds the contents of the block. RequestId Long The client-generated ID that identifies a client request.","title":"AppendBlock - Request"},{"location":"wire-protocol/#appendblockend-request","text":"Field Datatype Description writerId UUID Identifies the requesting appender. sizeOfWholeEvents Integer The total number of bytes in this block (starting from the beginning) that is composed of whole (meaning non-partial) events. Data Binary This holds the contents of the block. numEvents Integer Specifies the current number of events. lastEventNumber Long Specifies the value of last event number in the Stream. RequestId Long The client-generated ID that identifies a client request. The ApppendBlockEnd has a sizeOfWholeEvents to allow the append block to be less than full. This allows the client to begin writing a block before it has a large number of events. This avoids the need to buffer up events in the client and allows for lower latency.","title":"AppendBlockEnd - Request"},{"location":"wire-protocol/#partial-event-requestreply","text":"Data : A Partial Event is an Event at the end of an Append block that did not fully fit in the Append block. The remainder of the Event will be available in the AppendBlockEnd .","title":"Partial Event - Request/Reply"},{"location":"wire-protocol/#event-request","text":"Field Description Data Specifies the Event's data (only valid inside the block).","title":"Event - Request"},{"location":"wire-protocol/#data-appended-reply","text":"Field Datatype Description writerId UUID Identifies the requesting appender. eventNumber Long This matches the lastEventNumber in the append block. previousEventNumber Long This is the previous value of eventNumber that was returned in the last DataAppeneded . RequestId Long The client-generated ID that identifies a client request. When appending a client: Establishes a connection to the host chosen by it. Sends a \"Setup Append\" request. Waits for the \"Append Setup\" reply. After receiving the \"Append Setup\" reply, it performs the following: Send a AppendBlock request. Send as many Events that can fit in the block. Send an AppendBlockEnd request. While this is happening, the server will be periodically sending it DataAppended replies acking messages. Note that there can be multiple \"Appends Setup\" for a given TCP connection. This allows a client to share a connection when producing to multiple Segments. A client can optimize its appending by specifying a large value in it's AppendBlock message, as the events inside of the block do not need to be processed individually.","title":"Data Appended - Reply"},{"location":"wire-protocol/#segment-attribute","text":"","title":"Segment Attribute"},{"location":"wire-protocol/#getsegmentattribute-request","text":"Field Datatype Description RequestId Long The client-generated ID that identifies a client request. SegmentName String The Segment to retrieve the attribute from. attributeId UUID The attribute to retrieve. delegationToken String This was added to perform auth . It is an opaque-to-the-client token provided by the Controller that says it's allowed to make this call.","title":"GetSegmentAttribute - Request"},{"location":"wire-protocol/#segmentatrribute-reply","text":"Field Datatype Description RequestId Long The client-generated ID that identifies a client request. Value Long The value of the attribute. More information on SegmentAttribute Request message like updateSegmentAttribute and Reply message like SegmentAttributeUpdate can be found here .","title":"SegmentAtrribute - Reply"},{"location":"wire-protocol/#tablesegment","text":"","title":"TableSegment"},{"location":"wire-protocol/#readtable-request","text":"Field Datatype Description RequestId Long The client-generated ID that identifies a client request. Segment String The Stream Segment that was read. delegationToken String This was added to perform auth . It is an opaque-to-the-client token provided by the Controller that says it's allowed to make this call. keys List The version of the key is always set to io.pravega.segmentstore.contracts.tables.TableKey.NO_VERSION . More information on TableSegments Request messages like MergeTableSegments , SealTableSegment , DeleteTableSegment , UpdateTableEntries , RemoveTableKeys , ReadTableKeys and ReadTableEntries can be found here .","title":"ReadTable - Request"},{"location":"wire-protocol/#tableread-reply","text":"Field Datatype Description RequestId Long The client-generated ID that identifies a client request. Segment String The Stream Segment that was read. Entries TableEntries The entries of the Table that was read. More information on TableSegments Reply messages like TableEntriesUpdated , TableKeysRemoved , TableKeysRead and TableEntriesRead can be found here .","title":"TableRead - Reply"},{"location":"auth/auth-plugin/","text":"Implementation of Pravega Authentication/Authorization Plugin \u00b6 This guide describes in detail the Authentication/Authorization plugin model for Pravega. Pravega auth interface \u00b6 The custom implementation performs the implementation of the AuthHandler interface. Dynamic loading of auth implementations \u00b6 Administrators and users are allowed to implement their own Authorization/Authentication plugins. Multiple plugins of such kind can exist together. The implementation of plugin follows the Java Service Loader approach. The required Jars for the custom implementation needs to be located in the CLASSPATH to enable the access for Pravega Controller for implementation. Note: The custom implementation performs the implementation of the AuthHandler interface.","title":"Pravega Authentication/Authorization Plugin"},{"location":"auth/auth-plugin/#implementation-of-pravega-authenticationauthorization-plugin","text":"This guide describes in detail the Authentication/Authorization plugin model for Pravega.","title":"Implementation of Pravega Authentication/Authorization Plugin"},{"location":"auth/auth-plugin/#pravega-auth-interface","text":"The custom implementation performs the implementation of the AuthHandler interface.","title":"Pravega auth interface"},{"location":"auth/auth-plugin/#dynamic-loading-of-auth-implementations","text":"Administrators and users are allowed to implement their own Authorization/Authentication plugins. Multiple plugins of such kind can exist together. The implementation of plugin follows the Java Service Loader approach. The required Jars for the custom implementation needs to be located in the CLASSPATH to enable the access for Pravega Controller for implementation. Note: The custom implementation performs the implementation of the AuthHandler interface.","title":"Dynamic loading of auth implementations"},{"location":"auth/client-auth/","text":"Client auth Interface \u00b6 Pravega client can access Pravega APIs through grpc . Some of the admin APIs can be accessed via REST API. The Authorization/Authentication API and plugin works for both of these interfaces. grpc Client auth Interface \u00b6 If multiple plugin exists, a client selects its auth handler by setting a grpc header with the name method . This is performed by implementing Credentials interface by passing through the ClientConfig object to the Pravega client. The parameters for authentication are passed through custom grpc headers. These are extracted through grpc interceptors and passed on to the specific auth plugin. This plugin is identified by the method header. Dynamic extraction of the auth parameters on the client \u00b6 Dynamic extraction of parameters is also possible using the system properties or environment variables. The order of preference is listed below: User explicitly provides a credential object through the API. This results in overriding the other settings. System properties: System properties are defined in the format: pravega.client.auth.* Environment variables: Environment variables are defined in the format: pravega_client_auth_* In case of option 2 and 3, the caller decides on whether, the class needs to be loaded dynamically by setting the property pravega.client.auth.loadDynamic to true. REST Client auth Interface \u00b6 The REST client in order to access the Pravega API uses the similar approach as mentioned in the above sections. The custom auth parameters are sent as the part of the Authorization HTTP header. The REST server implementation on Pravega Controller extracts these headers and passes it to the valid auth plugin implementation. Then it resumes, if the authentication and authorization matches the intended access pattern.","title":"Client Auth Interface"},{"location":"auth/client-auth/#client-auth-interface","text":"Pravega client can access Pravega APIs through grpc . Some of the admin APIs can be accessed via REST API. The Authorization/Authentication API and plugin works for both of these interfaces.","title":"Client auth Interface"},{"location":"auth/client-auth/#grpc-client-auth-interface","text":"If multiple plugin exists, a client selects its auth handler by setting a grpc header with the name method . This is performed by implementing Credentials interface by passing through the ClientConfig object to the Pravega client. The parameters for authentication are passed through custom grpc headers. These are extracted through grpc interceptors and passed on to the specific auth plugin. This plugin is identified by the method header.","title":"grpc Client auth Interface"},{"location":"auth/client-auth/#dynamic-extraction-of-the-auth-parameters-on-the-client","text":"Dynamic extraction of parameters is also possible using the system properties or environment variables. The order of preference is listed below: User explicitly provides a credential object through the API. This results in overriding the other settings. System properties: System properties are defined in the format: pravega.client.auth.* Environment variables: Environment variables are defined in the format: pravega_client_auth_* In case of option 2 and 3, the caller decides on whether, the class needs to be loaded dynamically by setting the property pravega.client.auth.loadDynamic to true.","title":"Dynamic extraction of the auth parameters on the client"},{"location":"auth/client-auth/#rest-client-auth-interface","text":"The REST client in order to access the Pravega API uses the similar approach as mentioned in the above sections. The custom auth parameters are sent as the part of the Authorization HTTP header. The REST server implementation on Pravega Controller extracts these headers and passes it to the valid auth plugin implementation. Then it resumes, if the authentication and authorization matches the intended access pattern.","title":"REST Client auth Interface"},{"location":"deployment/aws-install/","text":"Running on AWS \u00b6 Pre-reqs: Have an AWS account and have Terraform installed. To install and download Terraform, follow the instructions here: https://www.terraform.io/downloads.html Deploy Steps \u00b6 Run \"sudo terraform apply\" under the deployment/aws directory, and then follow prompt instruction, enter the AWS account credentials. There are four variables would be needed: AWS access key and AWS secret key, which can be obtained from AWS account cred_path, which is the absolute path of key pair file. It would be downloaded when key pair is created AWS region: Currently, we only support two regions: us-east-1 and us-west-1. We list below the instance types we recommend for them. Region us-east-1: Three m3.xlarge for EMR Three m3.2xlarge for Pravega One m3.medium for bootstrap, also as client Region us-west-1: Three m3.xlarge for EMR Three i3.4xlarge for Pravega One i3.xlarge for bootstrap, also as client Other instance types might present conflicts with the Linux Images used. How to customize the pravega cluster \u00b6 Change default value of \"pravega_num\" in variable.tf Define the your own nodes layout in installer/hosts-template, default hosts-template is under installer directory. There are three sections of hosts-template: 1. common-services is the section for zookeeper and bookkeeper 2. pravega-controller is the section for pravega controller node 3. pravega-hosts is the section for the pravega segment store node. How to destroy the pravega cluster \u00b6 Run \"sudo terraform destroy\", then enter \"yes\"","title":"Running in the Cloud (AWS)"},{"location":"deployment/aws-install/#running-on-aws","text":"Pre-reqs: Have an AWS account and have Terraform installed. To install and download Terraform, follow the instructions here: https://www.terraform.io/downloads.html","title":"Running on AWS"},{"location":"deployment/aws-install/#deploy-steps","text":"Run \"sudo terraform apply\" under the deployment/aws directory, and then follow prompt instruction, enter the AWS account credentials. There are four variables would be needed: AWS access key and AWS secret key, which can be obtained from AWS account cred_path, which is the absolute path of key pair file. It would be downloaded when key pair is created AWS region: Currently, we only support two regions: us-east-1 and us-west-1. We list below the instance types we recommend for them. Region us-east-1: Three m3.xlarge for EMR Three m3.2xlarge for Pravega One m3.medium for bootstrap, also as client Region us-west-1: Three m3.xlarge for EMR Three i3.4xlarge for Pravega One i3.xlarge for bootstrap, also as client Other instance types might present conflicts with the Linux Images used.","title":"Deploy Steps"},{"location":"deployment/aws-install/#how-to-customize-the-pravega-cluster","text":"Change default value of \"pravega_num\" in variable.tf Define the your own nodes layout in installer/hosts-template, default hosts-template is under installer directory. There are three sections of hosts-template: 1. common-services is the section for zookeeper and bookkeeper 2. pravega-controller is the section for pravega controller node 3. pravega-hosts is the section for the pravega segment store node.","title":"How to customize the pravega cluster"},{"location":"deployment/aws-install/#how-to-destroy-the-pravega-cluster","text":"Run \"sudo terraform destroy\", then enter \"yes\"","title":"How to destroy the pravega cluster"},{"location":"deployment/dcos-install/","text":"Deploying on DC/OS \u00b6 Prerequisities: DC/OS cli needs to be installed. To install the cli, follow the instructions here: https://docs.mesosphere.com/1.8/usage/cli/install/ Pravega can be run on DC/OS by leveraging Marathon. PravegaGroup.json defines the docker hub image locations and necessary application configration to start a simple Pravega cluster. Download PravegaGroup.json to your DC/OS cluster. For example: wget https://github.com/pravega/pravega/blob/master/PravegaGroup.json Add to Marathon using: dcos marathon group add PravegaGroup.json","title":"Deployment on DC/OS"},{"location":"deployment/dcos-install/#deploying-on-dcos","text":"Prerequisities: DC/OS cli needs to be installed. To install the cli, follow the instructions here: https://docs.mesosphere.com/1.8/usage/cli/install/ Pravega can be run on DC/OS by leveraging Marathon. PravegaGroup.json defines the docker hub image locations and necessary application configration to start a simple Pravega cluster. Download PravegaGroup.json to your DC/OS cluster. For example: wget https://github.com/pravega/pravega/blob/master/PravegaGroup.json Add to Marathon using: dcos marathon group add PravegaGroup.json","title":"Deploying on DC/OS"},{"location":"deployment/deployment/","text":"Pravega Deployment Overview \u00b6 This guide describes the options for running Pravega for development, testing and in production. Pravega Modes \u00b6 There are two modes for running Pravega. Standalone - Standalone mode is suitable for development and testing Pravega applications. It can either be run from the source code, from the distribution package or as a docker container. Distributed - Distributed mode runs each component separately on a single or multiple nodes. This is suitable for production in addition for development and testing. The deployment options in this mode include a manual installation, running in a docker swarm or DC/OS. Prerequisites \u00b6 The following prerequisites are required for running Pravega in all modes. Java 8 The following prerequisites are required for running in production. These are only required for running in distributed mode. External HDFS 2.7 Zookeeper 3.5.4-beta Bookkeeper 4.7.3 For more details on the prerequisites and recommended configuration options for bookkeeper see the Manual Install Guide . Installation \u00b6 There are multiple options provided for running Pravega in different environments. Most of these use the installation package from a Pravega release. You can find the latest Pravega release on the GitHub Releases page . Local - Running Pravega locally is suitable for development and testing. Running from source Local Standalone Mode Docker Compose (Distributed Mode) Production - Multi-node installation suitable for running in production. Manual Installation Kubernetes Docker Swarm DC/OS Cloud - AWS","title":"Deployment Overview"},{"location":"deployment/deployment/#pravega-deployment-overview","text":"This guide describes the options for running Pravega for development, testing and in production.","title":"Pravega Deployment Overview"},{"location":"deployment/deployment/#pravega-modes","text":"There are two modes for running Pravega. Standalone - Standalone mode is suitable for development and testing Pravega applications. It can either be run from the source code, from the distribution package or as a docker container. Distributed - Distributed mode runs each component separately on a single or multiple nodes. This is suitable for production in addition for development and testing. The deployment options in this mode include a manual installation, running in a docker swarm or DC/OS.","title":"Pravega Modes"},{"location":"deployment/deployment/#prerequisites","text":"The following prerequisites are required for running Pravega in all modes. Java 8 The following prerequisites are required for running in production. These are only required for running in distributed mode. External HDFS 2.7 Zookeeper 3.5.4-beta Bookkeeper 4.7.3 For more details on the prerequisites and recommended configuration options for bookkeeper see the Manual Install Guide .","title":"Prerequisites"},{"location":"deployment/deployment/#installation","text":"There are multiple options provided for running Pravega in different environments. Most of these use the installation package from a Pravega release. You can find the latest Pravega release on the GitHub Releases page . Local - Running Pravega locally is suitable for development and testing. Running from source Local Standalone Mode Docker Compose (Distributed Mode) Production - Multi-node installation suitable for running in production. Manual Installation Kubernetes Docker Swarm DC/OS Cloud - AWS","title":"Installation"},{"location":"deployment/docker-swarm/","text":"Deploying in a Docker Swarm \u00b6 Docker Swarm can be used to quickly spin up a distributed Pravega cluster that can easily scale up and down. Unlike docker-compose , this is useful for more than just testing and development. In future, Docker Swarm will be suitable for production workloads. Prerequisites \u00b6 A working single or multi-node Docker Swarm. Please refer to swarm-tutorial . HDFS and ZooKeeper. We provide compose files for both of these, but both are single instance deploys that should only be used for testing/development. More information to deploy our HDFS and ZooKeeper can be found here . Please refer to hdfs.yml and zookeeper.yml files. docker stack up --compose-file hdfs.yml pravega docker stack up --compose-file zookeeper.yml pravega This runs a single node HDFS container and single node ZooKeeper inside the pravega_default overlay network, and adds them to the pravega stack. HDFS is reachable inside the swarm as hdfs://hdfs:8020 ZooKeeper is reachable at tcp://zookeeper:2181. Either one or both of these can be initiated for running, but serious workloads cannot be handled. Network Considerations \u00b6 Each Pravega Segment Store needs to be directly reachable by clients. Docker Swarm runs all traffic coming into its overlay network through a load balancer, which makes it more or less impossible to reach a specific instance of a scaled service from outside the cluster. This means that Pravega clients must either run inside the swarm, or we must run each Segment Store as a unique service on a distinct port. Both approaches are demonstrated in the below section. Deploying (Swarm only clients) \u00b6 The easiest way to deploy is to keep all traffic inside the swarm. This means your client apps must also run inside the swarm. ZK_URL=zookeeper:2181 HDFS_URL=hdfs:8020 docker stack up --compose-file pravega.yml pravega Note that ZK_URL and HDFS_URL don't include the protocol. They have default values assigned as zookeeper:2181 and hdfs:8020 , when deployed using zookeeper.yml / hdfs.yml . Your clients must then be deployed into the swarm, using the following command. docker service create --name=myapp --network=pravega_default mycompany/myapp The crucial bit being --network=pravega_default. Your client should talk to Pravega at tcp://controller:9090. Deploying (External clients) \u00b6 If you intend to run clients outside the swarm, you must provide two additional environment variables, PUBLISHED_ADDRESS and LISTENING_ADDRESS . PUBLISHED_ADDRESS must be an IP or Hostname that resolves to one or more swarm nodes (or a load balancer that sits in front of them). LISTENING_ADDRESS should always be 0 , or 0.0.0.0 . PUBLISHED_ADDRESS=1.2.3.4 LISTENING_ADDRESS=0 ZK_URL=zookeeper:2181 HDFS_URL=hdfs:8020 docker stack up --compose-file pravega.yml pravega As above, ZK_URL and HDFS_URL can be omitted if the services are at their default locations. Your client should talk to Pravega at tcp://${PUBLISHED_ADDRESS}:9090`. Scaling BookKeeper \u00b6 BookKeeper can be scaled up or down using the following command. docker service scale pravega_bookkeeper=N As configured in this package, Pravega requires at least 3 BookKeeper nodes, (i.e., N must be >= 3.) Scaling Pravega Controller \u00b6 Pravega Controller can be scaled up or down using the following command. docker service scale pravega_controller=N Scaling Pravega Segment Store (Swarm only clients) \u00b6 If you app will run inside the swarm and you didn't run with PUBLISHED_ADDRESS , you can scale the Segment Store the usual way using the following command. docker service scale pravega_segmentstore=N Scaling Pravega Segment Store (External clients) \u00b6 If you require access to Pravega from outside the swarm and have deployed with PUBLISHED_ADDRESS , each instance of the Segment Store must be deployed as a unique service. This is a cumbersome process, but we've provided a helper script to make it fairly painless: ./scale_segmentstore N Tearing down \u00b6 All services, (including HDFS and ZooKeeper if you've deployed our package) can be destroyed using the following command. docker stack down pravega","title":"Deployment on Docker Swarm"},{"location":"deployment/docker-swarm/#deploying-in-a-docker-swarm","text":"Docker Swarm can be used to quickly spin up a distributed Pravega cluster that can easily scale up and down. Unlike docker-compose , this is useful for more than just testing and development. In future, Docker Swarm will be suitable for production workloads.","title":"Deploying in a Docker Swarm"},{"location":"deployment/docker-swarm/#prerequisites","text":"A working single or multi-node Docker Swarm. Please refer to swarm-tutorial . HDFS and ZooKeeper. We provide compose files for both of these, but both are single instance deploys that should only be used for testing/development. More information to deploy our HDFS and ZooKeeper can be found here . Please refer to hdfs.yml and zookeeper.yml files. docker stack up --compose-file hdfs.yml pravega docker stack up --compose-file zookeeper.yml pravega This runs a single node HDFS container and single node ZooKeeper inside the pravega_default overlay network, and adds them to the pravega stack. HDFS is reachable inside the swarm as hdfs://hdfs:8020 ZooKeeper is reachable at tcp://zookeeper:2181. Either one or both of these can be initiated for running, but serious workloads cannot be handled.","title":"Prerequisites"},{"location":"deployment/docker-swarm/#network-considerations","text":"Each Pravega Segment Store needs to be directly reachable by clients. Docker Swarm runs all traffic coming into its overlay network through a load balancer, which makes it more or less impossible to reach a specific instance of a scaled service from outside the cluster. This means that Pravega clients must either run inside the swarm, or we must run each Segment Store as a unique service on a distinct port. Both approaches are demonstrated in the below section.","title":"Network Considerations"},{"location":"deployment/docker-swarm/#deploying-swarm-only-clients","text":"The easiest way to deploy is to keep all traffic inside the swarm. This means your client apps must also run inside the swarm. ZK_URL=zookeeper:2181 HDFS_URL=hdfs:8020 docker stack up --compose-file pravega.yml pravega Note that ZK_URL and HDFS_URL don't include the protocol. They have default values assigned as zookeeper:2181 and hdfs:8020 , when deployed using zookeeper.yml / hdfs.yml . Your clients must then be deployed into the swarm, using the following command. docker service create --name=myapp --network=pravega_default mycompany/myapp The crucial bit being --network=pravega_default. Your client should talk to Pravega at tcp://controller:9090.","title":"Deploying (Swarm only clients)"},{"location":"deployment/docker-swarm/#deploying-external-clients","text":"If you intend to run clients outside the swarm, you must provide two additional environment variables, PUBLISHED_ADDRESS and LISTENING_ADDRESS . PUBLISHED_ADDRESS must be an IP or Hostname that resolves to one or more swarm nodes (or a load balancer that sits in front of them). LISTENING_ADDRESS should always be 0 , or 0.0.0.0 . PUBLISHED_ADDRESS=1.2.3.4 LISTENING_ADDRESS=0 ZK_URL=zookeeper:2181 HDFS_URL=hdfs:8020 docker stack up --compose-file pravega.yml pravega As above, ZK_URL and HDFS_URL can be omitted if the services are at their default locations. Your client should talk to Pravega at tcp://${PUBLISHED_ADDRESS}:9090`.","title":"Deploying (External clients)"},{"location":"deployment/docker-swarm/#scaling-bookkeeper","text":"BookKeeper can be scaled up or down using the following command. docker service scale pravega_bookkeeper=N As configured in this package, Pravega requires at least 3 BookKeeper nodes, (i.e., N must be >= 3.)","title":"Scaling BookKeeper"},{"location":"deployment/docker-swarm/#scaling-pravega-controller","text":"Pravega Controller can be scaled up or down using the following command. docker service scale pravega_controller=N","title":"Scaling Pravega Controller"},{"location":"deployment/docker-swarm/#scaling-pravega-segment-store-swarm-only-clients","text":"If you app will run inside the swarm and you didn't run with PUBLISHED_ADDRESS , you can scale the Segment Store the usual way using the following command. docker service scale pravega_segmentstore=N","title":"Scaling Pravega Segment Store (Swarm only clients)"},{"location":"deployment/docker-swarm/#scaling-pravega-segment-store-external-clients","text":"If you require access to Pravega from outside the swarm and have deployed with PUBLISHED_ADDRESS , each instance of the Segment Store must be deployed as a unique service. This is a cumbersome process, but we've provided a helper script to make it fairly painless: ./scale_segmentstore N","title":"Scaling Pravega Segment Store (External clients)"},{"location":"deployment/docker-swarm/#tearing-down","text":"All services, (including HDFS and ZooKeeper if you've deployed our package) can be destroyed using the following command. docker stack down pravega","title":"Tearing down"},{"location":"deployment/kubernetes-install/","text":"Deploying in Kubernetes \u00b6 Table of Contents \u00b6 Requirements Pravega Operator Usage Installation of the Pravega Operator Deploy a sample Pravega Cluster Scale a Pravega Cluster Upgrade a Pravega Cluster Uninstall the Pravega Cluster Uninstall the Pravega Operator Configuration Use non-default service accounts Installing on a Custom Namespace with RBAC enabled Tier 2: Google Filestore Storage Tune Pravega Configurations Enable External Access Releases Requirements \u00b6 Kubernetes 1.8+ An existing Apache Zookeeper 3.5 cluster. This can be easily deployed using our Zookeeper Operator . Pravega Operator manages Pravega clusters deployed to Kubernetes and automates tasks related to operating a Pravega cluster. Usage \u00b6 Install the Pravega Operator \u00b6 Note: If you are running on Google Kubernetes Engine (GKE), please check this first . Run the following command to install the PravegaCluster custom resource definition (CRD), create the pravega-operator service account, roles, bindings, and the deploy the Pravega Operator. $ kubectl create -f deploy Verify that the Pravega Operator is running. $ kubectl get deploy NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE pravega-operator 1 1 1 1 17s Deploy a sample Pravega cluster \u00b6 Pravega requires a long term storage provider known as Tier 2 storage. The following Tier 2 storage providers are supported: Filesystem (NFS) Google Filestore DellEMC ECS HDFS (must support Append operation) The following example uses an NFS volume provisioned by the NFS Server Provisioner helm chart to provide Tier 2 storage. $ helm install stable/nfs-server-provisioner Verify that the nfs storage class is now available. $ kubectl get storageclass NAME PROVISIONER AGE nfs cluster.local/elevated-leopard-nfs-server-provisioner 24s ... Note: This is ONLY intended as a demo and should NOT be used for production deployments. Once the NFS server provisioner is installed, you can create a PersistentVolumeClaim that will be used as Tier 2 for Pravega. Create a pvc.yaml file with the following content. kind : PersistentVolumeClaim apiVersion : v1 metadata : name : pravega-tier2 spec : storageClassName : \"nfs\" accessModes : - ReadWriteMany resources : requests : storage : 50Gi $ kubectl create -f pvc.yaml Use the following YAML template to install a small development Pravega Cluster (3 Bookies, 1 Controller, 3 Segment Stores). Create a pravega.yaml file with the following content. apiVersion : \"pravega.pravega.io/v1alpha1\" kind : \"PravegaCluster\" metadata : name : \"example\" spec : version : 0.4.0 zookeeperUri : [ ZOOKEEPER_HOST ] :2181 bookkeeper : replicas : 3 image : repository : pravega/bookkeeper autoRecovery : true pravega : controllerReplicas : 1 segmentStoreReplicas : 3 image : repository : pravega/pravega tier2 : filesystem : persistentVolumeClaim : claimName : pravega-tier2 where: [ZOOKEEPER_HOST] is the host or IP address of your Zookeeper deployment. Deploy the Pravega cluster. $ kubectl create -f pravega.yaml Verify that the cluster instances and its components are being created. $ kubectl get PravegaCluster NAME VERSION DESIRED MEMBERS READY MEMBERS AGE example 0.4.0 7 0 25s After a couple of minutes, all cluster members should become ready. $ kubectl get PravegaCluster NAME VERSION DESIRED MEMBERS READY MEMBERS AGE example 0.4.0 7 7 2m $ kubectl get all -l pravega_cluster=example NAME READY STATUS RESTARTS AGE pod/example-bookie-0 1/1 Running 0 2m pod/example-bookie-1 1/1 Running 0 2m pod/example-bookie-2 1/1 Running 0 2m pod/example-pravega-controller-64ff87fc49-kqp9k 1/1 Running 0 2m pod/example-pravega-segmentstore-0 1/1 Running 0 2m pod/example-pravega-segmentstore-1 1/1 Running 0 1m pod/example-pravega-segmentstore-2 1/1 Running 0 30s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/example-bookie-headless ClusterIP None <none> 3181/TCP 2m service/example-pravega-controller ClusterIP 10.23.244.3 <none> 10080/TCP,9090/TCP 2m service/example-pravega-segmentstore-headless ClusterIP None <none> 12345/TCP 2m NAME DESIRED CURRENT READY AGE replicaset.apps/example-pravega-controller-64ff87fc49 1 1 1 2m NAME DESIRED CURRENT AGE statefulset.apps/example-bookie 3 3 2m statefulset.apps/example-pravega-segmentstore 3 3 2m By default, a PravegaCluster instance is only accessible within the cluster through the Controller ClusterIP service. From within the Kubernetes cluster, a client can connect to Pravega at: tcp://<pravega-name>-pravega-controller.<namespace>:9090 And the REST management interface is available at: http://<pravega-name>-pravega-controller.<namespace>:10080/ Check this to enable external access to a Pravega cluster. Scale a Pravega Cluster \u00b6 You can scale Pravega components independently by modifying their corresponding field in the Pravega resource spec. You can either kubectl edit the cluster or kubectl patch it. If you edit it, update the number of replicas for BookKeeper, Controller, and/or Segment Store and save the updated spec. Example of patching the Pravega resource to scale the Segment Store instances to 4. kubectl patch PravegaCluster example --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/pravega/segmentStoreReplicas\", \"value\": 4}]' Upgrade a Pravega Cluster \u00b6 Check out the Upgrade Guide . Uninstall the Pravega cluster \u00b6 $ kubectl delete -f pravega.yaml $ kubectl delete -f pvc.yaml Uninstall the Pravega cluster \u00b6 $ kubectl delete -f pravega.yaml $ kubectl delete -f pvc.yaml Uninstall the Pravega Operator \u00b6 Note that the Pravega clusters managed by the Pravega operator will NOT be deleted even if the operator is uninstalled. To delete all clusters, delete all cluster CR objects before uninstalling the Pravega Operator. $ kubectl delete -f deploy Configuration \u00b6 Use non-default service accounts \u00b6 You can optionally configure non-default service accounts for the Bookkeeper, Pravega Controller, and Pravega Segment Store pods. For BookKeeper, set the serviceAccountName field under the bookkeeper block. ... spec: bookkeeper: serviceAccountName: bk-service-account ... For Pravega, set the controllerServiceAccountName and segmentStoreServiceAccountName fields under the pravega block. ... spec: pravega: controllerServiceAccountName: ctrl-service-account segmentStoreServiceAccountName: ss-service-account ... If external access is enabled in your Pravega cluster, Segment Store pods will require access to some Kubernetes API endpoints to obtain the external IP and port. Make sure that the service account you are using for the Segment Store has, at least, the following permissions. kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: pravega-components namespace: \"pravega-namespace\" rules: - apiGroups: [\"pravega.pravega.io\"] resources: [\"*\"] verbs: [\"get\"] - apiGroups: [\"\"] resources: [\"pods\", \"services\"] verbs: [\"get\"] --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: pravega-components rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\"] Replace the namespace with your own namespace. Installing on a Custom Namespace with RBAC enabled \u00b6 Create the namespace. $ kubectl create namespace pravega-io Update the namespace configured in the deploy/role_binding.yaml file. $ sed -i -e 's/namespace: default/namespace: pravega-io/g' deploy/role_binding.yaml Apply the changes. $ kubectl -n pravega-io apply -f deploy Note that the Pravega Operator only monitors the PravegaCluster resources which are created in the same namespace, pravega-io in this example. Therefore, before creating a PravegaCluster resource, make sure an Operator exists in that namespace. $ kubectl -n pravega-io create -f example/cr.yaml $ kubectl -n pravega-io get pravegaclusters NAME AGE pravega 28m $ kubectl -n pravega-io get pods -l pravega_cluster=pravega NAME READY STATUS RESTARTS AGE pravega-bookie-0 1/1 Running 0 29m pravega-bookie-1 1/1 Running 0 29m pravega-bookie-2 1/1 Running 0 29m pravega-pravega-controller-6c54fdcdf5-947nw 1/1 Running 0 29m pravega-pravega-segmentstore-0 1/1 Running 0 29m pravega-pravega-segmentstore-1 1/1 Running 0 29m pravega-pravega-segmentstore-2 1/1 Running 0 29m Use Google Filestore Storage as Tier 2 \u00b6 Create a Google Filestore . Refer to https://cloud.google.com/filestore/docs/accessing-fileshares for more information Create a pv.yaml file with the PersistentVolume specification to provide Tier 2 storage. apiVersion : v1 kind : PersistentVolume metadata : name : pravega-volume spec : capacity : storage : 1T accessModes : - ReadWriteMany nfs : path : /[FILESHARE] server : [ IP_ADDRESS ] where: [FILESHARE] is the name of the fileshare on the Cloud Filestore instance (e.g. vol1 ) [IP_ADDRESS] is the IP address for the Cloud Filestore instance (e.g. 10.123.189.202 ) Deploy the PersistentVolume specification. $ kubectl create -f pv.yaml Create and deploy a PersistentVolumeClaim to consume the volume created. kind : PersistentVolumeClaim apiVersion : v1 metadata : name : pravega-tier2 spec : storageClassName : \"\" accessModes : - ReadWriteMany resources : requests : storage : 50Gi $ kubectl create -f pvc.yaml Use the same pravega.yaml above to deploy the Pravega cluster. Tune Pravega configuration \u00b6 Pravega has many configuration options for setting up metrics, tuning, etc. The available options can be found here and are expressed through the pravega/options part of the resource specification. All values must be expressed as Strings. ... spec : pravega : options : metrics.enableStatistics : \"true\" metrics.statsdHost : \"telegraph.default\" metrics.statsdPort : \"8125\" ... Enable External Access \u00b6 By default, a Pravega cluster uses ClusterIP services which are only accessible from within Kubernetes. However, when creating the Pravega cluster resource, you can opt to enable external access. In Pravega, clients initiate the communication with the Pravega Controller, which is a stateless component frontended by a Kubernetes service that load-balances the requests to the backend pods. Then, clients discover the individual Segment Store instances to which they directly read and write data to. Clients need to be able to reach each and every Segment Store pod in the Pravega cluster. If your Pravega cluster needs to be consumed by clients from outside Kubernetes (or from another Kubernetes deployment), you can enable external access in two ways, depending on your environment constraints and requirements. Both ways will create one service for all Controllers, and one service for each Segment Store pod. Via LoadBalancer service type. Via NodePort service type. For more information, Please check Kubernetes documentation . Example of configuration for using LoadBalancer service types: ... spec : externalAccess : enabled : true type : LoadBalancer ... Clients will need to connect to the external Controller address and will automatically discover the external address of all Segment Store pods. Releases \u00b6 The latest Pravega releases can be found on the GitHub Release project page.","title":"Deployment on Kubernetes"},{"location":"deployment/kubernetes-install/#deploying-in-kubernetes","text":"","title":"Deploying in Kubernetes"},{"location":"deployment/kubernetes-install/#table-of-contents","text":"Requirements Pravega Operator Usage Installation of the Pravega Operator Deploy a sample Pravega Cluster Scale a Pravega Cluster Upgrade a Pravega Cluster Uninstall the Pravega Cluster Uninstall the Pravega Operator Configuration Use non-default service accounts Installing on a Custom Namespace with RBAC enabled Tier 2: Google Filestore Storage Tune Pravega Configurations Enable External Access Releases","title":"Table of Contents"},{"location":"deployment/kubernetes-install/#requirements","text":"Kubernetes 1.8+ An existing Apache Zookeeper 3.5 cluster. This can be easily deployed using our Zookeeper Operator . Pravega Operator manages Pravega clusters deployed to Kubernetes and automates tasks related to operating a Pravega cluster.","title":"Requirements"},{"location":"deployment/kubernetes-install/#usage","text":"","title":"Usage"},{"location":"deployment/kubernetes-install/#install-the-pravega-operator","text":"Note: If you are running on Google Kubernetes Engine (GKE), please check this first . Run the following command to install the PravegaCluster custom resource definition (CRD), create the pravega-operator service account, roles, bindings, and the deploy the Pravega Operator. $ kubectl create -f deploy Verify that the Pravega Operator is running. $ kubectl get deploy NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE pravega-operator 1 1 1 1 17s","title":"Install the Pravega Operator"},{"location":"deployment/kubernetes-install/#deploy-a-sample-pravega-cluster","text":"Pravega requires a long term storage provider known as Tier 2 storage. The following Tier 2 storage providers are supported: Filesystem (NFS) Google Filestore DellEMC ECS HDFS (must support Append operation) The following example uses an NFS volume provisioned by the NFS Server Provisioner helm chart to provide Tier 2 storage. $ helm install stable/nfs-server-provisioner Verify that the nfs storage class is now available. $ kubectl get storageclass NAME PROVISIONER AGE nfs cluster.local/elevated-leopard-nfs-server-provisioner 24s ... Note: This is ONLY intended as a demo and should NOT be used for production deployments. Once the NFS server provisioner is installed, you can create a PersistentVolumeClaim that will be used as Tier 2 for Pravega. Create a pvc.yaml file with the following content. kind : PersistentVolumeClaim apiVersion : v1 metadata : name : pravega-tier2 spec : storageClassName : \"nfs\" accessModes : - ReadWriteMany resources : requests : storage : 50Gi $ kubectl create -f pvc.yaml Use the following YAML template to install a small development Pravega Cluster (3 Bookies, 1 Controller, 3 Segment Stores). Create a pravega.yaml file with the following content. apiVersion : \"pravega.pravega.io/v1alpha1\" kind : \"PravegaCluster\" metadata : name : \"example\" spec : version : 0.4.0 zookeeperUri : [ ZOOKEEPER_HOST ] :2181 bookkeeper : replicas : 3 image : repository : pravega/bookkeeper autoRecovery : true pravega : controllerReplicas : 1 segmentStoreReplicas : 3 image : repository : pravega/pravega tier2 : filesystem : persistentVolumeClaim : claimName : pravega-tier2 where: [ZOOKEEPER_HOST] is the host or IP address of your Zookeeper deployment. Deploy the Pravega cluster. $ kubectl create -f pravega.yaml Verify that the cluster instances and its components are being created. $ kubectl get PravegaCluster NAME VERSION DESIRED MEMBERS READY MEMBERS AGE example 0.4.0 7 0 25s After a couple of minutes, all cluster members should become ready. $ kubectl get PravegaCluster NAME VERSION DESIRED MEMBERS READY MEMBERS AGE example 0.4.0 7 7 2m $ kubectl get all -l pravega_cluster=example NAME READY STATUS RESTARTS AGE pod/example-bookie-0 1/1 Running 0 2m pod/example-bookie-1 1/1 Running 0 2m pod/example-bookie-2 1/1 Running 0 2m pod/example-pravega-controller-64ff87fc49-kqp9k 1/1 Running 0 2m pod/example-pravega-segmentstore-0 1/1 Running 0 2m pod/example-pravega-segmentstore-1 1/1 Running 0 1m pod/example-pravega-segmentstore-2 1/1 Running 0 30s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/example-bookie-headless ClusterIP None <none> 3181/TCP 2m service/example-pravega-controller ClusterIP 10.23.244.3 <none> 10080/TCP,9090/TCP 2m service/example-pravega-segmentstore-headless ClusterIP None <none> 12345/TCP 2m NAME DESIRED CURRENT READY AGE replicaset.apps/example-pravega-controller-64ff87fc49 1 1 1 2m NAME DESIRED CURRENT AGE statefulset.apps/example-bookie 3 3 2m statefulset.apps/example-pravega-segmentstore 3 3 2m By default, a PravegaCluster instance is only accessible within the cluster through the Controller ClusterIP service. From within the Kubernetes cluster, a client can connect to Pravega at: tcp://<pravega-name>-pravega-controller.<namespace>:9090 And the REST management interface is available at: http://<pravega-name>-pravega-controller.<namespace>:10080/ Check this to enable external access to a Pravega cluster.","title":"Deploy a sample Pravega cluster"},{"location":"deployment/kubernetes-install/#scale-a-pravega-cluster","text":"You can scale Pravega components independently by modifying their corresponding field in the Pravega resource spec. You can either kubectl edit the cluster or kubectl patch it. If you edit it, update the number of replicas for BookKeeper, Controller, and/or Segment Store and save the updated spec. Example of patching the Pravega resource to scale the Segment Store instances to 4. kubectl patch PravegaCluster example --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/pravega/segmentStoreReplicas\", \"value\": 4}]'","title":"Scale a Pravega Cluster"},{"location":"deployment/kubernetes-install/#upgrade-a-pravega-cluster","text":"Check out the Upgrade Guide .","title":"Upgrade a Pravega Cluster"},{"location":"deployment/kubernetes-install/#uninstall-the-pravega-cluster","text":"$ kubectl delete -f pravega.yaml $ kubectl delete -f pvc.yaml","title":"Uninstall the Pravega cluster"},{"location":"deployment/kubernetes-install/#uninstall-the-pravega-cluster_1","text":"$ kubectl delete -f pravega.yaml $ kubectl delete -f pvc.yaml","title":"Uninstall the Pravega cluster"},{"location":"deployment/kubernetes-install/#uninstall-the-pravega-operator","text":"Note that the Pravega clusters managed by the Pravega operator will NOT be deleted even if the operator is uninstalled. To delete all clusters, delete all cluster CR objects before uninstalling the Pravega Operator. $ kubectl delete -f deploy","title":"Uninstall the Pravega Operator"},{"location":"deployment/kubernetes-install/#configuration","text":"","title":"Configuration"},{"location":"deployment/kubernetes-install/#use-non-default-service-accounts","text":"You can optionally configure non-default service accounts for the Bookkeeper, Pravega Controller, and Pravega Segment Store pods. For BookKeeper, set the serviceAccountName field under the bookkeeper block. ... spec: bookkeeper: serviceAccountName: bk-service-account ... For Pravega, set the controllerServiceAccountName and segmentStoreServiceAccountName fields under the pravega block. ... spec: pravega: controllerServiceAccountName: ctrl-service-account segmentStoreServiceAccountName: ss-service-account ... If external access is enabled in your Pravega cluster, Segment Store pods will require access to some Kubernetes API endpoints to obtain the external IP and port. Make sure that the service account you are using for the Segment Store has, at least, the following permissions. kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: pravega-components namespace: \"pravega-namespace\" rules: - apiGroups: [\"pravega.pravega.io\"] resources: [\"*\"] verbs: [\"get\"] - apiGroups: [\"\"] resources: [\"pods\", \"services\"] verbs: [\"get\"] --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: pravega-components rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\"] Replace the namespace with your own namespace.","title":"Use non-default service accounts"},{"location":"deployment/kubernetes-install/#installing-on-a-custom-namespace-with-rbac-enabled","text":"Create the namespace. $ kubectl create namespace pravega-io Update the namespace configured in the deploy/role_binding.yaml file. $ sed -i -e 's/namespace: default/namespace: pravega-io/g' deploy/role_binding.yaml Apply the changes. $ kubectl -n pravega-io apply -f deploy Note that the Pravega Operator only monitors the PravegaCluster resources which are created in the same namespace, pravega-io in this example. Therefore, before creating a PravegaCluster resource, make sure an Operator exists in that namespace. $ kubectl -n pravega-io create -f example/cr.yaml $ kubectl -n pravega-io get pravegaclusters NAME AGE pravega 28m $ kubectl -n pravega-io get pods -l pravega_cluster=pravega NAME READY STATUS RESTARTS AGE pravega-bookie-0 1/1 Running 0 29m pravega-bookie-1 1/1 Running 0 29m pravega-bookie-2 1/1 Running 0 29m pravega-pravega-controller-6c54fdcdf5-947nw 1/1 Running 0 29m pravega-pravega-segmentstore-0 1/1 Running 0 29m pravega-pravega-segmentstore-1 1/1 Running 0 29m pravega-pravega-segmentstore-2 1/1 Running 0 29m","title":"Installing on a Custom Namespace with RBAC enabled"},{"location":"deployment/kubernetes-install/#use-google-filestore-storage-as-tier-2","text":"Create a Google Filestore . Refer to https://cloud.google.com/filestore/docs/accessing-fileshares for more information Create a pv.yaml file with the PersistentVolume specification to provide Tier 2 storage. apiVersion : v1 kind : PersistentVolume metadata : name : pravega-volume spec : capacity : storage : 1T accessModes : - ReadWriteMany nfs : path : /[FILESHARE] server : [ IP_ADDRESS ] where: [FILESHARE] is the name of the fileshare on the Cloud Filestore instance (e.g. vol1 ) [IP_ADDRESS] is the IP address for the Cloud Filestore instance (e.g. 10.123.189.202 ) Deploy the PersistentVolume specification. $ kubectl create -f pv.yaml Create and deploy a PersistentVolumeClaim to consume the volume created. kind : PersistentVolumeClaim apiVersion : v1 metadata : name : pravega-tier2 spec : storageClassName : \"\" accessModes : - ReadWriteMany resources : requests : storage : 50Gi $ kubectl create -f pvc.yaml Use the same pravega.yaml above to deploy the Pravega cluster.","title":"Use Google Filestore Storage as Tier 2"},{"location":"deployment/kubernetes-install/#tune-pravega-configuration","text":"Pravega has many configuration options for setting up metrics, tuning, etc. The available options can be found here and are expressed through the pravega/options part of the resource specification. All values must be expressed as Strings. ... spec : pravega : options : metrics.enableStatistics : \"true\" metrics.statsdHost : \"telegraph.default\" metrics.statsdPort : \"8125\" ...","title":"Tune Pravega configuration"},{"location":"deployment/kubernetes-install/#enable-external-access","text":"By default, a Pravega cluster uses ClusterIP services which are only accessible from within Kubernetes. However, when creating the Pravega cluster resource, you can opt to enable external access. In Pravega, clients initiate the communication with the Pravega Controller, which is a stateless component frontended by a Kubernetes service that load-balances the requests to the backend pods. Then, clients discover the individual Segment Store instances to which they directly read and write data to. Clients need to be able to reach each and every Segment Store pod in the Pravega cluster. If your Pravega cluster needs to be consumed by clients from outside Kubernetes (or from another Kubernetes deployment), you can enable external access in two ways, depending on your environment constraints and requirements. Both ways will create one service for all Controllers, and one service for each Segment Store pod. Via LoadBalancer service type. Via NodePort service type. For more information, Please check Kubernetes documentation . Example of configuration for using LoadBalancer service types: ... spec : externalAccess : enabled : true type : LoadBalancer ... Clients will need to connect to the external Controller address and will automatically discover the external address of all Segment Store pods.","title":"Enable External Access"},{"location":"deployment/kubernetes-install/#releases","text":"The latest Pravega releases can be found on the GitHub Release project page.","title":"Releases"},{"location":"deployment/manual-install/","text":"Manual Installation \u00b6 This page describes the prerequisites and installation steps to deploy Pravega in a multi-node production environment. Prerequisites \u00b6 HDFS \u00b6 Setup a HDFS storage cluster running HDFS version 2.7+ . HDFS is used as Tier 2 Storage and must have sufficient capacity to store the contents of all the streams. The storage cluster is recommended to be run alongside Pravega on separate nodes. Java \u00b6 Install the latest Java 8 from java.oracle.com . Packages are available for all major operating systems. Zookeeper \u00b6 Pravega requires Zookeeper 3.5.4-beta . At least 3 Zookeeper nodes are recommended for a quorum. No special configuration is required for Zookeeper but it is recommended to use a dedicated cluster for Pravega. This specific version of Zookeeper can be downloaded from Apache at zookeeper-3.5.4-beta.tar.gz . For installing Zookeeper see the Getting Started Guide . Bookkeeper \u00b6 Pravega requires Bookkeeper 4.7.3 . At least 3 Bookkeeper servers are recommended for a quorum. This specific version of Bookkeeper can be downloaded from Apache at bookkeeper-server-4.7.3-bin.tar.gz . For installing Bookkeeper see the Getting Started Guide . Some specific Pravega instructions are shown below. All sets are assumed to be run from the bookkeeper-server-4.7.3 directory. Bookkeeper Configuration \u00b6 In the file conf/bk_server.conf , the following configuration options should be implemented: metadataServiceUri=zk://localhost:2181/pravega/bookkeeper/ledgers # Alternatively specify a different path to the storage for /bk journalDirectory=/bk/journal ledgerDirectories=/bk/ledgers indexDirectories=/bk/index Initializing Zookeeper paths \u00b6 The following paths need to be created in Zookeeper. Open the zookeeper-3.5.4-beta directory on the Zookeeper servers and run the following paths: bin/zkCli.sh -server $ZK_URL create /pravega bin/zkCli.sh -server $ZK_URL create /pravega/bookkeeper Replace <$ZK_URL> with the IP address of the Zookeeper nodes. Running Bookkeeper \u00b6 The bookie needs the following formatting before starting it: bin/bookkeeper shell metaformat -nonInteractive Start the bookie as mentioned below: bin/bookkeeper bookie Running Bookkeeper with encryption enabled \u00b6 Apache BookKeeper can be deployed with TLS enabled. Details can be found here . Installing Pravega \u00b6 For non-production systems, the containers can be used that are provided by the Docker installation to run non-production HDFS, Zookeeper or Bookkeeper. The following two key components of Pravega needs to be run: Controller : The Control plane for Pravega. Installation requires at least one Controller. (Two or more are recommended for HA). Segment Store : The Storage node for Pravega. Installation requires at least one Segment Store. Before we start, the latest Pravega release needs to be downloaded from the GitHub Releases page . Recommendations \u00b6 For a simple 3 node cluster, the following table depicts on layout of the services: Node 1 Node 2 Node 3 Zookeeper X X X Bookkeeper X X X Pravega Controller X X Pravega Segment Store X X X All Nodes \u00b6 On each node, extract the distribution package to the desired directory as follows: tar xfvz pravega-<version>.tgz cd pravega-<version> Installation of the Controller \u00b6 The controller can be run by using the following command. Replace <zk-ip> with the IP address of the Zookeeper nodes in the following command: ZK_URL=<zk-ip>:2181 bin/pravega-controller Instead specifying the <zk-ip> on every startup, we can edit the conf/controller.conf file and change the zk url as follows: zk { url = \"<zk-ip>:2181\" ... } Then run the controller with the following command: bin/pravega-controller Installation of the Segment Store \u00b6 In the file conf/config.properties , make the following changes as mentioned: Replace <zk-ip> , <controller-ip> and <hdfs-ip> with the IPs of the respective services. pravegaservice.zkURL=<zk-ip>:2181 bookkeeper.zkAddress=<zk-ip>:2181 autoScale.controllerUri=tcp://<controller-ip>:9090 # Settings required for HDFS hdfs.hdfsUrl=<hdfs-ip>:8020 After making the configuration changes, the segment store can be run using the following command: bin/pravega-segmentstore Running a Pravega Cluster with Security enabled \u00b6 Steps for securing a distributed mode cluster can be found here . For detailed information about security configuration parameters for Controller ) and Segment Store , see [this]((../security/pravega-security-configurations.md) document.","title":"Manual Install"},{"location":"deployment/manual-install/#manual-installation","text":"This page describes the prerequisites and installation steps to deploy Pravega in a multi-node production environment.","title":"Manual Installation"},{"location":"deployment/manual-install/#prerequisites","text":"","title":"Prerequisites"},{"location":"deployment/manual-install/#hdfs","text":"Setup a HDFS storage cluster running HDFS version 2.7+ . HDFS is used as Tier 2 Storage and must have sufficient capacity to store the contents of all the streams. The storage cluster is recommended to be run alongside Pravega on separate nodes.","title":"HDFS"},{"location":"deployment/manual-install/#java","text":"Install the latest Java 8 from java.oracle.com . Packages are available for all major operating systems.","title":"Java"},{"location":"deployment/manual-install/#zookeeper","text":"Pravega requires Zookeeper 3.5.4-beta . At least 3 Zookeeper nodes are recommended for a quorum. No special configuration is required for Zookeeper but it is recommended to use a dedicated cluster for Pravega. This specific version of Zookeeper can be downloaded from Apache at zookeeper-3.5.4-beta.tar.gz . For installing Zookeeper see the Getting Started Guide .","title":"Zookeeper"},{"location":"deployment/manual-install/#bookkeeper","text":"Pravega requires Bookkeeper 4.7.3 . At least 3 Bookkeeper servers are recommended for a quorum. This specific version of Bookkeeper can be downloaded from Apache at bookkeeper-server-4.7.3-bin.tar.gz . For installing Bookkeeper see the Getting Started Guide . Some specific Pravega instructions are shown below. All sets are assumed to be run from the bookkeeper-server-4.7.3 directory.","title":"Bookkeeper"},{"location":"deployment/manual-install/#bookkeeper-configuration","text":"In the file conf/bk_server.conf , the following configuration options should be implemented: metadataServiceUri=zk://localhost:2181/pravega/bookkeeper/ledgers # Alternatively specify a different path to the storage for /bk journalDirectory=/bk/journal ledgerDirectories=/bk/ledgers indexDirectories=/bk/index","title":"Bookkeeper Configuration"},{"location":"deployment/manual-install/#initializing-zookeeper-paths","text":"The following paths need to be created in Zookeeper. Open the zookeeper-3.5.4-beta directory on the Zookeeper servers and run the following paths: bin/zkCli.sh -server $ZK_URL create /pravega bin/zkCli.sh -server $ZK_URL create /pravega/bookkeeper Replace <$ZK_URL> with the IP address of the Zookeeper nodes.","title":"Initializing Zookeeper paths"},{"location":"deployment/manual-install/#running-bookkeeper","text":"The bookie needs the following formatting before starting it: bin/bookkeeper shell metaformat -nonInteractive Start the bookie as mentioned below: bin/bookkeeper bookie","title":"Running Bookkeeper"},{"location":"deployment/manual-install/#running-bookkeeper-with-encryption-enabled","text":"Apache BookKeeper can be deployed with TLS enabled. Details can be found here .","title":"Running Bookkeeper with encryption enabled"},{"location":"deployment/manual-install/#installing-pravega","text":"For non-production systems, the containers can be used that are provided by the Docker installation to run non-production HDFS, Zookeeper or Bookkeeper. The following two key components of Pravega needs to be run: Controller : The Control plane for Pravega. Installation requires at least one Controller. (Two or more are recommended for HA). Segment Store : The Storage node for Pravega. Installation requires at least one Segment Store. Before we start, the latest Pravega release needs to be downloaded from the GitHub Releases page .","title":"Installing Pravega"},{"location":"deployment/manual-install/#recommendations","text":"For a simple 3 node cluster, the following table depicts on layout of the services: Node 1 Node 2 Node 3 Zookeeper X X X Bookkeeper X X X Pravega Controller X X Pravega Segment Store X X X","title":"Recommendations"},{"location":"deployment/manual-install/#all-nodes","text":"On each node, extract the distribution package to the desired directory as follows: tar xfvz pravega-<version>.tgz cd pravega-<version>","title":"All Nodes"},{"location":"deployment/manual-install/#installation-of-the-controller","text":"The controller can be run by using the following command. Replace <zk-ip> with the IP address of the Zookeeper nodes in the following command: ZK_URL=<zk-ip>:2181 bin/pravega-controller Instead specifying the <zk-ip> on every startup, we can edit the conf/controller.conf file and change the zk url as follows: zk { url = \"<zk-ip>:2181\" ... } Then run the controller with the following command: bin/pravega-controller","title":"Installation of the Controller"},{"location":"deployment/manual-install/#installation-of-the-segment-store","text":"In the file conf/config.properties , make the following changes as mentioned: Replace <zk-ip> , <controller-ip> and <hdfs-ip> with the IPs of the respective services. pravegaservice.zkURL=<zk-ip>:2181 bookkeeper.zkAddress=<zk-ip>:2181 autoScale.controllerUri=tcp://<controller-ip>:9090 # Settings required for HDFS hdfs.hdfsUrl=<hdfs-ip>:8020 After making the configuration changes, the segment store can be run using the following command: bin/pravega-segmentstore","title":"Installation of the Segment Store"},{"location":"deployment/manual-install/#running-a-pravega-cluster-with-security-enabled","text":"Steps for securing a distributed mode cluster can be found here . For detailed information about security configuration parameters for Controller ) and Segment Store , see [this]((../security/pravega-security-configurations.md) document.","title":"Running a Pravega Cluster with Security enabled"},{"location":"deployment/run-local/","text":"Running Pravega in Local Machine \u00b6 As an alternative to running Pravega on a cluster of machines, you may run Pravega on a local/single machine. Running Pravega locally on a single host allows you to get started with Pravega quickly. Running Pravega locally is especially suitable for development and testing purposes. You may run Pravega on local machine using either of these two options: Standalone mode deployment: In this option, Pravega server runs on a single process. Distributed mode Docker Compose deployment: In this option, Pravega components run on separate processes within the same host. These options are explained in below subsections. Standalone Mode \u00b6 In standalone mode, the Pravega server is accessible from clients through the localhost interface only. Controller REST APIs, however, are accessible from remote hosts/machines. Security is off by default in Pravega. Please see this document to find how to enable security in standalone mode. You can launch a standalone mode server using the following options: From source code From installation package From Docker image From Source Code \u00b6 Checkout the source code: $ git clone https://github.com/pravega/pravega.git $ cd pravega Build the Pravega standalone mode distribution: ./gradlew startStandalone From Installation Package \u00b6 Download the Pravega release from the GitHub Releases . $ tar xfvz pravega-<version>.tgz Download and extract either tarball or zip files. Follow the instructions provided for the tar files (same can be applied for zip file) to launch all the components of Pravega on your local machine. Run Pravega Standalone: $ pravega-<version>/bin/pravega-standalone From Docker Image \u00b6 The below command will download and run Pravega from the container image on docker hub. Note: We must replace the <ip> with the IP of our machine to connect to Pravega from our local machine. Optionally we can replace latest with the version of Pravega as per the requirement. docker run -it -e HOST_IP=<ip> -p 9090:9090 -p 12345:12345 pravega/pravega:latest standalone Docker Compose (Distributed Mode) \u00b6 Unlike other options for running locally, the Docker Compose option runs a full deployment of Pravega in distributed mode. It contains containers for running Zookeeper, Bookkeeper and HDFS. Hence Pravega operates as if it would in production. This is the easiest way to get started with the standalone option but requires additional resources. Ensure that your host machine meets the following prerequisites: It has Docker 1.12 or later installed. It has Docker Compose installed. Download the docker-compose.yml file from Pravega GitHub repository. $ wget https://raw.githubusercontent.com/pravega/pravega/master/docker/compose/docker-compose.yml Alternatively, clone the Pravega repository to fetch the code. $ git clone https://github.com/pravega/pravega.git Navigate to the directory containing Docker Compose configuration .yml files. $ cd /path/to/pravega/docker/compose Add HOST_IP as an environment variable with the value as the IP address of the host. $ export HOST_IP=<HOST_IP> Run the following command to start a deployment comprising of multiple Docker containers, as specified in the docker-compose.yml file. $ docker-compose up -d To use one of the other files in the directory, use the -f option to specify the file. $ docker-compose up -d -f docker-compose-nfs.yml Verify that the deployment is up and running. $ docker-compose ps Clients can then connect to the Controller at <HOST_IP>:9090 . To access the Pravega Controller REST API, invoke it using a URL of the form http://<HOST_IP>:10080/v1/scopes (where /scopes is one of the many endpoints that the API supports).","title":"Running Locally"},{"location":"deployment/run-local/#running-pravega-in-local-machine","text":"As an alternative to running Pravega on a cluster of machines, you may run Pravega on a local/single machine. Running Pravega locally on a single host allows you to get started with Pravega quickly. Running Pravega locally is especially suitable for development and testing purposes. You may run Pravega on local machine using either of these two options: Standalone mode deployment: In this option, Pravega server runs on a single process. Distributed mode Docker Compose deployment: In this option, Pravega components run on separate processes within the same host. These options are explained in below subsections.","title":"Running Pravega in Local Machine"},{"location":"deployment/run-local/#standalone-mode","text":"In standalone mode, the Pravega server is accessible from clients through the localhost interface only. Controller REST APIs, however, are accessible from remote hosts/machines. Security is off by default in Pravega. Please see this document to find how to enable security in standalone mode. You can launch a standalone mode server using the following options: From source code From installation package From Docker image","title":"Standalone Mode"},{"location":"deployment/run-local/#from-source-code","text":"Checkout the source code: $ git clone https://github.com/pravega/pravega.git $ cd pravega Build the Pravega standalone mode distribution: ./gradlew startStandalone","title":"From Source Code"},{"location":"deployment/run-local/#from-installation-package","text":"Download the Pravega release from the GitHub Releases . $ tar xfvz pravega-<version>.tgz Download and extract either tarball or zip files. Follow the instructions provided for the tar files (same can be applied for zip file) to launch all the components of Pravega on your local machine. Run Pravega Standalone: $ pravega-<version>/bin/pravega-standalone","title":"From Installation Package"},{"location":"deployment/run-local/#from-docker-image","text":"The below command will download and run Pravega from the container image on docker hub. Note: We must replace the <ip> with the IP of our machine to connect to Pravega from our local machine. Optionally we can replace latest with the version of Pravega as per the requirement. docker run -it -e HOST_IP=<ip> -p 9090:9090 -p 12345:12345 pravega/pravega:latest standalone","title":"From Docker Image"},{"location":"deployment/run-local/#docker-compose-distributed-mode","text":"Unlike other options for running locally, the Docker Compose option runs a full deployment of Pravega in distributed mode. It contains containers for running Zookeeper, Bookkeeper and HDFS. Hence Pravega operates as if it would in production. This is the easiest way to get started with the standalone option but requires additional resources. Ensure that your host machine meets the following prerequisites: It has Docker 1.12 or later installed. It has Docker Compose installed. Download the docker-compose.yml file from Pravega GitHub repository. $ wget https://raw.githubusercontent.com/pravega/pravega/master/docker/compose/docker-compose.yml Alternatively, clone the Pravega repository to fetch the code. $ git clone https://github.com/pravega/pravega.git Navigate to the directory containing Docker Compose configuration .yml files. $ cd /path/to/pravega/docker/compose Add HOST_IP as an environment variable with the value as the IP address of the host. $ export HOST_IP=<HOST_IP> Run the following command to start a deployment comprising of multiple Docker containers, as specified in the docker-compose.yml file. $ docker-compose up -d To use one of the other files in the directory, use the -f option to specify the file. $ docker-compose up -d -f docker-compose-nfs.yml Verify that the deployment is up and running. $ docker-compose ps Clients can then connect to the Controller at <HOST_IP>:9090 . To access the Pravega Controller REST API, invoke it using a URL of the form http://<HOST_IP>:10080/v1/scopes (where /scopes is one of the many endpoints that the API supports).","title":"Docker Compose (Distributed Mode)"},{"location":"rest/restapis/","text":"Pravega Controller APIs \u00b6 Overview \u00b6 List of admin REST APIs for the pravega controller service. Version information \u00b6 Version : 0.0.1 License information \u00b6 License : Apache 2.0 License URL : http://www.apache.org/licenses/LICENSE-2.0 Terms of service : null URI scheme \u00b6 BasePath : /v1 Schemes : HTTP Tags \u00b6 ReaderGroups : Reader group related APIs Scopes : Scope related APIs Streams : Stream related APIs Paths \u00b6 POST /scopes \u00b6 Description \u00b6 Create a new scope Parameters \u00b6 Type Name Description Schema Body CreateScopeRequest required The scope configuration CreateScopeRequest CreateScopeRequest Name Description Schema scopeName optional Example : \"string\" string Responses \u00b6 HTTP Code Description Schema 201 Successfully created the scope ScopeProperty 409 Scope with the given name already exists No Content 500 Internal server error while creating a scope No Content Consumes \u00b6 application/json Produces \u00b6 application/json Tags \u00b6 Scopes Example HTTP request \u00b6 Request path \u00b6 /scopes Request body \u00b6 { \"scopeName\" : \"string\" } Example HTTP response \u00b6 Response 201 \u00b6 { \"scopeName\" : \"string\" } GET /scopes \u00b6 Description \u00b6 List all available scopes in pravega Responses \u00b6 HTTP Code Description Schema 200 List of currently available scopes ScopesList 500 Internal server error while fetching list of scopes No Content Produces \u00b6 application/json Tags \u00b6 Scopes Example HTTP request \u00b6 Request path \u00b6 /scopes Example HTTP response \u00b6 Response 200 \u00b6 { \"scopes\" : [ { \"scopeName\" : \"string\" } ] } GET /scopes/{scopeName} \u00b6 Description \u00b6 Retrieve details of an existing scope Parameters \u00b6 Type Name Description Schema Path scopeName required Scope name string Responses \u00b6 HTTP Code Description Schema 200 Successfully retrieved the scope details ScopeProperty 404 Scope with the given name not found No Content 500 Internal server error while fetching scope details No Content Produces \u00b6 application/json Tags \u00b6 Scopes Example HTTP request \u00b6 Request path \u00b6 /scopes/string Example HTTP response \u00b6 Response 200 \u00b6 { \"scopeName\" : \"string\" } DELETE /scopes/{scopeName} \u00b6 Description \u00b6 Delete a scope Parameters \u00b6 Type Name Description Schema Path scopeName required Scope name string Responses \u00b6 HTTP Code Description Schema 204 Successfully deleted the scope No Content 404 Scope not found No Content 412 Cannot delete scope since it has non-empty list of streams No Content 500 Internal server error while deleting a scope No Content Tags \u00b6 Scopes Example HTTP request \u00b6 Request path \u00b6 /scopes/string GET /scopes/{scopeName}/readergroups \u00b6 Description \u00b6 List reader groups within the given scope Parameters \u00b6 Type Name Description Schema Path scopeName required Scope name string Responses \u00b6 HTTP Code Description Schema 200 List of all reader groups configured for the given scope ReaderGroupsList 404 Scope not found No Content 500 Internal server error while fetching the list of reader groups for the given scope No Content Produces \u00b6 application/json Tags \u00b6 ReaderGroups Example HTTP request \u00b6 Request path \u00b6 /scopes/string/readergroups Example HTTP response \u00b6 Response 200 \u00b6 { \"readerGroups\" : [ \"object\" ] } GET /scopes/{scopeName}/readergroups/{readerGroupName} \u00b6 Description \u00b6 Fetch the properties of an existing reader group Parameters \u00b6 Type Name Description Schema Path readerGroupName required Reader group name string Path scopeName required Scope name string Responses \u00b6 HTTP Code Description Schema 200 Found reader group properties ReaderGroupProperty 404 Scope or reader group with given name not found No Content 500 Internal server error while fetching reader group details No Content Produces \u00b6 application/json Tags \u00b6 ReaderGroups Example HTTP request \u00b6 Request path \u00b6 /scopes/string/readergroups/string Example HTTP response \u00b6 Response 200 \u00b6 { \"scopeName\" : \"string\" , \"readerGroupName\" : \"string\" , \"streamList\" : [ \"string\" ], \"onlineReaderIds\" : [ \"string\" ] } POST /scopes/{scopeName}/streams \u00b6 Description \u00b6 Create a new stream Parameters \u00b6 Type Name Description Schema Path scopeName required Scope name string Body CreateStreamRequest required The stream configuration CreateStreamRequest CreateStreamRequest Name Description Schema retentionPolicy optional Example : \"[retentionconfig](#retentionconfig)\" RetentionConfig scalingPolicy optional Example : \"[scalingconfig](#scalingconfig)\" ScalingConfig streamName optional Example : \"string\" string Responses \u00b6 HTTP Code Description Schema 201 Successfully created the stream with the given configuration StreamProperty 404 Scope not found No Content 409 Stream with given name already exists No Content 500 Internal server error while creating a stream No Content Consumes \u00b6 application/json Produces \u00b6 application/json Tags \u00b6 Streams Example HTTP request \u00b6 Request path \u00b6 /scopes/string/streams Request body \u00b6 { \"streamName\" : \"string\" , \"scalingPolicy\" : { \"type\" : \"string\" , \"targetRate\" : 0 , \"scaleFactor\" : 0 , \"minSegments\" : 0 }, \"retentionPolicy\" : { \"type\" : \"string\" , \"value\" : 0 } } Example HTTP response \u00b6 Response 201 \u00b6 { \"scopeName\" : \"string\" , \"streamName\" : \"string\" , \"scalingPolicy\" : { \"type\" : \"string\" , \"targetRate\" : 0 , \"scaleFactor\" : 0 , \"minSegments\" : 0 }, \"retentionPolicy\" : { \"type\" : \"string\" , \"value\" : 0 } } GET /scopes/{scopeName}/streams \u00b6 Description \u00b6 List streams within the given scope Parameters \u00b6 Type Name Description Schema Path scopeName required Scope name string Query showInternalStreams optional Optional flag whether to display system created streams. If not specified only user created streams will be returned string Responses \u00b6 HTTP Code Description Schema 200 List of all streams configured for the given scope StreamsList 404 Scope not found No Content 500 Internal server error while fetching the list of streams for the given scope No Content Produces \u00b6 application/json Tags \u00b6 Streams Example HTTP request \u00b6 Request path \u00b6 /scopes/string/streams Example HTTP response \u00b6 Response 200 \u00b6 { \"streams\" : [ { \"scopeName\" : \"string\" , \"streamName\" : \"string\" , \"scalingPolicy\" : { \"type\" : \"string\" , \"targetRate\" : 0 , \"scaleFactor\" : 0 , \"minSegments\" : 0 }, \"retentionPolicy\" : { \"type\" : \"string\" , \"value\" : 0 } } ] } GET /scopes/{scopeName}/streams/{streamName} \u00b6 Description \u00b6 Fetch the properties of an existing stream Parameters \u00b6 Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string Responses \u00b6 HTTP Code Description Schema 200 Found stream properties StreamProperty 404 Scope or stream with given name not found No Content 500 Internal server error while fetching stream details No Content Produces \u00b6 application/json Tags \u00b6 Streams Example HTTP request \u00b6 Request path \u00b6 /scopes/string/streams/string Example HTTP response \u00b6 Response 200 \u00b6 { \"scopeName\" : \"string\" , \"streamName\" : \"string\" , \"scalingPolicy\" : { \"type\" : \"string\" , \"targetRate\" : 0 , \"scaleFactor\" : 0 , \"minSegments\" : 0 }, \"retentionPolicy\" : { \"type\" : \"string\" , \"value\" : 0 } } PUT /scopes/{scopeName}/streams/{streamName} \u00b6 Description \u00b6 Update configuration of an existing stream Parameters \u00b6 Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string Body UpdateStreamRequest required The new stream configuration UpdateStreamRequest UpdateStreamRequest Name Description Schema retentionPolicy optional Example : \"[retentionconfig](#retentionconfig)\" RetentionConfig scalingPolicy optional Example : \"[scalingconfig](#scalingconfig)\" ScalingConfig Responses \u00b6 HTTP Code Description Schema 200 Successfully updated the stream configuration StreamProperty 404 Scope or stream with given name not found No Content 500 Internal server error while updating the stream No Content Consumes \u00b6 application/json Produces \u00b6 application/json Tags \u00b6 Streams Example HTTP request \u00b6 Request path \u00b6 /scopes/string/streams/string Request body \u00b6 { \"scalingPolicy\" : { \"type\" : \"string\" , \"targetRate\" : 0 , \"scaleFactor\" : 0 , \"minSegments\" : 0 }, \"retentionPolicy\" : { \"type\" : \"string\" , \"value\" : 0 } } Example HTTP response \u00b6 Response 200 \u00b6 { \"scopeName\" : \"string\" , \"streamName\" : \"string\" , \"scalingPolicy\" : { \"type\" : \"string\" , \"targetRate\" : 0 , \"scaleFactor\" : 0 , \"minSegments\" : 0 }, \"retentionPolicy\" : { \"type\" : \"string\" , \"value\" : 0 } } DELETE /scopes/{scopeName}/streams/{streamName} \u00b6 Description \u00b6 Delete a stream Parameters \u00b6 Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string Responses \u00b6 HTTP Code Description Schema 204 Successfully deleted the stream No Content 404 Stream not found No Content 412 Cannot delete stream since it is not sealed No Content 500 Internal server error while deleting the stream No Content Tags \u00b6 Streams Example HTTP request \u00b6 Request path \u00b6 /scopes/string/streams/string GET /scopes/{scopeName}/streams/{streamName}/scaling-events \u00b6 Description \u00b6 Get scaling events for a given datetime period. Parameters \u00b6 Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string Query from required Parameter to display scaling events from that particular datetime. Input should be milliseconds from Jan 1 1970. integer (int64) Query to required Parameter to display scaling events to that particular datetime. Input should be milliseconds from Jan 1 1970. integer (int64) Responses \u00b6 HTTP Code Description Schema 200 Successfully fetched list of scaling events. ScalingEventList 404 Scope/Stream not found. No Content 500 Internal Server error while fetching scaling events. No Content Produces \u00b6 application/json Tags \u00b6 Streams Example HTTP request \u00b6 Request path \u00b6 /scopes/string/streams/string/scaling-events?from=0&to=0 Example HTTP response \u00b6 Response 200 \u00b6 { \"scalingEvents\" : [ { \"timestamp\" : 0 , \"segmentList\" : [ { \"number\" : 0 , \"startTime\" : 0 , \"keyStart\" : 0 , \"keyEnd\" : 0 } ], \"splits\" : 0 , \"merges\" : 0 } ] } PUT /scopes/{scopeName}/streams/{streamName}/state \u00b6 Description \u00b6 Updates the current state of the stream Parameters \u00b6 Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string Body UpdateStreamStateRequest required The state info to be updated StreamState Responses \u00b6 HTTP Code Description Schema 200 Successfully updated the stream state StreamState 404 Scope or stream with given name not found No Content 500 Internal server error while updating the stream state No Content Consumes \u00b6 application/json Produces \u00b6 application/json Tags \u00b6 Streams Example HTTP request \u00b6 Request path \u00b6 /scopes/string/streams/string/state Request body \u00b6 { \"streamState\" : \"string\" } Example HTTP response \u00b6 Response 200 \u00b6 { \"streamState\" : \"string\" } Definitions \u00b6 ReaderGroupProperty \u00b6 Name Description Schema onlineReaderIds optional Example : [ \"string\" ] < string > array readerGroupName optional Example : \"string\" string scopeName optional Example : \"string\" string streamList optional Example : [ \"string\" ] < string > array ReaderGroupsList \u00b6 Name Description Schema readerGroups optional Example : [ \"object\" ] < readerGroups > array readerGroups Name Description Schema readerGroupName optional Example : \"string\" string RetentionConfig \u00b6 Name Description Schema type optional Example : \"string\" enum (LIMITED_DAYS, LIMITED_SIZE_MB) value optional Example : 0 integer (int64) ScaleMetadata \u00b6 Name Description Schema merges optional Example : 0 integer (int64) segmentList optional Example : [ \"[segment](#segment)\" ] < Segment > array splits optional Example : 0 integer (int64) timestamp optional Example : 0 integer (int64) ScalingConfig \u00b6 Name Description Schema minSegments optional Example : 0 integer (int32) scaleFactor optional Example : 0 integer (int32) targetRate optional Example : 0 integer (int32) type optional Example : \"string\" enum (FIXED_NUM_SEGMENTS, BY_RATE_IN_KBYTES_PER_SEC, BY_RATE_IN_EVENTS_PER_SEC) ScalingEventList \u00b6 Name Description Schema scalingEvents optional Example : [ \"[scalemetadata](#scalemetadata)\" ] < ScaleMetadata > array ScopeProperty \u00b6 Name Description Schema scopeName optional Example : \"string\" string ScopesList \u00b6 Name Description Schema scopes optional Example : [ \"[scopeproperty](#scopeproperty)\" ] < ScopeProperty > array Segment \u00b6 Name Description Schema keyEnd optional Example : 0 integer (double) keyStart optional Example : 0 integer (double) number optional Example : 0 integer (int32) startTime optional Example : 0 integer (int64) StreamProperty \u00b6 Name Description Schema retentionPolicy optional Example : \"[retentionconfig](#retentionconfig)\" RetentionConfig scalingPolicy optional Example : \"[scalingconfig](#scalingconfig)\" ScalingConfig scopeName optional Example : \"string\" string streamName optional Example : \"string\" string StreamState \u00b6 Name Description Schema streamState optional Example : \"string\" enum (SEALED) StreamsList \u00b6 Name Description Schema streams optional Example : [ \"[streamproperty](#streamproperty)\" ] < StreamProperty > array","title":"REST API - Controller"},{"location":"rest/restapis/#pravega-controller-apis","text":"","title":"Pravega Controller APIs"},{"location":"rest/restapis/#overview","text":"List of admin REST APIs for the pravega controller service.","title":"Overview"},{"location":"rest/restapis/#version-information","text":"Version : 0.0.1","title":"Version information"},{"location":"rest/restapis/#license-information","text":"License : Apache 2.0 License URL : http://www.apache.org/licenses/LICENSE-2.0 Terms of service : null","title":"License information"},{"location":"rest/restapis/#uri-scheme","text":"BasePath : /v1 Schemes : HTTP","title":"URI scheme"},{"location":"rest/restapis/#tags","text":"ReaderGroups : Reader group related APIs Scopes : Scope related APIs Streams : Stream related APIs","title":"Tags"},{"location":"rest/restapis/#paths","text":"","title":"Paths"},{"location":"rest/restapis/#post-scopes","text":"","title":"POST /scopes"},{"location":"rest/restapis/#description","text":"Create a new scope","title":"Description"},{"location":"rest/restapis/#parameters","text":"Type Name Description Schema Body CreateScopeRequest required The scope configuration CreateScopeRequest CreateScopeRequest Name Description Schema scopeName optional Example : \"string\" string","title":"Parameters"},{"location":"rest/restapis/#responses","text":"HTTP Code Description Schema 201 Successfully created the scope ScopeProperty 409 Scope with the given name already exists No Content 500 Internal server error while creating a scope No Content","title":"Responses"},{"location":"rest/restapis/#consumes","text":"application/json","title":"Consumes"},{"location":"rest/restapis/#produces","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_1","text":"Scopes","title":"Tags"},{"location":"rest/restapis/#example-http-request","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path","text":"/scopes","title":"Request path"},{"location":"rest/restapis/#request-body","text":"{ \"scopeName\" : \"string\" }","title":"Request body"},{"location":"rest/restapis/#example-http-response","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-201","text":"{ \"scopeName\" : \"string\" }","title":"Response 201"},{"location":"rest/restapis/#get-scopes","text":"","title":"GET /scopes"},{"location":"rest/restapis/#description_1","text":"List all available scopes in pravega","title":"Description"},{"location":"rest/restapis/#responses_1","text":"HTTP Code Description Schema 200 List of currently available scopes ScopesList 500 Internal server error while fetching list of scopes No Content","title":"Responses"},{"location":"rest/restapis/#produces_1","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_2","text":"Scopes","title":"Tags"},{"location":"rest/restapis/#example-http-request_1","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_1","text":"/scopes","title":"Request path"},{"location":"rest/restapis/#example-http-response_1","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-200","text":"{ \"scopes\" : [ { \"scopeName\" : \"string\" } ] }","title":"Response 200"},{"location":"rest/restapis/#get-scopesscopename","text":"","title":"GET /scopes/{scopeName}"},{"location":"rest/restapis/#description_2","text":"Retrieve details of an existing scope","title":"Description"},{"location":"rest/restapis/#parameters_1","text":"Type Name Description Schema Path scopeName required Scope name string","title":"Parameters"},{"location":"rest/restapis/#responses_2","text":"HTTP Code Description Schema 200 Successfully retrieved the scope details ScopeProperty 404 Scope with the given name not found No Content 500 Internal server error while fetching scope details No Content","title":"Responses"},{"location":"rest/restapis/#produces_2","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_3","text":"Scopes","title":"Tags"},{"location":"rest/restapis/#example-http-request_2","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_2","text":"/scopes/string","title":"Request path"},{"location":"rest/restapis/#example-http-response_2","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-200_1","text":"{ \"scopeName\" : \"string\" }","title":"Response 200"},{"location":"rest/restapis/#delete-scopesscopename","text":"","title":"DELETE /scopes/{scopeName}"},{"location":"rest/restapis/#description_3","text":"Delete a scope","title":"Description"},{"location":"rest/restapis/#parameters_2","text":"Type Name Description Schema Path scopeName required Scope name string","title":"Parameters"},{"location":"rest/restapis/#responses_3","text":"HTTP Code Description Schema 204 Successfully deleted the scope No Content 404 Scope not found No Content 412 Cannot delete scope since it has non-empty list of streams No Content 500 Internal server error while deleting a scope No Content","title":"Responses"},{"location":"rest/restapis/#tags_4","text":"Scopes","title":"Tags"},{"location":"rest/restapis/#example-http-request_3","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_3","text":"/scopes/string","title":"Request path"},{"location":"rest/restapis/#get-scopesscopenamereadergroups","text":"","title":"GET /scopes/{scopeName}/readergroups"},{"location":"rest/restapis/#description_4","text":"List reader groups within the given scope","title":"Description"},{"location":"rest/restapis/#parameters_3","text":"Type Name Description Schema Path scopeName required Scope name string","title":"Parameters"},{"location":"rest/restapis/#responses_4","text":"HTTP Code Description Schema 200 List of all reader groups configured for the given scope ReaderGroupsList 404 Scope not found No Content 500 Internal server error while fetching the list of reader groups for the given scope No Content","title":"Responses"},{"location":"rest/restapis/#produces_3","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_5","text":"ReaderGroups","title":"Tags"},{"location":"rest/restapis/#example-http-request_4","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_4","text":"/scopes/string/readergroups","title":"Request path"},{"location":"rest/restapis/#example-http-response_3","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-200_2","text":"{ \"readerGroups\" : [ \"object\" ] }","title":"Response 200"},{"location":"rest/restapis/#get-scopesscopenamereadergroupsreadergroupname","text":"","title":"GET /scopes/{scopeName}/readergroups/{readerGroupName}"},{"location":"rest/restapis/#description_5","text":"Fetch the properties of an existing reader group","title":"Description"},{"location":"rest/restapis/#parameters_4","text":"Type Name Description Schema Path readerGroupName required Reader group name string Path scopeName required Scope name string","title":"Parameters"},{"location":"rest/restapis/#responses_5","text":"HTTP Code Description Schema 200 Found reader group properties ReaderGroupProperty 404 Scope or reader group with given name not found No Content 500 Internal server error while fetching reader group details No Content","title":"Responses"},{"location":"rest/restapis/#produces_4","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_6","text":"ReaderGroups","title":"Tags"},{"location":"rest/restapis/#example-http-request_5","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_5","text":"/scopes/string/readergroups/string","title":"Request path"},{"location":"rest/restapis/#example-http-response_4","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-200_3","text":"{ \"scopeName\" : \"string\" , \"readerGroupName\" : \"string\" , \"streamList\" : [ \"string\" ], \"onlineReaderIds\" : [ \"string\" ] }","title":"Response 200"},{"location":"rest/restapis/#post-scopesscopenamestreams","text":"","title":"POST /scopes/{scopeName}/streams"},{"location":"rest/restapis/#description_6","text":"Create a new stream","title":"Description"},{"location":"rest/restapis/#parameters_5","text":"Type Name Description Schema Path scopeName required Scope name string Body CreateStreamRequest required The stream configuration CreateStreamRequest CreateStreamRequest Name Description Schema retentionPolicy optional Example : \"[retentionconfig](#retentionconfig)\" RetentionConfig scalingPolicy optional Example : \"[scalingconfig](#scalingconfig)\" ScalingConfig streamName optional Example : \"string\" string","title":"Parameters"},{"location":"rest/restapis/#responses_6","text":"HTTP Code Description Schema 201 Successfully created the stream with the given configuration StreamProperty 404 Scope not found No Content 409 Stream with given name already exists No Content 500 Internal server error while creating a stream No Content","title":"Responses"},{"location":"rest/restapis/#consumes_1","text":"application/json","title":"Consumes"},{"location":"rest/restapis/#produces_5","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_7","text":"Streams","title":"Tags"},{"location":"rest/restapis/#example-http-request_6","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_6","text":"/scopes/string/streams","title":"Request path"},{"location":"rest/restapis/#request-body_1","text":"{ \"streamName\" : \"string\" , \"scalingPolicy\" : { \"type\" : \"string\" , \"targetRate\" : 0 , \"scaleFactor\" : 0 , \"minSegments\" : 0 }, \"retentionPolicy\" : { \"type\" : \"string\" , \"value\" : 0 } }","title":"Request body"},{"location":"rest/restapis/#example-http-response_5","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-201_1","text":"{ \"scopeName\" : \"string\" , \"streamName\" : \"string\" , \"scalingPolicy\" : { \"type\" : \"string\" , \"targetRate\" : 0 , \"scaleFactor\" : 0 , \"minSegments\" : 0 }, \"retentionPolicy\" : { \"type\" : \"string\" , \"value\" : 0 } }","title":"Response 201"},{"location":"rest/restapis/#get-scopesscopenamestreams","text":"","title":"GET /scopes/{scopeName}/streams"},{"location":"rest/restapis/#description_7","text":"List streams within the given scope","title":"Description"},{"location":"rest/restapis/#parameters_6","text":"Type Name Description Schema Path scopeName required Scope name string Query showInternalStreams optional Optional flag whether to display system created streams. If not specified only user created streams will be returned string","title":"Parameters"},{"location":"rest/restapis/#responses_7","text":"HTTP Code Description Schema 200 List of all streams configured for the given scope StreamsList 404 Scope not found No Content 500 Internal server error while fetching the list of streams for the given scope No Content","title":"Responses"},{"location":"rest/restapis/#produces_6","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_8","text":"Streams","title":"Tags"},{"location":"rest/restapis/#example-http-request_7","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_7","text":"/scopes/string/streams","title":"Request path"},{"location":"rest/restapis/#example-http-response_6","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-200_4","text":"{ \"streams\" : [ { \"scopeName\" : \"string\" , \"streamName\" : \"string\" , \"scalingPolicy\" : { \"type\" : \"string\" , \"targetRate\" : 0 , \"scaleFactor\" : 0 , \"minSegments\" : 0 }, \"retentionPolicy\" : { \"type\" : \"string\" , \"value\" : 0 } } ] }","title":"Response 200"},{"location":"rest/restapis/#get-scopesscopenamestreamsstreamname","text":"","title":"GET /scopes/{scopeName}/streams/{streamName}"},{"location":"rest/restapis/#description_8","text":"Fetch the properties of an existing stream","title":"Description"},{"location":"rest/restapis/#parameters_7","text":"Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string","title":"Parameters"},{"location":"rest/restapis/#responses_8","text":"HTTP Code Description Schema 200 Found stream properties StreamProperty 404 Scope or stream with given name not found No Content 500 Internal server error while fetching stream details No Content","title":"Responses"},{"location":"rest/restapis/#produces_7","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_9","text":"Streams","title":"Tags"},{"location":"rest/restapis/#example-http-request_8","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_8","text":"/scopes/string/streams/string","title":"Request path"},{"location":"rest/restapis/#example-http-response_7","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-200_5","text":"{ \"scopeName\" : \"string\" , \"streamName\" : \"string\" , \"scalingPolicy\" : { \"type\" : \"string\" , \"targetRate\" : 0 , \"scaleFactor\" : 0 , \"minSegments\" : 0 }, \"retentionPolicy\" : { \"type\" : \"string\" , \"value\" : 0 } }","title":"Response 200"},{"location":"rest/restapis/#put-scopesscopenamestreamsstreamname","text":"","title":"PUT /scopes/{scopeName}/streams/{streamName}"},{"location":"rest/restapis/#description_9","text":"Update configuration of an existing stream","title":"Description"},{"location":"rest/restapis/#parameters_8","text":"Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string Body UpdateStreamRequest required The new stream configuration UpdateStreamRequest UpdateStreamRequest Name Description Schema retentionPolicy optional Example : \"[retentionconfig](#retentionconfig)\" RetentionConfig scalingPolicy optional Example : \"[scalingconfig](#scalingconfig)\" ScalingConfig","title":"Parameters"},{"location":"rest/restapis/#responses_9","text":"HTTP Code Description Schema 200 Successfully updated the stream configuration StreamProperty 404 Scope or stream with given name not found No Content 500 Internal server error while updating the stream No Content","title":"Responses"},{"location":"rest/restapis/#consumes_2","text":"application/json","title":"Consumes"},{"location":"rest/restapis/#produces_8","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_10","text":"Streams","title":"Tags"},{"location":"rest/restapis/#example-http-request_9","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_9","text":"/scopes/string/streams/string","title":"Request path"},{"location":"rest/restapis/#request-body_2","text":"{ \"scalingPolicy\" : { \"type\" : \"string\" , \"targetRate\" : 0 , \"scaleFactor\" : 0 , \"minSegments\" : 0 }, \"retentionPolicy\" : { \"type\" : \"string\" , \"value\" : 0 } }","title":"Request body"},{"location":"rest/restapis/#example-http-response_8","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-200_6","text":"{ \"scopeName\" : \"string\" , \"streamName\" : \"string\" , \"scalingPolicy\" : { \"type\" : \"string\" , \"targetRate\" : 0 , \"scaleFactor\" : 0 , \"minSegments\" : 0 }, \"retentionPolicy\" : { \"type\" : \"string\" , \"value\" : 0 } }","title":"Response 200"},{"location":"rest/restapis/#delete-scopesscopenamestreamsstreamname","text":"","title":"DELETE /scopes/{scopeName}/streams/{streamName}"},{"location":"rest/restapis/#description_10","text":"Delete a stream","title":"Description"},{"location":"rest/restapis/#parameters_9","text":"Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string","title":"Parameters"},{"location":"rest/restapis/#responses_10","text":"HTTP Code Description Schema 204 Successfully deleted the stream No Content 404 Stream not found No Content 412 Cannot delete stream since it is not sealed No Content 500 Internal server error while deleting the stream No Content","title":"Responses"},{"location":"rest/restapis/#tags_11","text":"Streams","title":"Tags"},{"location":"rest/restapis/#example-http-request_10","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_10","text":"/scopes/string/streams/string","title":"Request path"},{"location":"rest/restapis/#get-scopesscopenamestreamsstreamnamescaling-events","text":"","title":"GET /scopes/{scopeName}/streams/{streamName}/scaling-events"},{"location":"rest/restapis/#description_11","text":"Get scaling events for a given datetime period.","title":"Description"},{"location":"rest/restapis/#parameters_10","text":"Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string Query from required Parameter to display scaling events from that particular datetime. Input should be milliseconds from Jan 1 1970. integer (int64) Query to required Parameter to display scaling events to that particular datetime. Input should be milliseconds from Jan 1 1970. integer (int64)","title":"Parameters"},{"location":"rest/restapis/#responses_11","text":"HTTP Code Description Schema 200 Successfully fetched list of scaling events. ScalingEventList 404 Scope/Stream not found. No Content 500 Internal Server error while fetching scaling events. No Content","title":"Responses"},{"location":"rest/restapis/#produces_9","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_12","text":"Streams","title":"Tags"},{"location":"rest/restapis/#example-http-request_11","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_11","text":"/scopes/string/streams/string/scaling-events?from=0&to=0","title":"Request path"},{"location":"rest/restapis/#example-http-response_9","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-200_7","text":"{ \"scalingEvents\" : [ { \"timestamp\" : 0 , \"segmentList\" : [ { \"number\" : 0 , \"startTime\" : 0 , \"keyStart\" : 0 , \"keyEnd\" : 0 } ], \"splits\" : 0 , \"merges\" : 0 } ] }","title":"Response 200"},{"location":"rest/restapis/#put-scopesscopenamestreamsstreamnamestate","text":"","title":"PUT /scopes/{scopeName}/streams/{streamName}/state"},{"location":"rest/restapis/#description_12","text":"Updates the current state of the stream","title":"Description"},{"location":"rest/restapis/#parameters_11","text":"Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string Body UpdateStreamStateRequest required The state info to be updated StreamState","title":"Parameters"},{"location":"rest/restapis/#responses_12","text":"HTTP Code Description Schema 200 Successfully updated the stream state StreamState 404 Scope or stream with given name not found No Content 500 Internal server error while updating the stream state No Content","title":"Responses"},{"location":"rest/restapis/#consumes_3","text":"application/json","title":"Consumes"},{"location":"rest/restapis/#produces_10","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_13","text":"Streams","title":"Tags"},{"location":"rest/restapis/#example-http-request_12","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_12","text":"/scopes/string/streams/string/state","title":"Request path"},{"location":"rest/restapis/#request-body_3","text":"{ \"streamState\" : \"string\" }","title":"Request body"},{"location":"rest/restapis/#example-http-response_10","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-200_8","text":"{ \"streamState\" : \"string\" }","title":"Response 200"},{"location":"rest/restapis/#definitions","text":"","title":"Definitions"},{"location":"rest/restapis/#readergroupproperty","text":"Name Description Schema onlineReaderIds optional Example : [ \"string\" ] < string > array readerGroupName optional Example : \"string\" string scopeName optional Example : \"string\" string streamList optional Example : [ \"string\" ] < string > array","title":"ReaderGroupProperty"},{"location":"rest/restapis/#readergroupslist","text":"Name Description Schema readerGroups optional Example : [ \"object\" ] < readerGroups > array readerGroups Name Description Schema readerGroupName optional Example : \"string\" string","title":"ReaderGroupsList"},{"location":"rest/restapis/#retentionconfig","text":"Name Description Schema type optional Example : \"string\" enum (LIMITED_DAYS, LIMITED_SIZE_MB) value optional Example : 0 integer (int64)","title":"RetentionConfig"},{"location":"rest/restapis/#scalemetadata","text":"Name Description Schema merges optional Example : 0 integer (int64) segmentList optional Example : [ \"[segment](#segment)\" ] < Segment > array splits optional Example : 0 integer (int64) timestamp optional Example : 0 integer (int64)","title":"ScaleMetadata"},{"location":"rest/restapis/#scalingconfig","text":"Name Description Schema minSegments optional Example : 0 integer (int32) scaleFactor optional Example : 0 integer (int32) targetRate optional Example : 0 integer (int32) type optional Example : \"string\" enum (FIXED_NUM_SEGMENTS, BY_RATE_IN_KBYTES_PER_SEC, BY_RATE_IN_EVENTS_PER_SEC)","title":"ScalingConfig"},{"location":"rest/restapis/#scalingeventlist","text":"Name Description Schema scalingEvents optional Example : [ \"[scalemetadata](#scalemetadata)\" ] < ScaleMetadata > array","title":"ScalingEventList"},{"location":"rest/restapis/#scopeproperty","text":"Name Description Schema scopeName optional Example : \"string\" string","title":"ScopeProperty"},{"location":"rest/restapis/#scopeslist","text":"Name Description Schema scopes optional Example : [ \"[scopeproperty](#scopeproperty)\" ] < ScopeProperty > array","title":"ScopesList"},{"location":"rest/restapis/#segment","text":"Name Description Schema keyEnd optional Example : 0 integer (double) keyStart optional Example : 0 integer (double) number optional Example : 0 integer (int32) startTime optional Example : 0 integer (int64)","title":"Segment"},{"location":"rest/restapis/#streamproperty","text":"Name Description Schema retentionPolicy optional Example : \"[retentionconfig](#retentionconfig)\" RetentionConfig scalingPolicy optional Example : \"[scalingconfig](#scalingconfig)\" ScalingConfig scopeName optional Example : \"string\" string streamName optional Example : \"string\" string","title":"StreamProperty"},{"location":"rest/restapis/#streamstate","text":"Name Description Schema streamState optional Example : \"string\" enum (SEALED)","title":"StreamState"},{"location":"rest/restapis/#streamslist","text":"Name Description Schema streams optional Example : [ \"[streamproperty](#streamproperty)\" ] < StreamProperty > array","title":"StreamsList"},{"location":"security/pravega-security-authorization-authentication/","text":"TLS, Authorization, Authentication - Enabling encryption, authorization and authentication features \u00b6 Pravega ingests application data, which is often sensitive and requires security mechanisms to avoid unauthorized access. To prevent such unauthorized accesses in shared environments, we have enabled mechanisms in Pravega that secure Stream data stored in a Pravega cluster. The security documentation covers aspects of our mechanisms and provides configuration details to enable security in Pravega. Key features of security implementation: Pravega allows administrators to enable encryption for different communication channels using TLS. Pravega provides role Based access control which can be availed by a variety of enterprises. Pravega performs dynamic implementations of the Authorization/Authentication API . Multiple implementations can co-exist and different plugins can be used by different users. Multiple mechanisms are enabled by Pravega to the users for specifying auth parameters to the client. See here for more details. Components like Bookkeeper, Zookeeper etc., which are deployed with Pravega can be deployed securely with TLS. PDP-23 discusses various options for this design and anlayzes the pros and cons in detail.","title":"Pravega Security Authorization/Authentication"},{"location":"security/pravega-security-authorization-authentication/#tls-authorization-authentication-enabling-encryption-authorization-and-authentication-features","text":"Pravega ingests application data, which is often sensitive and requires security mechanisms to avoid unauthorized access. To prevent such unauthorized accesses in shared environments, we have enabled mechanisms in Pravega that secure Stream data stored in a Pravega cluster. The security documentation covers aspects of our mechanisms and provides configuration details to enable security in Pravega. Key features of security implementation: Pravega allows administrators to enable encryption for different communication channels using TLS. Pravega provides role Based access control which can be availed by a variety of enterprises. Pravega performs dynamic implementations of the Authorization/Authentication API . Multiple implementations can co-exist and different plugins can be used by different users. Multiple mechanisms are enabled by Pravega to the users for specifying auth parameters to the client. See here for more details. Components like Bookkeeper, Zookeeper etc., which are deployed with Pravega can be deployed securely with TLS. PDP-23 discusses various options for this design and anlayzes the pros and cons in detail.","title":"TLS, Authorization, Authentication - Enabling encryption, authorization and authentication features"},{"location":"security/pravega-security-configurations/","text":"Pravega Security Configurations \u00b6 This document describes the security configuration parameters of Pravega, in both distributed and standalone modes. Security Configuration Parameters in Distributed Mode \u00b6 In the distributed mode, Controllers and Segment Stores are configured via separate sets of parameters. These parameters may be specified via configuration files or Java system properties. Alternatively, you may use environment variables to configure them. The following sub-sections describe their Transport Layer Security (TLS) and auth (short for authentication and authorization) parameters. Controller \u00b6 Parameter (Corresponding Environment Variable) Details Default Value Feature controller.auth.tlsEnabled ( TLS_ENABLED ) Whether to enable TLS for client-server communication. False TLS controller.auth.segmentStoreTlsEnabled ( TLS_ENABLED_FOR_SEGMENT_STORE ) Whether to enable TLS for communications with Segment Store, even if TLS is disabled for the Controller. This is only useful in cases where the Controller has TLS disabled, but the Segment Store has it enabled. False TLS controller.auth.tlsCertFile ( TLS_CERT_FILE ) Path of the X.509 PEM-encoded server certificate file for the service. Empty TLS controller.auth.tlsKeyFile ( TLS_KEY_FILE ) Path of the PEM-encoded private key file for the service. Empty TLS controller.auth.tlsTrustStore ( TLS_TRUST_STORE ) Path of the PEM-encoded truststore file for TLS connections with Segment Stores. Empty TLS controller.rest.tlsKeyStoreFile ( REST_KEYSTORE_FILE_PATH ) Path of the keystore file in .jks for the REST interface. Empty TLS controller.rest.tlsKeyStorePasswordFile ( REST_KEYSTORE_PASSWORD_FILE_PATH ) Path of the file containing the keystore password for the REST interface. Empty TLS controller.zk.secureConnection ( SECURE_ZK ) Whether to enable TLS for communication with Apache Zookeeper False TLS controller.zk.tlsTrustStoreFile ( ZK_TRUSTSTORE_FILE_PATH ) Path of the truststore file in .jks format for TLS connections with Apache Zookeeer. Empty TLS controller.zk.tlsTrustStorePasswordFile ( ZK_TRUSTSTORE_PASSWORD_FILE_PATH ) Path of the file containing the password of the truststore used for TLS connections with Apache Zookeeper. Empty TLS controller.auth.enabled ( AUTHORIZATION_ENABLED ) Whether to enable authentication and authorization for clients. False Auth controller.auth.userPasswordFile ( USER_PASSWORD_FILE ) Path of the file containing user credentials and ACLs, for the PasswordAuthHandler. Empty Auth controller.auth.tokenSigningKey ( TOKEN_SIGNING_KEY ) Key used to sign the delegation tokens for Segment Stores. Empty Auth Segment Store \u00b6 Parameter (Corresponding Environment Variable Description Default Value Feature pravegaservice.enableTls ( ENABLE_TLS ) Whether to enable TLS for client-server communications. False TLS pravegaservice.enableTlsReload ( ENABLE_TLS_RELOAD ) Whether to automatically reload SSL/TLS context if the server certificate is updated. False TLS pravegaservice.certFile ( CERT_FILE ) Path of the X.509 PEM-encoded server certificate file for the service. Empty TLS pravegaservice.keyFile ( KEY_FILE ) Path of the PEM-encoded private key file for the service. Empty TLS pravegaservice.secureZK ( SECURE_ZK ) Whether to enable TLS for communication with Apache Zookeeper. False TLS pravegaservice.zkTrustStore ( ZK_TRUSTSTORE_LOCATION ) Path of the truststore file in .jks format for TLS connections with Apache Zookeeer. Empty TLS pravegaservice.zkTrustStorePasswordPath ( ZK_TRUST_STORE_PASSWORD_PATH ) Path of the file containing the password of the truststore used for TLS connections with Apache Zookeeper. Empty TLS autoScale.tlsEnabled ( TLS_ENABLED ) Whether to enable TLS for internal communication with the Controllers. False TLS autoScale.tlsCertFile ( TLS_CERT_FILE ) Path of the PEM-encoded X.509 certificate file used for TLS connections with the Controllers. Empty TLS autoScale.validateHostName ( VALIDATE_HOSTNAME ) Whether to enable hostname verification for TLS connections with the Controllers. True TLS autoScale.authEnabled ( AUTH_ENABLED ) Whether to enable authentication and authorization for internal communications with the Controllers. False Auth autoScale.tokenSigningKey ( TOKEN_SIGNING_KEY ) The key used for signing the delegation tokens. Empty Auth bookkeeper.tlsEnabled ( BK_TLS_ENABLED ) Whether to enable TLS for communication with Apache Bookkeeper. False TLS bookkeeper.tlsTrustStorePath ( TLS_TRUST_STORE_PATH ) Path of the truststore file in .jks format for TLS connections with Apache Bookkeeper. Empty TLS pravega.client.auth.loadDynamic ( pravega_client_auth_loadDynamic ) Whether to load a credentials object dynamically from a class available in Classpath. false Auth pravega.client.auth.token ( pravega_client_auth_method ) The token to use by the Auto Scale Processor when communicating with Controller. Empty Auth pravega.client.auth.method ( pravega_client_auth_token ) The auth method to use by the Auto Scale Processor when communicating with Controller. Empty Auth Security Configurations in Standalone Mode \u00b6 For ease of use, Pravega standalone mode abstracts away some of the configuration parameters of distributed mode. As a result, it has fewer security configuration parameters to configure. Parameter Details Default Value Feature singlenode.enableTls Whether to enable TLS for client-server communications. False TLS singlenode.certFile Path of the X.509 PEM-encoded server certificate file for the server. Empty TLS singlenode.keyFile Path of the PEM-encoded private key file for the service. Empty TLS singlenode.keyStoreJKS Path of the keystore file in .jks for the REST interface. Empty TLS singlenode.keyStoreJKSPasswordFile Path of the file containing the keystore password for the REST interface. Empty TLS singlenode.trustStoreJKS Path of the truststore file for internal TLS connections. Empty TLS singlenode.enableAuth Whether to enable authentication and authorization for clients. False Auth singlenode.passwdFile Path of the file containing user credentials and ACLs, for the PasswordAuthHandler. Empty Auth singlenode.userName The default username used for internal communication between Segment Store and Controller. Empty Auth singlenode.passwd The default password used for internal communication between Segment Store and Controller. Empty Auth singlenode.segmentstoreEnableTlsReload Whether to automatically reload SSL/TLS context if the server certificate is updated. False TLS","title":"Pravega Security Configurations"},{"location":"security/pravega-security-configurations/#pravega-security-configurations","text":"This document describes the security configuration parameters of Pravega, in both distributed and standalone modes.","title":"Pravega Security Configurations"},{"location":"security/pravega-security-configurations/#security-configuration-parameters-in-distributed-mode","text":"In the distributed mode, Controllers and Segment Stores are configured via separate sets of parameters. These parameters may be specified via configuration files or Java system properties. Alternatively, you may use environment variables to configure them. The following sub-sections describe their Transport Layer Security (TLS) and auth (short for authentication and authorization) parameters.","title":"Security Configuration Parameters in Distributed Mode"},{"location":"security/pravega-security-configurations/#controller","text":"Parameter (Corresponding Environment Variable) Details Default Value Feature controller.auth.tlsEnabled ( TLS_ENABLED ) Whether to enable TLS for client-server communication. False TLS controller.auth.segmentStoreTlsEnabled ( TLS_ENABLED_FOR_SEGMENT_STORE ) Whether to enable TLS for communications with Segment Store, even if TLS is disabled for the Controller. This is only useful in cases where the Controller has TLS disabled, but the Segment Store has it enabled. False TLS controller.auth.tlsCertFile ( TLS_CERT_FILE ) Path of the X.509 PEM-encoded server certificate file for the service. Empty TLS controller.auth.tlsKeyFile ( TLS_KEY_FILE ) Path of the PEM-encoded private key file for the service. Empty TLS controller.auth.tlsTrustStore ( TLS_TRUST_STORE ) Path of the PEM-encoded truststore file for TLS connections with Segment Stores. Empty TLS controller.rest.tlsKeyStoreFile ( REST_KEYSTORE_FILE_PATH ) Path of the keystore file in .jks for the REST interface. Empty TLS controller.rest.tlsKeyStorePasswordFile ( REST_KEYSTORE_PASSWORD_FILE_PATH ) Path of the file containing the keystore password for the REST interface. Empty TLS controller.zk.secureConnection ( SECURE_ZK ) Whether to enable TLS for communication with Apache Zookeeper False TLS controller.zk.tlsTrustStoreFile ( ZK_TRUSTSTORE_FILE_PATH ) Path of the truststore file in .jks format for TLS connections with Apache Zookeeer. Empty TLS controller.zk.tlsTrustStorePasswordFile ( ZK_TRUSTSTORE_PASSWORD_FILE_PATH ) Path of the file containing the password of the truststore used for TLS connections with Apache Zookeeper. Empty TLS controller.auth.enabled ( AUTHORIZATION_ENABLED ) Whether to enable authentication and authorization for clients. False Auth controller.auth.userPasswordFile ( USER_PASSWORD_FILE ) Path of the file containing user credentials and ACLs, for the PasswordAuthHandler. Empty Auth controller.auth.tokenSigningKey ( TOKEN_SIGNING_KEY ) Key used to sign the delegation tokens for Segment Stores. Empty Auth","title":"Controller"},{"location":"security/pravega-security-configurations/#segment-store","text":"Parameter (Corresponding Environment Variable Description Default Value Feature pravegaservice.enableTls ( ENABLE_TLS ) Whether to enable TLS for client-server communications. False TLS pravegaservice.enableTlsReload ( ENABLE_TLS_RELOAD ) Whether to automatically reload SSL/TLS context if the server certificate is updated. False TLS pravegaservice.certFile ( CERT_FILE ) Path of the X.509 PEM-encoded server certificate file for the service. Empty TLS pravegaservice.keyFile ( KEY_FILE ) Path of the PEM-encoded private key file for the service. Empty TLS pravegaservice.secureZK ( SECURE_ZK ) Whether to enable TLS for communication with Apache Zookeeper. False TLS pravegaservice.zkTrustStore ( ZK_TRUSTSTORE_LOCATION ) Path of the truststore file in .jks format for TLS connections with Apache Zookeeer. Empty TLS pravegaservice.zkTrustStorePasswordPath ( ZK_TRUST_STORE_PASSWORD_PATH ) Path of the file containing the password of the truststore used for TLS connections with Apache Zookeeper. Empty TLS autoScale.tlsEnabled ( TLS_ENABLED ) Whether to enable TLS for internal communication with the Controllers. False TLS autoScale.tlsCertFile ( TLS_CERT_FILE ) Path of the PEM-encoded X.509 certificate file used for TLS connections with the Controllers. Empty TLS autoScale.validateHostName ( VALIDATE_HOSTNAME ) Whether to enable hostname verification for TLS connections with the Controllers. True TLS autoScale.authEnabled ( AUTH_ENABLED ) Whether to enable authentication and authorization for internal communications with the Controllers. False Auth autoScale.tokenSigningKey ( TOKEN_SIGNING_KEY ) The key used for signing the delegation tokens. Empty Auth bookkeeper.tlsEnabled ( BK_TLS_ENABLED ) Whether to enable TLS for communication with Apache Bookkeeper. False TLS bookkeeper.tlsTrustStorePath ( TLS_TRUST_STORE_PATH ) Path of the truststore file in .jks format for TLS connections with Apache Bookkeeper. Empty TLS pravega.client.auth.loadDynamic ( pravega_client_auth_loadDynamic ) Whether to load a credentials object dynamically from a class available in Classpath. false Auth pravega.client.auth.token ( pravega_client_auth_method ) The token to use by the Auto Scale Processor when communicating with Controller. Empty Auth pravega.client.auth.method ( pravega_client_auth_token ) The auth method to use by the Auto Scale Processor when communicating with Controller. Empty Auth","title":"Segment Store"},{"location":"security/pravega-security-configurations/#security-configurations-in-standalone-mode","text":"For ease of use, Pravega standalone mode abstracts away some of the configuration parameters of distributed mode. As a result, it has fewer security configuration parameters to configure. Parameter Details Default Value Feature singlenode.enableTls Whether to enable TLS for client-server communications. False TLS singlenode.certFile Path of the X.509 PEM-encoded server certificate file for the server. Empty TLS singlenode.keyFile Path of the PEM-encoded private key file for the service. Empty TLS singlenode.keyStoreJKS Path of the keystore file in .jks for the REST interface. Empty TLS singlenode.keyStoreJKSPasswordFile Path of the file containing the keystore password for the REST interface. Empty TLS singlenode.trustStoreJKS Path of the truststore file for internal TLS connections. Empty TLS singlenode.enableAuth Whether to enable authentication and authorization for clients. False Auth singlenode.passwdFile Path of the file containing user credentials and ACLs, for the PasswordAuthHandler. Empty Auth singlenode.userName The default username used for internal communication between Segment Store and Controller. Empty Auth singlenode.passwd The default password used for internal communication between Segment Store and Controller. Empty Auth singlenode.segmentstoreEnableTlsReload Whether to automatically reload SSL/TLS context if the server certificate is updated. False TLS","title":"Security Configurations in Standalone Mode"},{"location":"security/pravega-security-encryption/","text":"Pravega Encryption \u00b6 Encryption of data in flight \u00b6 Pravega ensures that all the data in flight can be passed by applying encryption. The different channels can be configured with TLS and encryption can be enabled for them. Certificate Management \u00b6 Pravega expects administrators and users to create and manage certificate creation, deployment and management. Pravega provides various configuration parameters using which certificates for different communication channels can be specified. Encrypted data flow between Pravega client and Pravega Controller and Segment Store \u00b6 Pravega uses same certificate to interact with the Controller and Segment Store. The certificates needs to be mentioned specifically on the client and the server machine. Note: These certificates are not loaded from the truststore. Encrypted data flow between Pravega and Tier 1 (Apache Bookkeeper) \u00b6 Pravega Segment Store uses Apache Bookkeeper as Tier 1 Storage. Apache Bookkeeper supports JKS based truststore. Segment Store uses JKS based truststore to interact with it. See, configurations for more details. Encrypted access to Apache Zookeeper \u00b6 Pravega Segment Store and Pravega Controller interact with Apache Zookeeper. These connections can be encrypted based on configuration. The details about the configurations can be found at for Pravega Segment Store and for Pravega Controller . Encryption of data at rest \u00b6 Encryption of data in Tier 1 \u00b6 Pravega uses Apache BookKeeper as Tier 1 implementation. Apache Bookkeeper currently does not support encryption of data written to disk. Encryption of data in Tier 2 \u00b6 Pravega can work with different storage options for Tier 2. To use any specific storage option, it is necessary to implement a storage interface. We currently have the following options implemented: HDFS Extended S3 File system (NFS). Pravega does not encrypt data before storing it in Tier 2. Consequently, Tier 2 encryption is only an option in the case the storage system provides it natively, and as such, needs to be configured directly in the storage system, not via Pravega.","title":"Pravega Security Encryption"},{"location":"security/pravega-security-encryption/#pravega-encryption","text":"","title":"Pravega Encryption"},{"location":"security/pravega-security-encryption/#encryption-of-data-in-flight","text":"Pravega ensures that all the data in flight can be passed by applying encryption. The different channels can be configured with TLS and encryption can be enabled for them.","title":"Encryption of data in flight"},{"location":"security/pravega-security-encryption/#certificate-management","text":"Pravega expects administrators and users to create and manage certificate creation, deployment and management. Pravega provides various configuration parameters using which certificates for different communication channels can be specified.","title":"Certificate Management"},{"location":"security/pravega-security-encryption/#encrypted-data-flow-between-pravega-client-and-pravega-controller-and-segment-store","text":"Pravega uses same certificate to interact with the Controller and Segment Store. The certificates needs to be mentioned specifically on the client and the server machine. Note: These certificates are not loaded from the truststore.","title":"Encrypted data flow between Pravega client and Pravega Controller and Segment Store"},{"location":"security/pravega-security-encryption/#encrypted-data-flow-between-pravega-and-tier-1-apache-bookkeeper","text":"Pravega Segment Store uses Apache Bookkeeper as Tier 1 Storage. Apache Bookkeeper supports JKS based truststore. Segment Store uses JKS based truststore to interact with it. See, configurations for more details.","title":"Encrypted data flow between Pravega and Tier 1 (Apache Bookkeeper)"},{"location":"security/pravega-security-encryption/#encrypted-access-to-apache-zookeeper","text":"Pravega Segment Store and Pravega Controller interact with Apache Zookeeper. These connections can be encrypted based on configuration. The details about the configurations can be found at for Pravega Segment Store and for Pravega Controller .","title":"Encrypted access to Apache Zookeeper"},{"location":"security/pravega-security-encryption/#encryption-of-data-at-rest","text":"","title":"Encryption of data at rest"},{"location":"security/pravega-security-encryption/#encryption-of-data-in-tier-1","text":"Pravega uses Apache BookKeeper as Tier 1 implementation. Apache Bookkeeper currently does not support encryption of data written to disk.","title":"Encryption of data in Tier 1"},{"location":"security/pravega-security-encryption/#encryption-of-data-in-tier-2","text":"Pravega can work with different storage options for Tier 2. To use any specific storage option, it is necessary to implement a storage interface. We currently have the following options implemented: HDFS Extended S3 File system (NFS). Pravega does not encrypt data before storing it in Tier 2. Consequently, Tier 2 encryption is only an option in the case the storage system provides it natively, and as such, needs to be configured directly in the storage system, not via Pravega.","title":"Encryption of data in Tier 2"},{"location":"security/securing-distributed-mode-cluster/","text":"Setting Up Security for a Distributed Mode Cluster \u00b6 Introduction Setting Up SSL/TLS Stage 1: Setting Up a Certificate Authority (CA) Stage 2: Obtaining Server Certificates and Keys Stage 3: Enabling TLS and Deploying Certificates Enabling TLS and Auth in Pravega Configuring TLS and Auth on Server Side Configuring TLS and Credentials on Client Side * Server Hostname Verification Having TLS and Auth Take Effect Conclusion Introduction \u00b6 In the distributed mode of running a Pravega cluster, each service runs separately on one or more processes, usually spread across multiple machines. The deployment options of this mode include: A manual deployment in hardware or virtual machines Containerized deployments of these types: A Kubernetes native application deployed using the Pravega Operator A Docker Compose application deployment A Docker Swarm based distributed deployment Regardless of the deployment option used, setting up Transport Layer Security (SSL/TLS) and client auth (short for authentication and authorization) are important steps towards a secure Pravega deployment. TLS encrypts client-server and internal communication. It also enables clients to authenticate the services running on the server nodes. Client auth enables the services to authenticate and authorize the clients. Pravega strongly recommends enabling both TLS and auth, for production clusters. Setting up security - especially TLS - in a large cluster can be daunting at first. To make it easier, this document provides step-by-step instructions on how to enable and configure security manually. Depending on the deployment option used and your environment, you might need to modify the steps and commands to suit your specific needs and policies. Setting Up SSL/TLS \u00b6 There are broadly two ways of using TLS for client-server communications: Setup Pravega to handle TLS directly. In this case, end-to-end traffic is encrypted. Terminate TLS outside of Pravega, in an infrastructure component such as a reverse proxy or a load balancer. Traffic is encrypted until the terminating point and is in plaintext from there to Pravega. Depending on the deployment option used, it might be easier to use one or the other approach. For example, if you are deploying a Pravega cluster in Kubernetes, you might find approach 2 simpler and easier to manage. In a cluster deployed manually on hardware machines, it might be more convenient to use approach 1 in many cases. The specifics of enabling TLS will also differ depending on the deployment option used. Here, we describe the steps applicable for approach 1 in manual deployments. For approach 2, refer to the platform vendors' documentation, such as this one from Google Kubernetes Engine. At a high level, setting up TLS can be divided into three distinct stages: Setting up a Certificate Authority (CA) Obtaining server certificates and keys Enabling TLS and Deploying certificates They are described in detail in the following sub-sections. Before you Begin As the steps in this section use either OpenSSL or Java Keytool, install OpenSSL and Java Development Kit (JDK) on the hosts that will be used to generate TLS certificates, keys, keystores are truststores. Note: * The examples shown in this section use command line arguments to pass all inputs to the command. To pass sensitive command arguments via prompts instead, just exclude the corresponding option. For example, # Inputs are passed as command line arguments. $ keytool -keystore server01.keystore.jks -alias server01 -validity <validity> -genkey \\ -storepass <keystore-password> -keypass <key-password> \\ -dname <distinguished-name> -ext SAN = DNS:<hostname>, # Passwords and other arguments are entered interactively on the prompt in this case. $ keytool -keystore server01.keystore.jks -alias server01 -genkey * A weak password changeit is used everywhere, for easier reading. Be sure to replace it with a strong and separate password for each file. Stage 1: Setting Up a Certificate Authority (CA) \u00b6 If you are going to use an existing public or internal CA service or certificate and key bundle, you may skip this part altogether, and go to Obtaining Server Certificates and keys . Here, we'll generate a CA in the form of a public/private key pair and a self-signed certificate. Later, we'll use the CA certificate/key bundle to sign server certificates used in the cluster. Generate a certificate and public/private key pair, for use as a CA. # All inputs provided using command line arguments $ openssl req -new -x509 -keyout ca-key -out ca-cert -days <validity> \\ -subj \"<distinguished_name>\" \\ -passout pass:<strong_password> # Sample command $ openssl req -new -x509 -keyout ca-key -out ca-cert -days 365 \\ -subj \"/C=US/ST=Washington/L=Seattle/O=Pravega/OU=CA/CN=Pravega-CA\" \\ -passout pass:changeit Create a truststore containing the CA's certificate. This truststore will be used by external and internal clients. External clients are client applications connected to a Pravega cluster. Services running on server nodes play the role of internal clients when accessing other services. $ keytool -keystore client.truststore.jks -noprompt -alias CARoot -import -file ca-cert \\ -storepass changeit # Optionally, list the truststore's contents to verify everything is in order. The output should show # a single entry with alias name `caroot` and entry type `trustedCertEntry`. $ keytool -list -v -keystore client.truststore.jks -storepass changeit At this point, the following CA and client truststore artifacts shall be available: File Description ca-cert PEM-encoded X.509 certificate of the CA ca-key PEM-encoded file containing the CA's encrypted private key client.truststore.jks A password-protected truststore file containing the CA's certificate Stage 2: Obtaining Server Certificates and keys \u00b6 This stage is about performing the following steps for each service. Generating server certificates and keys Generating a Certificate Signing Request (CSR) Submitting the CSR to a CA and obtaining a signed certificate Preparing a keystore containing the signed server certificate and the CA's certificate Exporting the server certificate's private key Note: For services running on the same host, the same certificate can be used, if those services are accessed using the same hostname/IP address. Also, wildcard certificates can be used to share certificates across hosts. However, it is strongly recommended that separate certificates be used for each service. The steps are: Generate a public/private key-pair and a X.509 certificate for each service. This certificate is used for TLS connections with clients and by clients to verifying the server's identity. $ keytool -keystore controller01.jks \\ -genkey -keyalg RSA -keysize 2048 -keypass changeit \\ -alias controller01 -validity 365 \\ -dname \"CN=controller01.pravega.io, OU=..., O=..., L=..., S=..., C=...\" \\ -ext san = dns:controller01.pravega.io,ip:... \\ -storepass changeit # Optionally, verify the contents of the generated file $ keytool -list -v -keystore controller01.jks -storepass changeit Generate a certificate signing request (CSR) for each service. It helps to think of a CSR as an application for getting a certificate signed by a trusted authority. A CSR is typically generated on the same server/node on which the service is planned to be installed. In some other environments, CSRs are generated in a central server and the resulting certificates are distributed to the services that need them. $ keytool -keystore controller01.jks -alias controller01 -certreq -file controller01.csr \\ -storepass changeit # Optionally, inspect the contents of the CSR file. The CSR is created in Base-64 encoded `PEM` format. $ openssl req -in controller01.csr -noout -text Submit the CSR to a CA and obtain a signed certificate for each service. If you are using a public or internal CA service, follow that CA's process for submitting the CSR and obtaining a signed certificate. To use the custom CA generated using the steps mentioned earlier or an internal CA certificate/key bundle, use the following command, to generate a CA-signed server certificate in PEM format: $ openssl x509 -req -CA ca-cert -CAkey ca-key -in controller01.csr -out controller01.pem \\ -days 3650 -CAcreateserial -passin pass:changeit Prepare a keystore containing the signed server certificate and the CA's certificate. # Import the CA certificate into a new keystore file. $ keytool -keystore controller01.server.jks -alias CARoot -noprompt \\ -import -file ca-cert -storepass changeit # Import the signed server certificate into the keystore. $ keytool -keystore controller01.server.jks -alias controller01 -noprompt \\ -import -file controller01.pem -storepass changeit Export each server's key into a separate PEM file. This is a two step process. * First, convert the server's keystore in .jks format into .p12 format. ```bash $ keytool -importkeystore -srckeystore controller01.jks \\ -destkeystore controller01.p12 \\ -srcstoretype jks -deststoretype pkcs12 \\ -srcstorepass changeit -deststorepass changeit ``` Then, export the private key of the server into a PEM file. Note that the generated PEM file is not protected by a password. The key itself is password-protected, as we are using the -nodes flag. So, be sure to protect it using operating system's technical controls as well as procedural controls. $ openssl pkcs12 -in controller01.p12 -out controller01.key.pem -passin pass:1111_aaaa -nodes Step 5 concludes this stage, and the stage is now set for installing the certificates and other PKI material in Pravega. The table below lists the key output of this stage. Note that you'll typically need one of each file per Pravega service, but you may share the same file for services collocated on the same host for logistical or economical reasons. Files Example File Description Certificate in PEM format controller01.pem file The signed certificate to be used by the service. Private key in PEM format controller01.key.pem file The private key to be used by the service. Server keystore in JKS format controller01.server.jks file The keystore file to be used by the service. Stage 3: Enabling TLS and Deploying Certificates \u00b6 We'll discuss this in the next section, together with other security configuration and setup. Enabling TLS and Auth in Pravega \u00b6 Enabling TLS and auth in Pravega involves the following steps: Configuring TLS and auth on server side Configuring TLS and credentials on client Side Having TLS and auth take effect These steps are discussed in the following sub-sections. Configuring TLS and Auth on Server Side \u00b6 This step is about using the certificates, keys, keystores and truststores generated earlier to configure TLS and Auth for the services on the server side. Note that if you enable TLS and Auth on one service, you must enable them for all the other services too. Pravega does not support using TLS and Auth for only some instances of the services. You can configure the following services for TLS and Auth: Controller Segment Store Zookeeper (optionally) Bookkeeper (optionally) For information about enabling TLS for Zookeeper and Bookeeper, refer to their documentation here: * ZooKeeper SSL Guide * BookKeeper - Encryption and Authentication using TLS Configuring security for Controllers and Segment Stores is discussed below. Controller Controller services can be configured in three different ways: By specifying the configuration parameter values directly in the controller.config.properties file. For example, controller.auth.tlsEnabled=true controller.auth.tlsCertFile=/etc/secrets/controller01.pem 2. By specifying configuration values via corresponding environment variables. For example, # TLS_ENABLED environment variable corresponds to Controller configuration parameter # \"controller.auth.tlsEnabled\". $ export TLS_ENABLED: \"true\" To identify the environment variables corresponding to the configuration parameter, inspect the default controller.config.properties file and locate the variables. controller.auth.tlsEnabled=${TLS_ENABLED} controller.auth.tlsCertFile=${TLS_CERT_FILE} By specifying configuration parameters as JVM system properties. This way of configuring Controller service is more relevant for container application deployment tools and orchestrators such as Docker Compose, Swarm and Kubernetes. # Example: docker-compose.yml file ... services: ... controller: environment: ... JAVA_OPTS: -dcontroller.auth.tlsEnabled=true -dcontroller.auth.tlsCertFile=... ... ... The following table lists the Controller's TLS and auth parameters and representative values, for quick reference. For a detailed description of these parameters, refer to this document. Configuration Parameter Example Value controller.auth.tlsEnabled true controller.auth.tlsCertFile /etc/secrets/controller01.pem controller.auth.tlsKeyFile /etc/secrets/controller01.key.pem controller.auth.tlsTrustStore /etc/secrets/ca-cert controller.rest.tlsKeyStoreFile /etc/secrets/controller01.server.jks controller.rest.tlsKeyStorePasswordFile /etc/secrets/controller01.server.jks.password 1 controller.zk.secureConnection false 2 controller.zk.tlsTrustStoreFile /etc/secrets/client.truststore.jks controller.zk.tlsTrustStorePasswordFile /etc/secrets/client.truststore.jks.password controller.auth.enabled true controller.auth.userPasswordFile 3 /etc/secrets/password-auth-handler.inputfile `controller.auth.tokenSigningKey a-secret-value [1]: This and other .password files are text files containing the password for the corresponding store. [2]: It is assumed here that Zookeeper TLS is disabled. You may enable it and specify the corresponding client-side TLS configuration properties via the controller.zk.* properties. [3]: This configuration property is required when using the default Password Auth Handler only. Segment Store Segment store supports configuration via a properties file ( config.properties ) or JVM system properties. The table below lists its TLS and auth parameters and sample values. For a detailed discription of these parameters refer to this document. Configuration Parameter Example Value pravegaservice.enableTls true pravegaservice.certFile /etc/secrets/segmentstore01.pem pravegaservice.keyFile /etc/secrets/segmentstore01.key.pem pravegaservice.secureZK false 2 pravegaservice.zkTrustStore /etc/secrets/client.truststore.jks pravegaservice.zkTrustStorePasswordPath /etc/secrets/client.truststore.jks.password autoScale.tlsEnabled true autoScale.tlsCertFile /etc/secrets/segmentstore01.key.pem autoScale.authEnabled true autoScale.tokenSigningKey a-secret-value 1 autoScale.validateHostName true [1]: It is assumed here that Zookeeper TLS is disabled. You may enable it and specify the corresponding client-side TLS configuration properties via the pravegaservice.zk.* properties. [2]: The secret value you use here must match the same value used for other Controller and Segment Store services. Configuring TLS and Credentials on Client Side \u00b6 After enabling and configuring TLS and auth on the server-side services, its time to update the clients, so that the they can establish TLS connections with the servers and are allowed access. For TLS, establish trust for the servers' certificates on the client side using one of the following ways: Supply the client library with the certificate of the trusted CA that has signed the servers' certificates. ClientConfig clientConfig = ClientConfig.builder() .controllerURI(\"tls://<DNS-NAME-OR-IP>:9090\") .trustStore(\"/etc/secrets/ca-cert\") ... .build(); Install the CA's certificate in the Java system key store. Create a custom truststore with the CA's certificate and supply it to the Pravega client application, via JVM system properties javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword . For auth, client-side configuration depends on the AuthHandler implementation used. If your server is configured to use the default PasswordAuthHandler , you may supply the credentials as shown below. ClientConfig clientConfig = ClientConfig.builder() .controllerURI(\"tls://<DNS-NAME-OR-IP>:9090\") .trustStore(\"/etc/secrets/ca-cert\") .credentials(new DefaultCredentials(\"changeit\", \"marketinganaylticsapp\")) .build(); Server Hostname Verification \u00b6 Hostname verification during TLS communications verifies that the DNS name to which the client connects matches the hostname specified in either of the following fields in the server's certificate: Common Name ( CN ) in the certificate's Subject field One of the Subject Alternative Names field entries If the server certificates have a hostname assigned, you have used IP addresses as endpoints for the services, and those hostnames are not accessible from the client nodes, you might need to add mappings of IP addresses and DNS/Host names in the client-side operating system hosts file. Alternatively, you may disable hostname verification by invoking validateHostName(false) of the ClientConfig builder. It is strongly recommended to avoid disabling hostname verification for production clusters. Having TLS and Auth Take Effect \u00b6 To ensure TLS and auth parameters take effect, all the services on the server-side need to be restarted. Existing client applications will need to be restarted as well, after they are reconfigured for TLS and auth. For fresh deployments, starting the cluster and the clients after configuring TLS and auth, will automatically ensure they take effect. Conclusion \u00b6 This document explained about how to enable security in a Pravega cluster running in distributed mode. Specifically, how to perform the following actions were discussed: Generating a CA (if needed) Generating server certificates and keys for Pravega services Signing the generated certificates using the generated CA Enabling and configuring TLS and auth on the server Side Setting up the ClientConfig on the client side for communicating with a Pravega cluster running with TLS and auth enabled Having TLS and auth take effect","title":"Securing Pravega in Distributed Mode"},{"location":"security/securing-distributed-mode-cluster/#setting-up-security-for-a-distributed-mode-cluster","text":"Introduction Setting Up SSL/TLS Stage 1: Setting Up a Certificate Authority (CA) Stage 2: Obtaining Server Certificates and Keys Stage 3: Enabling TLS and Deploying Certificates Enabling TLS and Auth in Pravega Configuring TLS and Auth on Server Side Configuring TLS and Credentials on Client Side * Server Hostname Verification Having TLS and Auth Take Effect Conclusion","title":"Setting Up Security for a Distributed Mode Cluster"},{"location":"security/securing-distributed-mode-cluster/#introduction","text":"In the distributed mode of running a Pravega cluster, each service runs separately on one or more processes, usually spread across multiple machines. The deployment options of this mode include: A manual deployment in hardware or virtual machines Containerized deployments of these types: A Kubernetes native application deployed using the Pravega Operator A Docker Compose application deployment A Docker Swarm based distributed deployment Regardless of the deployment option used, setting up Transport Layer Security (SSL/TLS) and client auth (short for authentication and authorization) are important steps towards a secure Pravega deployment. TLS encrypts client-server and internal communication. It also enables clients to authenticate the services running on the server nodes. Client auth enables the services to authenticate and authorize the clients. Pravega strongly recommends enabling both TLS and auth, for production clusters. Setting up security - especially TLS - in a large cluster can be daunting at first. To make it easier, this document provides step-by-step instructions on how to enable and configure security manually. Depending on the deployment option used and your environment, you might need to modify the steps and commands to suit your specific needs and policies.","title":"Introduction"},{"location":"security/securing-distributed-mode-cluster/#setting-up-ssltls","text":"There are broadly two ways of using TLS for client-server communications: Setup Pravega to handle TLS directly. In this case, end-to-end traffic is encrypted. Terminate TLS outside of Pravega, in an infrastructure component such as a reverse proxy or a load balancer. Traffic is encrypted until the terminating point and is in plaintext from there to Pravega. Depending on the deployment option used, it might be easier to use one or the other approach. For example, if you are deploying a Pravega cluster in Kubernetes, you might find approach 2 simpler and easier to manage. In a cluster deployed manually on hardware machines, it might be more convenient to use approach 1 in many cases. The specifics of enabling TLS will also differ depending on the deployment option used. Here, we describe the steps applicable for approach 1 in manual deployments. For approach 2, refer to the platform vendors' documentation, such as this one from Google Kubernetes Engine. At a high level, setting up TLS can be divided into three distinct stages: Setting up a Certificate Authority (CA) Obtaining server certificates and keys Enabling TLS and Deploying certificates They are described in detail in the following sub-sections. Before you Begin As the steps in this section use either OpenSSL or Java Keytool, install OpenSSL and Java Development Kit (JDK) on the hosts that will be used to generate TLS certificates, keys, keystores are truststores. Note: * The examples shown in this section use command line arguments to pass all inputs to the command. To pass sensitive command arguments via prompts instead, just exclude the corresponding option. For example, # Inputs are passed as command line arguments. $ keytool -keystore server01.keystore.jks -alias server01 -validity <validity> -genkey \\ -storepass <keystore-password> -keypass <key-password> \\ -dname <distinguished-name> -ext SAN = DNS:<hostname>, # Passwords and other arguments are entered interactively on the prompt in this case. $ keytool -keystore server01.keystore.jks -alias server01 -genkey * A weak password changeit is used everywhere, for easier reading. Be sure to replace it with a strong and separate password for each file.","title":"Setting Up SSL/TLS"},{"location":"security/securing-distributed-mode-cluster/#stage-1-setting-up-a-certificate-authority-ca","text":"If you are going to use an existing public or internal CA service or certificate and key bundle, you may skip this part altogether, and go to Obtaining Server Certificates and keys . Here, we'll generate a CA in the form of a public/private key pair and a self-signed certificate. Later, we'll use the CA certificate/key bundle to sign server certificates used in the cluster. Generate a certificate and public/private key pair, for use as a CA. # All inputs provided using command line arguments $ openssl req -new -x509 -keyout ca-key -out ca-cert -days <validity> \\ -subj \"<distinguished_name>\" \\ -passout pass:<strong_password> # Sample command $ openssl req -new -x509 -keyout ca-key -out ca-cert -days 365 \\ -subj \"/C=US/ST=Washington/L=Seattle/O=Pravega/OU=CA/CN=Pravega-CA\" \\ -passout pass:changeit Create a truststore containing the CA's certificate. This truststore will be used by external and internal clients. External clients are client applications connected to a Pravega cluster. Services running on server nodes play the role of internal clients when accessing other services. $ keytool -keystore client.truststore.jks -noprompt -alias CARoot -import -file ca-cert \\ -storepass changeit # Optionally, list the truststore's contents to verify everything is in order. The output should show # a single entry with alias name `caroot` and entry type `trustedCertEntry`. $ keytool -list -v -keystore client.truststore.jks -storepass changeit At this point, the following CA and client truststore artifacts shall be available: File Description ca-cert PEM-encoded X.509 certificate of the CA ca-key PEM-encoded file containing the CA's encrypted private key client.truststore.jks A password-protected truststore file containing the CA's certificate","title":"Stage 1: Setting Up a Certificate Authority (CA)"},{"location":"security/securing-distributed-mode-cluster/#stage-2-obtaining-server-certificates-and-keys","text":"This stage is about performing the following steps for each service. Generating server certificates and keys Generating a Certificate Signing Request (CSR) Submitting the CSR to a CA and obtaining a signed certificate Preparing a keystore containing the signed server certificate and the CA's certificate Exporting the server certificate's private key Note: For services running on the same host, the same certificate can be used, if those services are accessed using the same hostname/IP address. Also, wildcard certificates can be used to share certificates across hosts. However, it is strongly recommended that separate certificates be used for each service. The steps are: Generate a public/private key-pair and a X.509 certificate for each service. This certificate is used for TLS connections with clients and by clients to verifying the server's identity. $ keytool -keystore controller01.jks \\ -genkey -keyalg RSA -keysize 2048 -keypass changeit \\ -alias controller01 -validity 365 \\ -dname \"CN=controller01.pravega.io, OU=..., O=..., L=..., S=..., C=...\" \\ -ext san = dns:controller01.pravega.io,ip:... \\ -storepass changeit # Optionally, verify the contents of the generated file $ keytool -list -v -keystore controller01.jks -storepass changeit Generate a certificate signing request (CSR) for each service. It helps to think of a CSR as an application for getting a certificate signed by a trusted authority. A CSR is typically generated on the same server/node on which the service is planned to be installed. In some other environments, CSRs are generated in a central server and the resulting certificates are distributed to the services that need them. $ keytool -keystore controller01.jks -alias controller01 -certreq -file controller01.csr \\ -storepass changeit # Optionally, inspect the contents of the CSR file. The CSR is created in Base-64 encoded `PEM` format. $ openssl req -in controller01.csr -noout -text Submit the CSR to a CA and obtain a signed certificate for each service. If you are using a public or internal CA service, follow that CA's process for submitting the CSR and obtaining a signed certificate. To use the custom CA generated using the steps mentioned earlier or an internal CA certificate/key bundle, use the following command, to generate a CA-signed server certificate in PEM format: $ openssl x509 -req -CA ca-cert -CAkey ca-key -in controller01.csr -out controller01.pem \\ -days 3650 -CAcreateserial -passin pass:changeit Prepare a keystore containing the signed server certificate and the CA's certificate. # Import the CA certificate into a new keystore file. $ keytool -keystore controller01.server.jks -alias CARoot -noprompt \\ -import -file ca-cert -storepass changeit # Import the signed server certificate into the keystore. $ keytool -keystore controller01.server.jks -alias controller01 -noprompt \\ -import -file controller01.pem -storepass changeit Export each server's key into a separate PEM file. This is a two step process. * First, convert the server's keystore in .jks format into .p12 format. ```bash $ keytool -importkeystore -srckeystore controller01.jks \\ -destkeystore controller01.p12 \\ -srcstoretype jks -deststoretype pkcs12 \\ -srcstorepass changeit -deststorepass changeit ``` Then, export the private key of the server into a PEM file. Note that the generated PEM file is not protected by a password. The key itself is password-protected, as we are using the -nodes flag. So, be sure to protect it using operating system's technical controls as well as procedural controls. $ openssl pkcs12 -in controller01.p12 -out controller01.key.pem -passin pass:1111_aaaa -nodes Step 5 concludes this stage, and the stage is now set for installing the certificates and other PKI material in Pravega. The table below lists the key output of this stage. Note that you'll typically need one of each file per Pravega service, but you may share the same file for services collocated on the same host for logistical or economical reasons. Files Example File Description Certificate in PEM format controller01.pem file The signed certificate to be used by the service. Private key in PEM format controller01.key.pem file The private key to be used by the service. Server keystore in JKS format controller01.server.jks file The keystore file to be used by the service.","title":"Stage 2: Obtaining Server Certificates and keys"},{"location":"security/securing-distributed-mode-cluster/#stage-3-enabling-tls-and-deploying-certificates","text":"We'll discuss this in the next section, together with other security configuration and setup.","title":"Stage 3: Enabling TLS and Deploying Certificates"},{"location":"security/securing-distributed-mode-cluster/#enabling-tls-and-auth-in-pravega","text":"Enabling TLS and auth in Pravega involves the following steps: Configuring TLS and auth on server side Configuring TLS and credentials on client Side Having TLS and auth take effect These steps are discussed in the following sub-sections.","title":"Enabling TLS and Auth in Pravega"},{"location":"security/securing-distributed-mode-cluster/#configuring-tls-and-auth-on-server-side","text":"This step is about using the certificates, keys, keystores and truststores generated earlier to configure TLS and Auth for the services on the server side. Note that if you enable TLS and Auth on one service, you must enable them for all the other services too. Pravega does not support using TLS and Auth for only some instances of the services. You can configure the following services for TLS and Auth: Controller Segment Store Zookeeper (optionally) Bookkeeper (optionally) For information about enabling TLS for Zookeeper and Bookeeper, refer to their documentation here: * ZooKeeper SSL Guide * BookKeeper - Encryption and Authentication using TLS Configuring security for Controllers and Segment Stores is discussed below. Controller Controller services can be configured in three different ways: By specifying the configuration parameter values directly in the controller.config.properties file. For example, controller.auth.tlsEnabled=true controller.auth.tlsCertFile=/etc/secrets/controller01.pem 2. By specifying configuration values via corresponding environment variables. For example, # TLS_ENABLED environment variable corresponds to Controller configuration parameter # \"controller.auth.tlsEnabled\". $ export TLS_ENABLED: \"true\" To identify the environment variables corresponding to the configuration parameter, inspect the default controller.config.properties file and locate the variables. controller.auth.tlsEnabled=${TLS_ENABLED} controller.auth.tlsCertFile=${TLS_CERT_FILE} By specifying configuration parameters as JVM system properties. This way of configuring Controller service is more relevant for container application deployment tools and orchestrators such as Docker Compose, Swarm and Kubernetes. # Example: docker-compose.yml file ... services: ... controller: environment: ... JAVA_OPTS: -dcontroller.auth.tlsEnabled=true -dcontroller.auth.tlsCertFile=... ... ... The following table lists the Controller's TLS and auth parameters and representative values, for quick reference. For a detailed description of these parameters, refer to this document. Configuration Parameter Example Value controller.auth.tlsEnabled true controller.auth.tlsCertFile /etc/secrets/controller01.pem controller.auth.tlsKeyFile /etc/secrets/controller01.key.pem controller.auth.tlsTrustStore /etc/secrets/ca-cert controller.rest.tlsKeyStoreFile /etc/secrets/controller01.server.jks controller.rest.tlsKeyStorePasswordFile /etc/secrets/controller01.server.jks.password 1 controller.zk.secureConnection false 2 controller.zk.tlsTrustStoreFile /etc/secrets/client.truststore.jks controller.zk.tlsTrustStorePasswordFile /etc/secrets/client.truststore.jks.password controller.auth.enabled true controller.auth.userPasswordFile 3 /etc/secrets/password-auth-handler.inputfile `controller.auth.tokenSigningKey a-secret-value [1]: This and other .password files are text files containing the password for the corresponding store. [2]: It is assumed here that Zookeeper TLS is disabled. You may enable it and specify the corresponding client-side TLS configuration properties via the controller.zk.* properties. [3]: This configuration property is required when using the default Password Auth Handler only. Segment Store Segment store supports configuration via a properties file ( config.properties ) or JVM system properties. The table below lists its TLS and auth parameters and sample values. For a detailed discription of these parameters refer to this document. Configuration Parameter Example Value pravegaservice.enableTls true pravegaservice.certFile /etc/secrets/segmentstore01.pem pravegaservice.keyFile /etc/secrets/segmentstore01.key.pem pravegaservice.secureZK false 2 pravegaservice.zkTrustStore /etc/secrets/client.truststore.jks pravegaservice.zkTrustStorePasswordPath /etc/secrets/client.truststore.jks.password autoScale.tlsEnabled true autoScale.tlsCertFile /etc/secrets/segmentstore01.key.pem autoScale.authEnabled true autoScale.tokenSigningKey a-secret-value 1 autoScale.validateHostName true [1]: It is assumed here that Zookeeper TLS is disabled. You may enable it and specify the corresponding client-side TLS configuration properties via the pravegaservice.zk.* properties. [2]: The secret value you use here must match the same value used for other Controller and Segment Store services.","title":"Configuring TLS and Auth on Server Side"},{"location":"security/securing-distributed-mode-cluster/#configuring-tls-and-credentials-on-client-side","text":"After enabling and configuring TLS and auth on the server-side services, its time to update the clients, so that the they can establish TLS connections with the servers and are allowed access. For TLS, establish trust for the servers' certificates on the client side using one of the following ways: Supply the client library with the certificate of the trusted CA that has signed the servers' certificates. ClientConfig clientConfig = ClientConfig.builder() .controllerURI(\"tls://<DNS-NAME-OR-IP>:9090\") .trustStore(\"/etc/secrets/ca-cert\") ... .build(); Install the CA's certificate in the Java system key store. Create a custom truststore with the CA's certificate and supply it to the Pravega client application, via JVM system properties javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword . For auth, client-side configuration depends on the AuthHandler implementation used. If your server is configured to use the default PasswordAuthHandler , you may supply the credentials as shown below. ClientConfig clientConfig = ClientConfig.builder() .controllerURI(\"tls://<DNS-NAME-OR-IP>:9090\") .trustStore(\"/etc/secrets/ca-cert\") .credentials(new DefaultCredentials(\"changeit\", \"marketinganaylticsapp\")) .build();","title":"Configuring TLS and Credentials on Client Side"},{"location":"security/securing-distributed-mode-cluster/#server-hostname-verification","text":"Hostname verification during TLS communications verifies that the DNS name to which the client connects matches the hostname specified in either of the following fields in the server's certificate: Common Name ( CN ) in the certificate's Subject field One of the Subject Alternative Names field entries If the server certificates have a hostname assigned, you have used IP addresses as endpoints for the services, and those hostnames are not accessible from the client nodes, you might need to add mappings of IP addresses and DNS/Host names in the client-side operating system hosts file. Alternatively, you may disable hostname verification by invoking validateHostName(false) of the ClientConfig builder. It is strongly recommended to avoid disabling hostname verification for production clusters.","title":"Server Hostname Verification"},{"location":"security/securing-distributed-mode-cluster/#having-tls-and-auth-take-effect","text":"To ensure TLS and auth parameters take effect, all the services on the server-side need to be restarted. Existing client applications will need to be restarted as well, after they are reconfigured for TLS and auth. For fresh deployments, starting the cluster and the clients after configuring TLS and auth, will automatically ensure they take effect.","title":"Having TLS and Auth Take Effect"},{"location":"security/securing-distributed-mode-cluster/#conclusion","text":"This document explained about how to enable security in a Pravega cluster running in distributed mode. Specifically, how to perform the following actions were discussed: Generating a CA (if needed) Generating server certificates and keys for Pravega services Signing the generated certificates using the generated CA Enabling and configuring TLS and auth on the server Side Setting up the ClientConfig on the client side for communicating with a Pravega cluster running with TLS and auth enabled Having TLS and auth take effect","title":"Conclusion"},{"location":"security/securing-standalone-mode-cluster/","text":"Setting Up Security for a Standalone Mode Server \u00b6 Security in Pravega is off by default. You may start Pravega Standalone mode server with security enabled by modifying the security configuration before launching it. Depending on how you configure security, either or both of the following occurs: Client-server and internal communications are encrypted using SSL/TLS Server authenticates and authorizes client requests For standalone mode servers, you may enable SSL/TLS, and/ auth (short for Authentication and Authorization). We recommend that you enable both. The subsections below describe how to enable them. Enabling SSL/TLS and Auth \u00b6 The configuration parameter singlenode.enableTls determines whether SSL/TLS is enabled in a standalone mode server. Its default value is false , and therefore, SSL/TLS is disabled by default. Similarly, the configuration parameter singlenode.enableAuth determines whether auth is enabled. It is disabled by default as well. The following steps explain how to enable and configure SSL/TLS and/ auth : Locate the standalone-config.properties file. If you are running standalone mode server from source, you can find it under the /path/to/pravega/config directory. If you are running it from the distribution instead, you can find it under the /path/to/pravega-<version>/conf directory. If enabling SSL/TLS, optionally prepare the certificates and keys for SSL/TLS. In the same directory as the standalone-config.properties file, you'll find default/provided PKI material. You may use these material for SSL/TLS. If you'd like to create your own material instead, you may follow similar steps as those used for creating the supplied material. If enabling SSL/TLS, configure SSL/TLS parameters. If you are using custom material, replace the paths of the files accordingly. singlenode . enableTls = true singlenode . keyFile =../ config / server - key . key singlenode . certFile =../ config / server - cert . crt singlenode . keyStoreJKS =../ config / server . keystore . jks singlenode . keyStoreJKSPasswordFile =../ config / server . keystore . jks . passwd singlenode . trustStoreJKS =../ config / client . truststore . jks For descriptions of these properties, see Pravega Security Configurations . If enabling auth , configure auth parameters. singlenode . enableAuth = true singlenode . userName = admin singlenode . passwd = 1111_ aaaa singlenode . passwdFile =../ config / passwd For descriptions of these properties, see Pravega Security Configurations . Note: * The default Password Auth Handler supports Basic authentication for client-server communications. You may deploy additional Auth Handler plugins by placing them in the classpath. The default handler's configuration file shown above - passwd file - contains a single user with credentials containing username admin and password 1111_aaaa . To create additional users or change the password for the default admin user: a) Create a file containing 1 or more entries of the form <user>:<password>:<acl>; . For example: `jdoe:strong-password:*,READ_UPDATE;` b) Use the PasswordCreatorTool to create a new file with the passwords encrypted. Configure this new file instead of the supplied passwd file. If enabling SSL/TLS, ensure that the server's certificate is trusted. Note: This step is only needed the first time you run SSL/TLS-enabled standalone mode with a certificate. If you have already imported the certificate earlier, you don't need to do it again the next time you launch the server. A server certificate can be rendered trusted via either Chain of Trust or via Direct Trust. Chain of Trust: A chain of trust, which is the standard SSL/TLS certificate trust model, is established by verifying that the certificate is issued and signed by a trusted CA. To establish chain of trust for a certificate signed by a custom CA, we need to import the certificate of the CA into the JVM's truststore. The provided server-cert.crt is signed using a CA represented by another provided certificate ca-cert.crt : so, when using those certificate, you should import ca-cert.crt file. Direct Trust: This type of trust is established by adding the server's certificate to the truststore. It is a non-standard way of establishing trust, which is useful when using self-signed certificates. Here's how to import the provided CA certificate file ca-cert.crt to Java's system truststore, for establishing 'Chain of Trust': a) Change directory to Standalone mode config directory. ```bash $ cd /path/to/pravega/config ``` or ```bash $ cd /path/to/pravega-<version>/conf ``` b) Convert the X.509 PEM (ASCII) encoded ca-cert.crt file to DER (Binary) encoded format: ```bash $ openssl x509 -in server-cert.crt -inform pem -out ca-cert.der -outform der ``` c) Import the certificate into the local JVM's trust store: ```bash $ sudo keytool -importcert -alias local-CA \\ -keystore /path/to/jre/lib/security/cacerts -file ca-cert.der ``` When prompted for keystore password, use the default JRE keystore password `changeit` or the custom one that you might have configured for the JRE. Note: If you want to use a custom truststore instead of adding the certificate to the system truststore, create a new truststore using Java keytool utility, add the certificate to it and configure the JVM to use it by setting the system properties javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword . Now that you have enabled and configured security, restart the standalone mode server and verify that security is working. Verifying Security \u00b6 Verify that the controller REST API is responding properly: # If both TLS and Auth are enabled $ curl -v -k -u admin:1111_aaaa https://<host-name-or-ip/localhost>:9091/v1/scopes # If only Auth is enabled $ curl -v -u admin:1111_aaaa http://<host-name-or-ip/localhost>:9091/v1/scopes Note: -k in the command above avoids hostname verification and is needed only if you are enabling SSL/TLS. If you are using the provided certificate, the host's DNS name/IP isn't the subject (in CN or SAN ), and therefore hostname verification will fail. -k lets the command to return data successfully. You can find details about curl's options here . Optionally, run security-enabled Reader/Writer from Pravega sample applications against the standalone server to verify it is responding appropriately to Read/Write requests. To do so, configure security in the client application and run it. The example below assumes both TLS and auth are enabled. If TLS is disabled, modify the Controller URI scheme to tcp instead of tls and remove the lines that add TLS-related client-side configuration. ClientConfig clientConfig = ClientConfig . builder () . controllerURI ( URI . create ( \"tls://localhost:9090\" )) // TLS-related client-side configuration . trustStore ( \"/path/to/ca-cert.crt\" ) . validateHostName ( false ) // Auth-related client-side configuration . credentials ( new DefaultCredentials ( this . password , this . username )) . build (); You can find a client example with security enabled here . Note: * Remember that clients can access standalone mode servers through the localhost interface only. Therefore, the hostname in the Controller URI should be specified as localhost in client applications when accessing standalone mode servers. * .validateHostName(false) disables hostname verification for client-to-segment-store communications.","title":"Securing standalone mode cluster"},{"location":"security/securing-standalone-mode-cluster/#setting-up-security-for-a-standalone-mode-server","text":"Security in Pravega is off by default. You may start Pravega Standalone mode server with security enabled by modifying the security configuration before launching it. Depending on how you configure security, either or both of the following occurs: Client-server and internal communications are encrypted using SSL/TLS Server authenticates and authorizes client requests For standalone mode servers, you may enable SSL/TLS, and/ auth (short for Authentication and Authorization). We recommend that you enable both. The subsections below describe how to enable them.","title":"Setting Up Security for a Standalone Mode Server"},{"location":"security/securing-standalone-mode-cluster/#enabling-ssltls-and-auth","text":"The configuration parameter singlenode.enableTls determines whether SSL/TLS is enabled in a standalone mode server. Its default value is false , and therefore, SSL/TLS is disabled by default. Similarly, the configuration parameter singlenode.enableAuth determines whether auth is enabled. It is disabled by default as well. The following steps explain how to enable and configure SSL/TLS and/ auth : Locate the standalone-config.properties file. If you are running standalone mode server from source, you can find it under the /path/to/pravega/config directory. If you are running it from the distribution instead, you can find it under the /path/to/pravega-<version>/conf directory. If enabling SSL/TLS, optionally prepare the certificates and keys for SSL/TLS. In the same directory as the standalone-config.properties file, you'll find default/provided PKI material. You may use these material for SSL/TLS. If you'd like to create your own material instead, you may follow similar steps as those used for creating the supplied material. If enabling SSL/TLS, configure SSL/TLS parameters. If you are using custom material, replace the paths of the files accordingly. singlenode . enableTls = true singlenode . keyFile =../ config / server - key . key singlenode . certFile =../ config / server - cert . crt singlenode . keyStoreJKS =../ config / server . keystore . jks singlenode . keyStoreJKSPasswordFile =../ config / server . keystore . jks . passwd singlenode . trustStoreJKS =../ config / client . truststore . jks For descriptions of these properties, see Pravega Security Configurations . If enabling auth , configure auth parameters. singlenode . enableAuth = true singlenode . userName = admin singlenode . passwd = 1111_ aaaa singlenode . passwdFile =../ config / passwd For descriptions of these properties, see Pravega Security Configurations . Note: * The default Password Auth Handler supports Basic authentication for client-server communications. You may deploy additional Auth Handler plugins by placing them in the classpath. The default handler's configuration file shown above - passwd file - contains a single user with credentials containing username admin and password 1111_aaaa . To create additional users or change the password for the default admin user: a) Create a file containing 1 or more entries of the form <user>:<password>:<acl>; . For example: `jdoe:strong-password:*,READ_UPDATE;` b) Use the PasswordCreatorTool to create a new file with the passwords encrypted. Configure this new file instead of the supplied passwd file. If enabling SSL/TLS, ensure that the server's certificate is trusted. Note: This step is only needed the first time you run SSL/TLS-enabled standalone mode with a certificate. If you have already imported the certificate earlier, you don't need to do it again the next time you launch the server. A server certificate can be rendered trusted via either Chain of Trust or via Direct Trust. Chain of Trust: A chain of trust, which is the standard SSL/TLS certificate trust model, is established by verifying that the certificate is issued and signed by a trusted CA. To establish chain of trust for a certificate signed by a custom CA, we need to import the certificate of the CA into the JVM's truststore. The provided server-cert.crt is signed using a CA represented by another provided certificate ca-cert.crt : so, when using those certificate, you should import ca-cert.crt file. Direct Trust: This type of trust is established by adding the server's certificate to the truststore. It is a non-standard way of establishing trust, which is useful when using self-signed certificates. Here's how to import the provided CA certificate file ca-cert.crt to Java's system truststore, for establishing 'Chain of Trust': a) Change directory to Standalone mode config directory. ```bash $ cd /path/to/pravega/config ``` or ```bash $ cd /path/to/pravega-<version>/conf ``` b) Convert the X.509 PEM (ASCII) encoded ca-cert.crt file to DER (Binary) encoded format: ```bash $ openssl x509 -in server-cert.crt -inform pem -out ca-cert.der -outform der ``` c) Import the certificate into the local JVM's trust store: ```bash $ sudo keytool -importcert -alias local-CA \\ -keystore /path/to/jre/lib/security/cacerts -file ca-cert.der ``` When prompted for keystore password, use the default JRE keystore password `changeit` or the custom one that you might have configured for the JRE. Note: If you want to use a custom truststore instead of adding the certificate to the system truststore, create a new truststore using Java keytool utility, add the certificate to it and configure the JVM to use it by setting the system properties javax.net.ssl.trustStore and javax.net.ssl.trustStorePassword . Now that you have enabled and configured security, restart the standalone mode server and verify that security is working.","title":"Enabling SSL/TLS and Auth"},{"location":"security/securing-standalone-mode-cluster/#verifying-security","text":"Verify that the controller REST API is responding properly: # If both TLS and Auth are enabled $ curl -v -k -u admin:1111_aaaa https://<host-name-or-ip/localhost>:9091/v1/scopes # If only Auth is enabled $ curl -v -u admin:1111_aaaa http://<host-name-or-ip/localhost>:9091/v1/scopes Note: -k in the command above avoids hostname verification and is needed only if you are enabling SSL/TLS. If you are using the provided certificate, the host's DNS name/IP isn't the subject (in CN or SAN ), and therefore hostname verification will fail. -k lets the command to return data successfully. You can find details about curl's options here . Optionally, run security-enabled Reader/Writer from Pravega sample applications against the standalone server to verify it is responding appropriately to Read/Write requests. To do so, configure security in the client application and run it. The example below assumes both TLS and auth are enabled. If TLS is disabled, modify the Controller URI scheme to tcp instead of tls and remove the lines that add TLS-related client-side configuration. ClientConfig clientConfig = ClientConfig . builder () . controllerURI ( URI . create ( \"tls://localhost:9090\" )) // TLS-related client-side configuration . trustStore ( \"/path/to/ca-cert.crt\" ) . validateHostName ( false ) // Auth-related client-side configuration . credentials ( new DefaultCredentials ( this . password , this . username )) . build (); You can find a client example with security enabled here . Note: * Remember that clients can access standalone mode servers through the localhost interface only. Therefore, the hostname in the Controller URI should be specified as localhost in client applications when accessing standalone mode servers. * .validateHostName(false) disables hostname verification for client-to-segment-store communications.","title":"Verifying Security"}]}