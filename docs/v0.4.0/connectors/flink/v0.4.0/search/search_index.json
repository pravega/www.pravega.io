{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Apache Flink Connectors for Pravega This documentation describes the connectors API and it's usage to read and write Pravega streams with Apache Flink stream processing framework. Build end-to-end stream processing pipelines that use Pravega as the stream storage and message bus, and Apache Flink for computation over the streams. See the Pravega Concepts page for more information. Table of Contents Getting Started Quick Start Features Streaming Batch Table API/SQL Metrics Configurations Serialization","title":"Overview"},{"location":"#apache-flink-connectors-for-pravega","text":"This documentation describes the connectors API and it's usage to read and write Pravega streams with Apache Flink stream processing framework. Build end-to-end stream processing pipelines that use Pravega as the stream storage and message bus, and Apache Flink for computation over the streams. See the Pravega Concepts page for more information.","title":"Apache Flink Connectors for Pravega"},{"location":"#table-of-contents","text":"Getting Started Quick Start Features Streaming Batch Table API/SQL Metrics Configurations Serialization","title":"Table of Contents"},{"location":"batch/","text":"Batch Connector The Flink connector library for Pravega makes it possible to use a Pravega Stream as a data source and data sink in a batch program. See the below sections for details. Table of Contents FlinkPravegaInputFormat Parameters Input Stream(s) StreamCuts Parallelism FlinkPravegaOutputFormat Parameters Output Stream Parallelism Event Routing Serialization FlinkPravegaInputFormat A Pravega Stream may be used as a data source within a Flink batch program using an instance of io.pravega.connectors.flink.FlinkPravegaInputFormat . The input format reads events of a stream as a DataSet (the basic abstraction of the Flink Batch API). This input format opens the stream for batch reading, which processes stream segments in parallel and does not follow routing key order. Use the ExecutionEnvironment::createInput method to open a Pravega Stream as a DataSet . Example // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event deserializer DeserializationSchema < EventType > deserializer = ... // Define the input format based on a Pravega stream FlinkPravegaInputFormat < EventType > inputFormat = FlinkPravegaInputFormat .< EventType > builder () . forStream (...) . withPravegaConfig ( config ) . withDeserializationSchema ( deserializer ) . build (); DataSource < EventType > dataSet = env . createInput ( inputFormat , TypeInformation . of ( EventType . class ) . setParallelism ( 2 ); Parameters A builder API is provided to construct an instance of FlinkPravegaInputFormat . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. withDeserializationSchema The deserialization schema which describes how to turn byte messages into events. Input Stream(s) Each Pravega stream exists within a scope. A scope defines a namespace for streams such that names are unique. Across scopes, streams can have the same name. For example, if we have scopes A and B , then we can have a stream called myStream in each one of them. We cannot have a stream with the same name in the same scope. The builder API accepts both qualified and unqualified stream names. In qualified stream names, the scope is explicitly specified, e.g. my-scope/my-stream . In unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . See the configurations page for more information on default scope. A stream may be specified in one of three ways: 1. As a string containing a qualified name, in the form scope/stream . 2. As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. 3. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") . Multiple streams can be passed as parameter option (using the builder API). The BatchClient implementation is capable of reading from numerous streams in parallel, even across scopes. StreamCuts A StreamCut represents a specific position in a Pravega Stream, which may be obtained from various API interactions with the Pravega client. The BatchClient accepts a StreamCut as the start and/or end position of a given stream. For further reading on StreamCuts, please refer to documentation on StreamCut and sample code . If stream cuts are not provided then the default start position requested is assumed to be the earliest available data in the stream and the default end position is assumed to be all available data in that stream as of when the job execution begins. Parallelism FlinkPravegaInputFormat supports parallelization. Use the setParallelism method of DataSet to configure the number of parallel instances to execute. The parallel instances consume the stream in a coordinated manner, each consuming one or more stream segments. FlinkPravegaOutputFormat A Pravega Stream may be used as a data sink within a Flink batch program using an instance of io.pravega.connectors.flink.FlinkPravegaOutputFormat . The FlinkPravegaOutputFormat can be supplied as a sink to the DataSet (the basic abstraction of the Flink Batch API). Example // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event serializer SerializationSchema < EventType > serializer = ... // Define the event router for selecting the Routing Key PravegaEventRouter < EventType > router = ... // Define the input format based on a Pravega Stream FlinkPravegaOutputFormat < EventType > outputFormat = FlinkPravegaOutputFormat .< EventType > builder () . forStream (...) . withPravegaConfig ( config ) . withSerializationSchema ( serializer ) . withEventRouter ( router ) . build (); ExecutionEnvironment env = ExecutionEnvironment . getExecutionEnvironment (); Collection < EventType > inputData = Arrays . asList (...); env . fromCollection ( inputData ) . output ( outputFormat ); env . execute ( \"...\" ); Parameter A builder API is provided to construct an instance of FlinkPravegaOutputFormat . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withSerializationSchema The serialization schema which describes how to turn events into byte messages. withEventRouter The router function which determines the Routing Key for a given event. Output Stream Each stream in Pravega is contained by a scope. A scope acts as a namespace for one or more streams. The builder API accepts both qualified and unqualified stream names. In qualified, the scope is explicitly specified, e.g. my-scope/my-stream . In Unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") . Parallelism FlinkPravegaWriter supports parallelization. Use the setParallelism method to configure the number of parallel instances to execute. Event Routing Every event written to a Pravega Stream has an associated Routing Key. The Routing Key is the basis for event ordering. See the Pravega Concepts for details. To establish the routing key for each event, provide an implementation of io.pravega.connectors.flink.PravegaEventRouter when constructing the writer. Serialization Please, see the serialization page for more information on how to use the serializer and deserializer .","title":"Batch"},{"location":"batch/#batch-connector","text":"The Flink connector library for Pravega makes it possible to use a Pravega Stream as a data source and data sink in a batch program. See the below sections for details.","title":"Batch Connector"},{"location":"batch/#table-of-contents","text":"FlinkPravegaInputFormat Parameters Input Stream(s) StreamCuts Parallelism FlinkPravegaOutputFormat Parameters Output Stream Parallelism Event Routing Serialization","title":"Table of Contents"},{"location":"batch/#flinkpravegainputformat","text":"A Pravega Stream may be used as a data source within a Flink batch program using an instance of io.pravega.connectors.flink.FlinkPravegaInputFormat . The input format reads events of a stream as a DataSet (the basic abstraction of the Flink Batch API). This input format opens the stream for batch reading, which processes stream segments in parallel and does not follow routing key order. Use the ExecutionEnvironment::createInput method to open a Pravega Stream as a DataSet .","title":"FlinkPravegaInputFormat"},{"location":"batch/#example","text":"// Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event deserializer DeserializationSchema < EventType > deserializer = ... // Define the input format based on a Pravega stream FlinkPravegaInputFormat < EventType > inputFormat = FlinkPravegaInputFormat .< EventType > builder () . forStream (...) . withPravegaConfig ( config ) . withDeserializationSchema ( deserializer ) . build (); DataSource < EventType > dataSet = env . createInput ( inputFormat , TypeInformation . of ( EventType . class ) . setParallelism ( 2 );","title":"Example"},{"location":"batch/#parameters","text":"A builder API is provided to construct an instance of FlinkPravegaInputFormat . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. withDeserializationSchema The deserialization schema which describes how to turn byte messages into events.","title":"Parameters"},{"location":"batch/#input-streams","text":"Each Pravega stream exists within a scope. A scope defines a namespace for streams such that names are unique. Across scopes, streams can have the same name. For example, if we have scopes A and B , then we can have a stream called myStream in each one of them. We cannot have a stream with the same name in the same scope. The builder API accepts both qualified and unqualified stream names. In qualified stream names, the scope is explicitly specified, e.g. my-scope/my-stream . In unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . See the configurations page for more information on default scope. A stream may be specified in one of three ways: 1. As a string containing a qualified name, in the form scope/stream . 2. As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. 3. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") . Multiple streams can be passed as parameter option (using the builder API). The BatchClient implementation is capable of reading from numerous streams in parallel, even across scopes.","title":"Input Stream(s)"},{"location":"batch/#streamcuts","text":"A StreamCut represents a specific position in a Pravega Stream, which may be obtained from various API interactions with the Pravega client. The BatchClient accepts a StreamCut as the start and/or end position of a given stream. For further reading on StreamCuts, please refer to documentation on StreamCut and sample code . If stream cuts are not provided then the default start position requested is assumed to be the earliest available data in the stream and the default end position is assumed to be all available data in that stream as of when the job execution begins.","title":"StreamCuts"},{"location":"batch/#parallelism","text":"FlinkPravegaInputFormat supports parallelization. Use the setParallelism method of DataSet to configure the number of parallel instances to execute. The parallel instances consume the stream in a coordinated manner, each consuming one or more stream segments.","title":"Parallelism"},{"location":"batch/#flinkpravegaoutputformat","text":"A Pravega Stream may be used as a data sink within a Flink batch program using an instance of io.pravega.connectors.flink.FlinkPravegaOutputFormat . The FlinkPravegaOutputFormat can be supplied as a sink to the DataSet (the basic abstraction of the Flink Batch API).","title":"FlinkPravegaOutputFormat"},{"location":"batch/#example_1","text":"// Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event serializer SerializationSchema < EventType > serializer = ... // Define the event router for selecting the Routing Key PravegaEventRouter < EventType > router = ... // Define the input format based on a Pravega Stream FlinkPravegaOutputFormat < EventType > outputFormat = FlinkPravegaOutputFormat .< EventType > builder () . forStream (...) . withPravegaConfig ( config ) . withSerializationSchema ( serializer ) . withEventRouter ( router ) . build (); ExecutionEnvironment env = ExecutionEnvironment . getExecutionEnvironment (); Collection < EventType > inputData = Arrays . asList (...); env . fromCollection ( inputData ) . output ( outputFormat ); env . execute ( \"...\" );","title":"Example"},{"location":"batch/#parameter","text":"A builder API is provided to construct an instance of FlinkPravegaOutputFormat . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withSerializationSchema The serialization schema which describes how to turn events into byte messages. withEventRouter The router function which determines the Routing Key for a given event.","title":"Parameter"},{"location":"batch/#output-stream","text":"Each stream in Pravega is contained by a scope. A scope acts as a namespace for one or more streams. The builder API accepts both qualified and unqualified stream names. In qualified, the scope is explicitly specified, e.g. my-scope/my-stream . In Unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") .","title":"Output Stream"},{"location":"batch/#parallelism_1","text":"FlinkPravegaWriter supports parallelization. Use the setParallelism method to configure the number of parallel instances to execute.","title":"Parallelism"},{"location":"batch/#event-routing","text":"Every event written to a Pravega Stream has an associated Routing Key. The Routing Key is the basis for event ordering. See the Pravega Concepts for details. To establish the routing key for each event, provide an implementation of io.pravega.connectors.flink.PravegaEventRouter when constructing the writer.","title":"Event Routing"},{"location":"batch/#serialization","text":"Please, see the serialization page for more information on how to use the serializer and deserializer .","title":"Serialization"},{"location":"configurations/","text":"Configurations The Flink connector library for Pravega supports the Flink Streaming API , Table API and Batch API , using a common configuration class. Table of Contents Common Configuration PravegaConfig Class Creating PravegaConfig Using PravegaConfig Understanding the Default Scope Common Configuration PravegaConfig Class A top-level config object, PravegaConfig , is provided to establish a Pravega context for the Flink connector. The config object automatically configures itself from environment variables , system properties and program arguments . PravegaConfig information sources is given below: Setting Environment Variable / System Property / Program Argument Default Value Controller URI PRAVEGA_CONTROLLER_URI pravega.controller.uri --controller tcp://localhost:9090 Default Scope PRAVEGA_SCOPE pravega.scope --scope - Credentials - - Hostname Validation - true Creating PravegaConfig The recommended way to create an instance of PravegaConfig is to pass an instance of ParameterTool to fromParams : ParameterTool params = ParameterTool . fromArgs ( args ); PravegaConfig config = PravegaConfig . fromParams ( params ); If your application doesn't use the ParameterTool class that is provided by Flink, create the PravegaConfig using fromDefaults : PravegaConfig config = PravegaConfig . fromDefaults (); The PravegaConfig class provides a builder-style API to override the default configuration settings: PravegaConfig config = PravegaConfig . fromDefaults () . withControllerURI ( \"tcp://...\" ) . withDefaultScope ( \"SCOPE-NAME\" ) . withCredentials ( credentials ) . withHostnameValidation ( false ); Using PravegaConfig All of the various source and sink classes provided with the connector library have a builder-style API which accepts a PravegaConfig for common configuration. Pass a PravegaConfig object to the respective builder via withPravegaConfig . For example, see below code: PravegaConfig config = ...; FlinkPravegaReader < MyClass > pravegaSource = FlinkPravegaReader .< MyClass > builder () . forStream (...) . withPravegaConfig ( config ) . build (); Understanding the Default Scope Pravega organizes streams into scopes for the purposes of manageability. The PravegaConfig establishes a default scope name that is used in two scenarios: 1. For resolving unqualified stream names when constructing a source or sink. The sources and sinks accept stream names that may be qualified (e.g. my-scope/my-stream ) or unqualified (e.g. my-stream ). 2. For establishing the scope name for the coordination stream underlying a Pravega Reader Group. It is important to note that, the FlinkPravegaReader and the FlinkPravegaTableSource use the default scope configured on PravegaConfig as their Reader Group scope, and provide withReaderGroupScope as an override. The scope name of input stream(s) doesn't influence the Reader Group scope.","title":"Configurations"},{"location":"configurations/#configurations","text":"The Flink connector library for Pravega supports the Flink Streaming API , Table API and Batch API , using a common configuration class.","title":"Configurations"},{"location":"configurations/#table-of-contents","text":"Common Configuration PravegaConfig Class Creating PravegaConfig Using PravegaConfig Understanding the Default Scope","title":"Table of Contents"},{"location":"configurations/#common-configuration","text":"","title":"Common Configuration"},{"location":"configurations/#pravegaconfig-class","text":"A top-level config object, PravegaConfig , is provided to establish a Pravega context for the Flink connector. The config object automatically configures itself from environment variables , system properties and program arguments . PravegaConfig information sources is given below: Setting Environment Variable / System Property / Program Argument Default Value Controller URI PRAVEGA_CONTROLLER_URI pravega.controller.uri --controller tcp://localhost:9090 Default Scope PRAVEGA_SCOPE pravega.scope --scope - Credentials - - Hostname Validation - true","title":"PravegaConfig Class"},{"location":"configurations/#creating-pravegaconfig","text":"The recommended way to create an instance of PravegaConfig is to pass an instance of ParameterTool to fromParams : ParameterTool params = ParameterTool . fromArgs ( args ); PravegaConfig config = PravegaConfig . fromParams ( params ); If your application doesn't use the ParameterTool class that is provided by Flink, create the PravegaConfig using fromDefaults : PravegaConfig config = PravegaConfig . fromDefaults (); The PravegaConfig class provides a builder-style API to override the default configuration settings: PravegaConfig config = PravegaConfig . fromDefaults () . withControllerURI ( \"tcp://...\" ) . withDefaultScope ( \"SCOPE-NAME\" ) . withCredentials ( credentials ) . withHostnameValidation ( false );","title":"Creating PravegaConfig"},{"location":"configurations/#using-pravegaconfig","text":"All of the various source and sink classes provided with the connector library have a builder-style API which accepts a PravegaConfig for common configuration. Pass a PravegaConfig object to the respective builder via withPravegaConfig . For example, see below code: PravegaConfig config = ...; FlinkPravegaReader < MyClass > pravegaSource = FlinkPravegaReader .< MyClass > builder () . forStream (...) . withPravegaConfig ( config ) . build ();","title":"Using PravegaConfig"},{"location":"configurations/#understanding-the-default-scope","text":"Pravega organizes streams into scopes for the purposes of manageability. The PravegaConfig establishes a default scope name that is used in two scenarios: 1. For resolving unqualified stream names when constructing a source or sink. The sources and sinks accept stream names that may be qualified (e.g. my-scope/my-stream ) or unqualified (e.g. my-stream ). 2. For establishing the scope name for the coordination stream underlying a Pravega Reader Group. It is important to note that, the FlinkPravegaReader and the FlinkPravegaTableSource use the default scope configured on PravegaConfig as their Reader Group scope, and provide withReaderGroupScope as an override. The scope name of input stream(s) doesn't influence the Reader Group scope.","title":"Understanding the Default Scope"},{"location":"getting-started/","text":"Pravega Flink Connectors This repository implements connectors to read and write Pravega Streams with Apache Flink stream processing framework. The connectors can be used to build end-to-end stream processing pipelines (see Samples ) that use Pravega as the stream storage and message bus, and Apache Flink for computation over the streams. Features & Highlights Exactly-once processing guarantees for both Reader and Writer, supporting end-to-end exactly-once processing pipelines Seamless integration with Flink's checkpoints and savepoints. Parallel Readers and Writers supporting high throughput and low latency processing. Table API support to access Pravega Streams for both Batch and Streaming use case. Building Connectors Building the connectors from the source is only necessary when we want to use or contribute to the latest ( unreleased ) version of the Pravega Flink connectors. The connector project is linked to a specific version of Pravega, based on a git submodule pointing to a commit-id. By default the sub-module option is disabled and the build step will make use of the Pravega version defined in the gradle.properties file. You could override this option by enabling usePravegaVersionSubModule flag in gradle.properties to true . Checkout the source code repository by following below steps: git clone --recursive https://github.com/pravega/flink-connectors.git After cloning the repository, the project can be built by running the below command in the project root directory flink-connectors . ./gradlew clean build To install the artifacts in the local maven repository cache ~/.m2/repository , run the following command: ./gradlew clean install Customizing the Build Building against a custom Flink version We can check and change the Flink version that Pravega builds against via the flinkVersion variable in the gradle.properties file. Note : Only Flink versions that are compatible with the latest connector code can be chosen. Building against another Scala version This section is only relevant if you use Scala in the stream processing application with Flink and Pravega. Parts of the Apache Flink use the language or depend on libraries written in Scala. Because Scala is not strictly compatible across versions, there exist different versions of Flink compiled for different Scala versions. If we use Scala code in the same application where we use the Apache Flink or the Flink connectors, we typically have to make sure we use a version of Flink that uses the same Scala version as our application. By default, the dependencies point to Flink for Scala 2.11 . To depend on released Flink artifacts for a different Scala version, you need to edit the build.gradle file and change all entries for the Flink dependencies to have a different Scala version suffix. For example, flink-streaming-java_2.11 would be replaced by flink-streaming-java_2.12 for Scala 2.12 . In order to build a new version of Flink for a different Scala version, please refer to the Flink documentation . Setting up your IDE Connector project uses Project Lombok , so we should ensure that we have our IDE setup with the required plugins. ( IntelliJ is recommended ). To import the source into IntelliJ: Import the project directory into IntelliJ IDE. It will automatically detect the gradle project and import things correctly. Enable Annotation Processing by going to Build, Execution, Deployment -> Compiler > Annotation Processors and checking Enable annotation processing . Install the Lombok Plugin . This can be found in Preferences -> Plugins . Restart your IDE. Connectors project compiles properly after applying the above steps. For eclipse, we can generate eclipse project files by running ./gradlew eclipse . Releases The latest releases can be found on the Github Release project page. Support Don\u2019t hesitate to ask! Contact the developers and community on the Slack if you need any help. Open an issue if you found a bug on Github Issues . Samples Follow the Pravega Samples repository to learn more about how to build and use the Flink Connector library.","title":"Getting Started"},{"location":"getting-started/#pravega-flink-connectors","text":"This repository implements connectors to read and write Pravega Streams with Apache Flink stream processing framework. The connectors can be used to build end-to-end stream processing pipelines (see Samples ) that use Pravega as the stream storage and message bus, and Apache Flink for computation over the streams.","title":"Pravega Flink Connectors"},{"location":"getting-started/#features-highlights","text":"Exactly-once processing guarantees for both Reader and Writer, supporting end-to-end exactly-once processing pipelines Seamless integration with Flink's checkpoints and savepoints. Parallel Readers and Writers supporting high throughput and low latency processing. Table API support to access Pravega Streams for both Batch and Streaming use case.","title":"Features &amp; Highlights"},{"location":"getting-started/#building-connectors","text":"Building the connectors from the source is only necessary when we want to use or contribute to the latest ( unreleased ) version of the Pravega Flink connectors. The connector project is linked to a specific version of Pravega, based on a git submodule pointing to a commit-id. By default the sub-module option is disabled and the build step will make use of the Pravega version defined in the gradle.properties file. You could override this option by enabling usePravegaVersionSubModule flag in gradle.properties to true . Checkout the source code repository by following below steps: git clone --recursive https://github.com/pravega/flink-connectors.git After cloning the repository, the project can be built by running the below command in the project root directory flink-connectors . ./gradlew clean build To install the artifacts in the local maven repository cache ~/.m2/repository , run the following command: ./gradlew clean install","title":"Building Connectors"},{"location":"getting-started/#customizing-the-build","text":"","title":"Customizing the Build"},{"location":"getting-started/#building-against-a-custom-flink-version","text":"We can check and change the Flink version that Pravega builds against via the flinkVersion variable in the gradle.properties file. Note : Only Flink versions that are compatible with the latest connector code can be chosen.","title":"Building against a custom Flink version"},{"location":"getting-started/#building-against-another-scala-version","text":"This section is only relevant if you use Scala in the stream processing application with Flink and Pravega. Parts of the Apache Flink use the language or depend on libraries written in Scala. Because Scala is not strictly compatible across versions, there exist different versions of Flink compiled for different Scala versions. If we use Scala code in the same application where we use the Apache Flink or the Flink connectors, we typically have to make sure we use a version of Flink that uses the same Scala version as our application. By default, the dependencies point to Flink for Scala 2.11 . To depend on released Flink artifacts for a different Scala version, you need to edit the build.gradle file and change all entries for the Flink dependencies to have a different Scala version suffix. For example, flink-streaming-java_2.11 would be replaced by flink-streaming-java_2.12 for Scala 2.12 . In order to build a new version of Flink for a different Scala version, please refer to the Flink documentation .","title":"Building against another Scala version"},{"location":"getting-started/#setting-up-your-ide","text":"Connector project uses Project Lombok , so we should ensure that we have our IDE setup with the required plugins. ( IntelliJ is recommended ). To import the source into IntelliJ: Import the project directory into IntelliJ IDE. It will automatically detect the gradle project and import things correctly. Enable Annotation Processing by going to Build, Execution, Deployment -> Compiler > Annotation Processors and checking Enable annotation processing . Install the Lombok Plugin . This can be found in Preferences -> Plugins . Restart your IDE. Connectors project compiles properly after applying the above steps. For eclipse, we can generate eclipse project files by running ./gradlew eclipse .","title":"Setting up your IDE"},{"location":"getting-started/#releases","text":"The latest releases can be found on the Github Release project page.","title":"Releases"},{"location":"getting-started/#support","text":"Don\u2019t hesitate to ask! Contact the developers and community on the Slack if you need any help. Open an issue if you found a bug on Github Issues .","title":"Support"},{"location":"getting-started/#samples","text":"Follow the Pravega Samples repository to learn more about how to build and use the Flink Connector library.","title":"Samples"},{"location":"metrics/","text":"Metrics Pravega metrics are collected and exposed via Flink metrics framework when using FlinkPravegaReader or FlinkPravegaWriter . Reader Metrics The following metrics are exposed for FlinkPravegaReader related operations: Name Description readerGroupName The name of the Reader Group. scope The scope name of the Reader Group. streams The fully qualified name (i.e., scope/stream ) of the streams that are part of the Reader Group. onlineReaders The readers that are currently online/available. segmentPositions The StreamCut information that indicates where the readers have read so far. unreadBytes The total number of bytes that have not been read yet. Writer Metrics For FlinkPravegaWriter related operations, only the stream name is exposed: Name Description streams The fully qualified name of the stream i.e., scope/stream Querying Metrics The metrics can be viewed either from Flink UI or using the Flink REST API (like below): curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . readerGroupName curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . scope curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . streams curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . onlineReaders curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . stream . test . segmentPositions curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . unreadBytes","title":"Metrics"},{"location":"metrics/#metrics","text":"Pravega metrics are collected and exposed via Flink metrics framework when using FlinkPravegaReader or FlinkPravegaWriter .","title":"Metrics"},{"location":"metrics/#reader-metrics","text":"The following metrics are exposed for FlinkPravegaReader related operations: Name Description readerGroupName The name of the Reader Group. scope The scope name of the Reader Group. streams The fully qualified name (i.e., scope/stream ) of the streams that are part of the Reader Group. onlineReaders The readers that are currently online/available. segmentPositions The StreamCut information that indicates where the readers have read so far. unreadBytes The total number of bytes that have not been read yet.","title":"Reader Metrics"},{"location":"metrics/#writer-metrics","text":"For FlinkPravegaWriter related operations, only the stream name is exposed: Name Description streams The fully qualified name of the stream i.e., scope/stream","title":"Writer Metrics"},{"location":"metrics/#querying-metrics","text":"The metrics can be viewed either from Flink UI or using the Flink REST API (like below): curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . readerGroupName curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . scope curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . streams curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . onlineReaders curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . stream . test . segmentPositions curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . unreadBytes","title":"Querying Metrics"},{"location":"quickstart/","text":"Getting Started Creating a Flink Stream Processing Project Note : You can skip this step if you have a streaming project set up already. Please use the following project templates and setup guidelines, to set up a stream processing project with Apache Flink using Connectors Project template for Java Project template for Scala Once after the set up, please follow the below instructions to add the Flink Pravega connectors to the project. Add the Connector Dependencies To add the Pravega connector dependencies to your project, add the following entry to your project file: (For example, pom.xml for Maven) <dependency> <groupId>io.pravega</groupId> <artifactId>pravega-connectors-flink_2.11</artifactId> <version>0.3.2</version> </dependency> Use appropriate version as necessary. The snapshot versions are published to jcenter repository and the release artifacts are available in Maven Central repository. Alternatively, we could build and publish the connector artifacts to local maven repository by executing the following command and make use of that version as your application dependency. ./gradlew clean install Running / Deploying the Application From Flink's perspective, the connector to Pravega is part of the streaming application (not part of Flink's core runtime), so the connector code must be part of the application's code artifact (JAR file). Typically, a Flink application is bundled as a fat-jar (also known as an uber-jar ) , such that all its dependencies are embedded. The project set up should have been a success, if you have used the above linked templates/guides . If you set up a application's project and dependencies manually, you need to make sure that it builds a jar with dependencies , to include both the application and the connector classes.","title":"Quick Start"},{"location":"quickstart/#getting-started","text":"","title":"Getting Started"},{"location":"quickstart/#creating-a-flink-stream-processing-project","text":"Note : You can skip this step if you have a streaming project set up already. Please use the following project templates and setup guidelines, to set up a stream processing project with Apache Flink using Connectors Project template for Java Project template for Scala Once after the set up, please follow the below instructions to add the Flink Pravega connectors to the project.","title":"Creating a Flink Stream Processing Project"},{"location":"quickstart/#add-the-connector-dependencies","text":"To add the Pravega connector dependencies to your project, add the following entry to your project file: (For example, pom.xml for Maven) <dependency> <groupId>io.pravega</groupId> <artifactId>pravega-connectors-flink_2.11</artifactId> <version>0.3.2</version> </dependency> Use appropriate version as necessary. The snapshot versions are published to jcenter repository and the release artifacts are available in Maven Central repository. Alternatively, we could build and publish the connector artifacts to local maven repository by executing the following command and make use of that version as your application dependency. ./gradlew clean install","title":"Add the Connector Dependencies"},{"location":"quickstart/#running-deploying-the-application","text":"From Flink's perspective, the connector to Pravega is part of the streaming application (not part of Flink's core runtime), so the connector code must be part of the application's code artifact (JAR file). Typically, a Flink application is bundled as a fat-jar (also known as an uber-jar ) , such that all its dependencies are embedded. The project set up should have been a success, if you have used the above linked templates/guides . If you set up a application's project and dependencies manually, you need to make sure that it builds a jar with dependencies , to include both the application and the connector classes.","title":"Running / Deploying the Application"},{"location":"serialization/","text":"Serialization Serialization refers to converting a data element in your Flink program to/from a message in a Pravega stream. Flink defines a standard interface for data serialization to/from byte messages delivered by various connectors. The core interfaces are: - org.apache.flink.streaming.util.serialization.SerializationSchema - org.apache.flink.streaming.util.serialization.DeserializationSchema Built-in serializers include: - org.apache.flink.streaming.util.serialization.SimpleStringSchema - org.apache.flink.streaming.util.serialization.TypeInformationSerializationSchema The Pravega connector is designed to use Flink's serialization interfaces. For example, to read each stream event as a UTF-8 string: DeserializationSchema < String > schema = new SimpleStringSchema (); FlinkPravegaReader < String > reader = new FlinkPravegaReader <>(..., schema ); DataStream < MyEvent > stream = env . addSource ( reader ); Interoperability with Other Applications A common scenario is using Flink to process Pravega stream data produced by a non-Flink application. The Pravega client library used by such applications defines the io.pravega.client.stream.Serializer interface for working with event data. The implementations of Serializer directly in a Flink program via built-in adapters can be used: - io.pravega.connectors.flink.serialization.PravegaSerializationSchema - io.pravega.connectors.flink.serialization.PravegaDeserializationSchema Below is an example, to pass an instance of the appropriate Pravega de/serializer class to the adapter's constructor: import io.pravega.client.stream.impl.JavaSerializer ; ... DeserializationSchema < MyEvent > adapter = new PravegaDeserializationSchema <>( MyEvent . class , new JavaSerializer < MyEvent >()); FlinkPravegaReader < MyEvent > reader = new FlinkPravegaReader <>(..., adapter ); DataStream < MyEvent > stream = env . addSource ( reader ); Note that the Pravega serializer must implement java.io.Serializable to be usable in a Flink program.","title":"Serialization"},{"location":"serialization/#serialization","text":"Serialization refers to converting a data element in your Flink program to/from a message in a Pravega stream. Flink defines a standard interface for data serialization to/from byte messages delivered by various connectors. The core interfaces are: - org.apache.flink.streaming.util.serialization.SerializationSchema - org.apache.flink.streaming.util.serialization.DeserializationSchema Built-in serializers include: - org.apache.flink.streaming.util.serialization.SimpleStringSchema - org.apache.flink.streaming.util.serialization.TypeInformationSerializationSchema The Pravega connector is designed to use Flink's serialization interfaces. For example, to read each stream event as a UTF-8 string: DeserializationSchema < String > schema = new SimpleStringSchema (); FlinkPravegaReader < String > reader = new FlinkPravegaReader <>(..., schema ); DataStream < MyEvent > stream = env . addSource ( reader );","title":"Serialization"},{"location":"serialization/#interoperability-with-other-applications","text":"A common scenario is using Flink to process Pravega stream data produced by a non-Flink application. The Pravega client library used by such applications defines the io.pravega.client.stream.Serializer interface for working with event data. The implementations of Serializer directly in a Flink program via built-in adapters can be used: - io.pravega.connectors.flink.serialization.PravegaSerializationSchema - io.pravega.connectors.flink.serialization.PravegaDeserializationSchema Below is an example, to pass an instance of the appropriate Pravega de/serializer class to the adapter's constructor: import io.pravega.client.stream.impl.JavaSerializer ; ... DeserializationSchema < MyEvent > adapter = new PravegaDeserializationSchema <>( MyEvent . class , new JavaSerializer < MyEvent >()); FlinkPravegaReader < MyEvent > reader = new FlinkPravegaReader <>(..., adapter ); DataStream < MyEvent > stream = env . addSource ( reader ); Note that the Pravega serializer must implement java.io.Serializable to be usable in a Flink program.","title":"Interoperability with Other Applications"},{"location":"streaming/","text":"Streaming Connector The Flink connector library for Pravega provides a data source and data sink for use with the Flink Streaming API. See the below sections for details. Table of Contents FlinkPravegaReader Parameters Input Stream(s) Parallelism Checkpointing Timestamp Extraction / Watermark Emission Stream Cuts Historical Stream Processing FlinkPravegaWriter Parameters Parallelism Event Routing Event Time Ordering Writer Modes Metrics Data Serialization FlinkPravegaReader A Pravega Stream may be used as a data source within a Flink streaming program using an instance of io.pravega.connectors.flink.FlinkPravegaReader . The reader reads a given Pravega Stream (or multiple streams) as a DataStream (the basic abstraction of the Flink Streaming API). Open a Pravega Stream as a DataStream using the method StreamExecutionEnvironment::addSource . Example StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event deserializer DeserializationSchema < MyClass > deserializer = ... // Define the data stream FlinkPravegaReader < MyClass > pravegaSource = FlinkPravegaReader .< MyClass > builder () . forStream (...) . withPravegaConfig ( config ) . withDeserializationSchema ( deserializer ) . build (); DataStream < MyClass > stream = env . addSource ( pravegaSource ); Parameters A builder API is provided to construct an instance of FlinkPravegaReader . See the table below for a summary of builder properties. Note that, the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. uid The uid to identify the checkpoint state of this source. withReaderGroupScope The scope to store the Reader Group synchronization stream into. withReaderGroupName The Reader Group name for display purposes. withReaderGroupRefreshTime The interval for synchronizing the Reader Group state across parallel source instances. withCheckpointInitiateTimeout The timeout for executing a checkpoint of the Reader Group state. withDeserializationSchema The deserialization schema which describes how to turn byte messages into events. enableMetrics true or false to enable/disable reporting Pravega metrics. Metrics is enabled by default. Input Stream(s) Each stream in Pravega is contained by a scope. A scope acts as a namespace for one or more streams. The FlinkPravegaReader is able to read from numerous streams in parallel, even across scopes. The builder API accepts both qualified and unqualified stream names. In qualified, the scope is explicitly specified, e.g. my-scope/my-stream . In Unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") . Parallelism The FlinkPravegaReader supports parallelization. Use the setParallelism method to of Datastream to configure the number of parallel instances to execute. The parallel instances consume the stream in a coordinated manner, each consuming one or more stream segments. Note: Coordination is achieved with the use of a Pravega Reader Group, which is based on a State Synchronizer . The Synchronizer creates a backing stream that may be manually deleted after the completion of the job. Checkpointing In order to make state fault tolerant, Flink needs to checkpoint the state. Checkpoints allow Flink to recover state and positions in the streams to give the application the same semantics as a failure-free execution. The reader is compatible with Flink checkpoints and savepoints. The reader automatically recovers from failure by rewinding to the checkpointed position in the stream. A savepoint is self-contained; it contains all information needed to resume from the correct position. The checkpoint mechanism works as a two-step process: The master hook handler from the job manager initiates the triggerCheckpoint request to the ReaderCheckpointHook that was registered with the Job Manager during FlinkPravegaReader source initialization. The ReaderCheckpointHook handler notifies Pravega to checkpoint the current reader state. This is a non-blocking call which returns a future once Pravega readers are done with the checkpointing. A CheckPoint event will be sent by Pravega as part of the data stream flow and on receiving the event, the FlinkPravegaReader will initiate triggerCheckpoint request to effectively let Flink continue and complete the checkpoint process. Timestamp Extraction / Watermark Emission Flink requires the events\u2019 timestamps (each element in the stream needs to have its event timestamp assigned). This is achieved by accessing/extracting the timestamp from some field in the element. These are used to tell the system about progress in event time. Pravega is not aware (or does not track) of event time and does not store event timestamps or watermarks. Nonetheless it is possible to use event time semantics via an application-specific timestamp assigner and watermark generator as described in Flink documentation . Specify an AssignerWithPeriodicWatermarks or AssignerWithPunctuatedWatermarks on the DataStream as normal. Each parallel instance of the source processes one or more stream segments in parallel. Each watermark generator instance will receive events multiplexed from numerous segments. Be aware that segments are processed in parallel, and that no effort is made to order the events across segments in terms of their event time. Also, a given segment may be reassigned to another parallel instance at any time, preserving exactly-once behavior but causing further spread in observed event times. StreamCuts A StreamCut represents a specific position in a Pravega Stream, which may be obtained from various API interactions with the Pravega client. The FlinkPravegaReader accepts a StreamCut as the start and/or end position of a given stream. For further reading on StreamCuts, please refer to documentation on StreamCut and sample code . Historical Stream Processing Historical processing refers to processing stream data from a specific position in the stream rather than from the stream's tail. The builder API provides an overloaded method forStream that accepts a StreamCut parameter for this purpose. One such example is re-processing a stream, where we may have to process the data from the beginning (or from a certain point in the stream) to re-derive the output. For instance, in situations where the computation logic has been changed to address new additional criteria, or we fixed a bug or doing a typical A/B testing etc., where the ability to consume historical data as a stream is critical. FlinkPravegaWriter A Pravega Stream may be used as a data sink within a Flink program using an instance of io.pravega.connectors.flink.FlinkPravegaWriter . Add an instance of the writer to the dataflow program using the method DataStream::addSink . Example StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event serializer SerializationSchema < MyClass > serializer = ... // Define the event router for selecting the Routing Key PravegaEventRouter < MyClass > router = ... // Define the sink function FlinkPravegaWriter < MyClass > pravegaSink = FlinkPravegaWriter .< MyClass > builder () . forStream (...) . withPravegaConfig ( config ) . withSerializationSchema ( serializer ) . withEventRouter ( router ) . withWriterMode ( EXACTLY_ONCE ) . build (); DataStream < MyClass > stream = ... stream . addSink ( pravegaSink ); Parameters A builder API is provided to construct an instance of FlinkPravegaWriter . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withWriterMode The writer mode to provide Best-effort, _At-least-once , or Exactly-once guarantees. withTxnLeaseRenewalPeriod The Transaction lease renewal period that supports the Exactly-once writer mode. withSerializationSchema The serialization schema which describes how to turn events into byte messages. withEventRouter The router function which determines the Routing Key for a given event. enableMetrics true or false to enable/disable reporting Pravega metrics. Metrics is enabled by default. Parallelism FlinkPravegaWriter supports parallelization. Use the setParallelism method to configure the number of parallel instances to execute. Event Routing Every event written to a Pravega Stream has an associated Routing Key. The Routing Key is the basis for event ordering. See the Pravega Concepts for details. When constructing the FlinkPravegaWriter , please provide an implementation of io.pravega.connectors.flink.PravegaEventRouter which will guarantee the event ordering. In Pravega, events are guaranteed to be ordered at the segment level. For example, to guarantee write order specific to sensor id, you could provide a router implementation like below. private static class SensorEventRouter<SensorEvent> implements PravegaEventRouter<SensorEvent> { @Override public String getRoutingKey(SensorEvent event) { return event.getId(); } } Event Time Ordering For programs that use Flink's event time semantics, the connector library supports writing events in event time order. In combination with a Routing Key, this establishes a well-understood ordering for each key in the output stream. Use the method FlinkPravegaUtils::writeToPravegaInEventTimeOrder to write a given DataStream to a Pravega Stream such that events are automatically ordered by event time (on a per-key basis). Refer here for sample code. Writer Modes Writer modes relate to guarantees about the persistence of events emitted by the sink to a Pravega Stream. The writer supports three writer modes: 1. Best-effort - Any write failures will be ignored hence there could be data loss. 2. At-least-once - All events are persisted in Pravega. Duplicate events are possible, due to retries or in case of failure and subsequent recovery. 3. Exactly-once - All events are persisted in Pravega using a transactional approach integrated with the Flink checkpointing feature. By default, the At-least-once option is enabled and use .withWriterMode(...) option to override the value. See the Pravega documentation for details on transactional behavior. Metrics Metrics are reported by default unless it is explicitly disabled using enableMetrics(false) option. See Metrics page for more details on type of metrics that are reported. Serialization See the serialization page for more information on how to use the serializer and deserializer .","title":"Streaming"},{"location":"streaming/#streaming-connector","text":"The Flink connector library for Pravega provides a data source and data sink for use with the Flink Streaming API. See the below sections for details.","title":"Streaming Connector"},{"location":"streaming/#table-of-contents","text":"FlinkPravegaReader Parameters Input Stream(s) Parallelism Checkpointing Timestamp Extraction / Watermark Emission Stream Cuts Historical Stream Processing FlinkPravegaWriter Parameters Parallelism Event Routing Event Time Ordering Writer Modes Metrics Data Serialization","title":"Table of Contents"},{"location":"streaming/#flinkpravegareader","text":"A Pravega Stream may be used as a data source within a Flink streaming program using an instance of io.pravega.connectors.flink.FlinkPravegaReader . The reader reads a given Pravega Stream (or multiple streams) as a DataStream (the basic abstraction of the Flink Streaming API). Open a Pravega Stream as a DataStream using the method StreamExecutionEnvironment::addSource .","title":"FlinkPravegaReader"},{"location":"streaming/#example","text":"StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event deserializer DeserializationSchema < MyClass > deserializer = ... // Define the data stream FlinkPravegaReader < MyClass > pravegaSource = FlinkPravegaReader .< MyClass > builder () . forStream (...) . withPravegaConfig ( config ) . withDeserializationSchema ( deserializer ) . build (); DataStream < MyClass > stream = env . addSource ( pravegaSource );","title":"Example"},{"location":"streaming/#parameters","text":"A builder API is provided to construct an instance of FlinkPravegaReader . See the table below for a summary of builder properties. Note that, the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. uid The uid to identify the checkpoint state of this source. withReaderGroupScope The scope to store the Reader Group synchronization stream into. withReaderGroupName The Reader Group name for display purposes. withReaderGroupRefreshTime The interval for synchronizing the Reader Group state across parallel source instances. withCheckpointInitiateTimeout The timeout for executing a checkpoint of the Reader Group state. withDeserializationSchema The deserialization schema which describes how to turn byte messages into events. enableMetrics true or false to enable/disable reporting Pravega metrics. Metrics is enabled by default.","title":"Parameters"},{"location":"streaming/#input-streams","text":"Each stream in Pravega is contained by a scope. A scope acts as a namespace for one or more streams. The FlinkPravegaReader is able to read from numerous streams in parallel, even across scopes. The builder API accepts both qualified and unqualified stream names. In qualified, the scope is explicitly specified, e.g. my-scope/my-stream . In Unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") .","title":"Input Stream(s)"},{"location":"streaming/#parallelism","text":"The FlinkPravegaReader supports parallelization. Use the setParallelism method to of Datastream to configure the number of parallel instances to execute. The parallel instances consume the stream in a coordinated manner, each consuming one or more stream segments. Note: Coordination is achieved with the use of a Pravega Reader Group, which is based on a State Synchronizer . The Synchronizer creates a backing stream that may be manually deleted after the completion of the job.","title":"Parallelism"},{"location":"streaming/#checkpointing","text":"In order to make state fault tolerant, Flink needs to checkpoint the state. Checkpoints allow Flink to recover state and positions in the streams to give the application the same semantics as a failure-free execution. The reader is compatible with Flink checkpoints and savepoints. The reader automatically recovers from failure by rewinding to the checkpointed position in the stream. A savepoint is self-contained; it contains all information needed to resume from the correct position. The checkpoint mechanism works as a two-step process: The master hook handler from the job manager initiates the triggerCheckpoint request to the ReaderCheckpointHook that was registered with the Job Manager during FlinkPravegaReader source initialization. The ReaderCheckpointHook handler notifies Pravega to checkpoint the current reader state. This is a non-blocking call which returns a future once Pravega readers are done with the checkpointing. A CheckPoint event will be sent by Pravega as part of the data stream flow and on receiving the event, the FlinkPravegaReader will initiate triggerCheckpoint request to effectively let Flink continue and complete the checkpoint process.","title":"Checkpointing"},{"location":"streaming/#timestamp-extraction-watermark-emission","text":"Flink requires the events\u2019 timestamps (each element in the stream needs to have its event timestamp assigned). This is achieved by accessing/extracting the timestamp from some field in the element. These are used to tell the system about progress in event time. Pravega is not aware (or does not track) of event time and does not store event timestamps or watermarks. Nonetheless it is possible to use event time semantics via an application-specific timestamp assigner and watermark generator as described in Flink documentation . Specify an AssignerWithPeriodicWatermarks or AssignerWithPunctuatedWatermarks on the DataStream as normal. Each parallel instance of the source processes one or more stream segments in parallel. Each watermark generator instance will receive events multiplexed from numerous segments. Be aware that segments are processed in parallel, and that no effort is made to order the events across segments in terms of their event time. Also, a given segment may be reassigned to another parallel instance at any time, preserving exactly-once behavior but causing further spread in observed event times.","title":"Timestamp Extraction / Watermark Emission"},{"location":"streaming/#streamcuts","text":"A StreamCut represents a specific position in a Pravega Stream, which may be obtained from various API interactions with the Pravega client. The FlinkPravegaReader accepts a StreamCut as the start and/or end position of a given stream. For further reading on StreamCuts, please refer to documentation on StreamCut and sample code .","title":"StreamCuts"},{"location":"streaming/#historical-stream-processing","text":"Historical processing refers to processing stream data from a specific position in the stream rather than from the stream's tail. The builder API provides an overloaded method forStream that accepts a StreamCut parameter for this purpose. One such example is re-processing a stream, where we may have to process the data from the beginning (or from a certain point in the stream) to re-derive the output. For instance, in situations where the computation logic has been changed to address new additional criteria, or we fixed a bug or doing a typical A/B testing etc., where the ability to consume historical data as a stream is critical.","title":"Historical Stream Processing"},{"location":"streaming/#flinkpravegawriter","text":"A Pravega Stream may be used as a data sink within a Flink program using an instance of io.pravega.connectors.flink.FlinkPravegaWriter . Add an instance of the writer to the dataflow program using the method DataStream::addSink .","title":"FlinkPravegaWriter"},{"location":"streaming/#example_1","text":"StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event serializer SerializationSchema < MyClass > serializer = ... // Define the event router for selecting the Routing Key PravegaEventRouter < MyClass > router = ... // Define the sink function FlinkPravegaWriter < MyClass > pravegaSink = FlinkPravegaWriter .< MyClass > builder () . forStream (...) . withPravegaConfig ( config ) . withSerializationSchema ( serializer ) . withEventRouter ( router ) . withWriterMode ( EXACTLY_ONCE ) . build (); DataStream < MyClass > stream = ... stream . addSink ( pravegaSink );","title":"Example"},{"location":"streaming/#parameters_1","text":"A builder API is provided to construct an instance of FlinkPravegaWriter . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withWriterMode The writer mode to provide Best-effort, _At-least-once , or Exactly-once guarantees. withTxnLeaseRenewalPeriod The Transaction lease renewal period that supports the Exactly-once writer mode. withSerializationSchema The serialization schema which describes how to turn events into byte messages. withEventRouter The router function which determines the Routing Key for a given event. enableMetrics true or false to enable/disable reporting Pravega metrics. Metrics is enabled by default.","title":"Parameters"},{"location":"streaming/#parallelism_1","text":"FlinkPravegaWriter supports parallelization. Use the setParallelism method to configure the number of parallel instances to execute.","title":"Parallelism"},{"location":"streaming/#event-routing","text":"Every event written to a Pravega Stream has an associated Routing Key. The Routing Key is the basis for event ordering. See the Pravega Concepts for details. When constructing the FlinkPravegaWriter , please provide an implementation of io.pravega.connectors.flink.PravegaEventRouter which will guarantee the event ordering. In Pravega, events are guaranteed to be ordered at the segment level. For example, to guarantee write order specific to sensor id, you could provide a router implementation like below. private static class SensorEventRouter<SensorEvent> implements PravegaEventRouter<SensorEvent> { @Override public String getRoutingKey(SensorEvent event) { return event.getId(); } }","title":"Event Routing"},{"location":"streaming/#event-time-ordering","text":"For programs that use Flink's event time semantics, the connector library supports writing events in event time order. In combination with a Routing Key, this establishes a well-understood ordering for each key in the output stream. Use the method FlinkPravegaUtils::writeToPravegaInEventTimeOrder to write a given DataStream to a Pravega Stream such that events are automatically ordered by event time (on a per-key basis). Refer here for sample code.","title":"Event Time Ordering"},{"location":"streaming/#writer-modes","text":"Writer modes relate to guarantees about the persistence of events emitted by the sink to a Pravega Stream. The writer supports three writer modes: 1. Best-effort - Any write failures will be ignored hence there could be data loss. 2. At-least-once - All events are persisted in Pravega. Duplicate events are possible, due to retries or in case of failure and subsequent recovery. 3. Exactly-once - All events are persisted in Pravega using a transactional approach integrated with the Flink checkpointing feature. By default, the At-least-once option is enabled and use .withWriterMode(...) option to override the value. See the Pravega documentation for details on transactional behavior.","title":"Writer Modes"},{"location":"streaming/#metrics","text":"Metrics are reported by default unless it is explicitly disabled using enableMetrics(false) option. See Metrics page for more details on type of metrics that are reported.","title":"Metrics"},{"location":"streaming/#serialization","text":"See the serialization page for more information on how to use the serializer and deserializer .","title":"Serialization"},{"location":"table-api/","text":"Table Connector The Flink connector library for Pravega provides a table source and table sink for use with the Flink Table API. The Table API provides a unified API for both the Flink streaming and batch environment. See the below sections for details. Table of Contents Table Source Parameters Custom Formats Time Attribute Support Table Sink Parameters Custom Formats Table Source A Pravega Stream may be used as a table source within a Flink table program. The Flink Table API is oriented around Flink's TableSchema classes which describe the table fields. A concrete subclass of FlinkPravegaTableSource is then used to parse raw stream data as Row objects that conform to the table schema. The connector library provides out-of-box support for JSON-formatted data with FlinkPravegaJsonTableSource , and may be extended to support other formats. Example The following example uses the provided table source to read JSON-formatted events from a Pravega Stream: // Create a Flink Table environment ExecutionEnvironment env = ExecutionEnvironment . getExecutionEnvironment (); // Load the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); String [] fieldNames = { \"user\" , \"uri\" , \"accessTime\" }; // Read data from the stream using Table reader TableSchema tableSchema = TableSchema . builder () . field ( \"user\" , Types . STRING ()) . field ( \"uri\" , Types . STRING ()) . field ( \"accessTime\" , Types . SQL_TIMESTAMP ()) . build (); FlinkPravegaJsonTableSource source = FlinkPravegaJsonTableSource . builder () . forStream ( stream ) . withPravegaConfig ( pravegaConfig ) . failOnMissingField ( true ) . withRowtimeAttribute ( \"accessTime\" , new ExistingField ( \"accessTime\" ), new BoundedOutOfOrderTimestamps ( 30000L )) . withSchema ( tableSchema ) . withReaderGroupScope ( stream . getScope ()) . build (); // (Option-1) Read table as stream data StreamTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); tableEnv . registerTableSource ( \"MyTableRow\" , source ); String sqlQuery = \"SELECT user, count(uri) from MyTableRow GROUP BY user\" ; Table result = tableEnv . sqlQuery ( sqlQuery ); ... // (Option-2) Read table as batch data (use tumbling window as part of the query) BatchTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); tableEnv . registerTableSource ( \"MyTableRow\" , source ); String sqlQuery = \"SELECT user, \" + \"TUMBLE_END(accessTime, INTERVAL '5' MINUTE) AS accessTime, \" + \"COUNT(uri) AS cnt \" + \"from MyTableRow GROUP BY \" + \"user, TUMBLE(accessTime, INTERVAL '5' MINUTE)\" ; Table result = tableEnv . sqlQuery ( sqlQuery ); ... Parameters A builder API is provided to construct an instance of FlinkPravegaJsonTableSource . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Note that the table source supports both the Flink streaming and batch environments . In the streaming environment, the table source uses a FlinkPravegaReader connector. In the batch environment, the table source uses a FlinkPravegaInputFormat connectors. Please see the documentation of Streaming Connector and Batch Connector to have a better understanding on the below mentioned parameter list. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. uid The uid to identify the checkpoint state of this source. Applies only to streaming API. withReaderGroupScope The scope to store the Reader Group synchronization stream into. Applies only to streaming API. withReaderGroupName The Reader Group name for display purposes. Applies only to streaming API. withReaderGroupRefreshTime The interval for synchronizing the Reader Group state across parallel source instances. Applies only to streaming API. withCheckpointInitiateTimeout The timeout for executing a checkpoint of the Reader Group state. Applies only to streaming API. withSchema The table schema which describes which JSON fields to expect. withProctimeAttribute The name of the processing time attribute in the supplied table schema. withRowTimeAttribute supply the name of the rowtime attribute in the table schema, a TimeStampExtractor instance to extract the rowtime attribute value from the event and a WaterMarkStratergy to generate watermarks for the rowtime attribute. failOnMissingField A flag indicating whether to fail if a JSON field is missing. Custom Formats To work with stream events in a format other than JSON, extend FlinkPravegaTableSource . Please see the implementation of FlinkPravegaJsonTableSource for more details. Time Attribute Support With the use of withProctimeAttribute or withRowTimeAttribute builder method, one could supply the time attribute information of the event. The configured field must be present in the table schema and of type Types.SQL_TIMESTAMP() . Table Sink A Pravega Stream may be used as an append-only table within a Flink table program. The Flink Table API is oriented around Flink's TableSchema classes which describe the table fields. A concrete subclass of FlinkPravegaTableSink is then used to write table rows to a Pravega Stream in a particular format. The connector library provides out-of-box support for JSON-formatted data with FlinkPravegaJsonTableSource , and may be extended to support other formats. Example The following example uses the provided table sink to write JSON-formatted events to a Pravega Stream: // Create a Flink Table environment StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); StreamTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); // Load the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( ParameterTool . fromArgs ( args )); // Define a table (see Flink documentation) Table table = ... // Write the table to a Pravega Stream FlinkPravegaJsonTableSink sink = FlinkPravegaJsonTableSink . builder () . forStream ( \"sensor_stream\" ) . withPravegaConfig ( config ) . withRoutingKeyField ( \"sensor_id\" ) . withWriterMode ( EXACTLY_ONCE ) . build (); table . writeToSink ( sink ); Parameters A builder API is provided to construct an instance of FlinkPravegaJsonTableSink . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Note that the table sink supports both the Flink streaming and batch environments. In the streaming environment, the table sink uses a FlinkPravegaWriter connector. In the batch environment, the table sink uses a FlinkPravegaOutputFormat connector. Please see the documentation of Streaming Connector and Batch Connector to have a better understanding on the below mentioned parameter list. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withWriterMode The writer mode to provide Best-effort , At-least-once , or Exactly-once guarantees. withTxnTimeout The timeout for the Pravega Tansaction that supports the exactly-once writer mode. withSchema The table schema which describes which JSON fields to expect. withRoutingKeyField The table field to use as the Routing Key for written events. Custom Formats To work with stream events in a format other than JSON, extend FlinkPravegaTableSink . Please see the implementation of FlinkPravegaJsonTableSink for more details.","title":"Table API"},{"location":"table-api/#table-connector","text":"The Flink connector library for Pravega provides a table source and table sink for use with the Flink Table API. The Table API provides a unified API for both the Flink streaming and batch environment. See the below sections for details.","title":"Table Connector"},{"location":"table-api/#table-of-contents","text":"Table Source Parameters Custom Formats Time Attribute Support Table Sink Parameters Custom Formats","title":"Table of Contents"},{"location":"table-api/#table-source","text":"A Pravega Stream may be used as a table source within a Flink table program. The Flink Table API is oriented around Flink's TableSchema classes which describe the table fields. A concrete subclass of FlinkPravegaTableSource is then used to parse raw stream data as Row objects that conform to the table schema. The connector library provides out-of-box support for JSON-formatted data with FlinkPravegaJsonTableSource , and may be extended to support other formats.","title":"Table Source"},{"location":"table-api/#example","text":"The following example uses the provided table source to read JSON-formatted events from a Pravega Stream: // Create a Flink Table environment ExecutionEnvironment env = ExecutionEnvironment . getExecutionEnvironment (); // Load the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); String [] fieldNames = { \"user\" , \"uri\" , \"accessTime\" }; // Read data from the stream using Table reader TableSchema tableSchema = TableSchema . builder () . field ( \"user\" , Types . STRING ()) . field ( \"uri\" , Types . STRING ()) . field ( \"accessTime\" , Types . SQL_TIMESTAMP ()) . build (); FlinkPravegaJsonTableSource source = FlinkPravegaJsonTableSource . builder () . forStream ( stream ) . withPravegaConfig ( pravegaConfig ) . failOnMissingField ( true ) . withRowtimeAttribute ( \"accessTime\" , new ExistingField ( \"accessTime\" ), new BoundedOutOfOrderTimestamps ( 30000L )) . withSchema ( tableSchema ) . withReaderGroupScope ( stream . getScope ()) . build (); // (Option-1) Read table as stream data StreamTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); tableEnv . registerTableSource ( \"MyTableRow\" , source ); String sqlQuery = \"SELECT user, count(uri) from MyTableRow GROUP BY user\" ; Table result = tableEnv . sqlQuery ( sqlQuery ); ... // (Option-2) Read table as batch data (use tumbling window as part of the query) BatchTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); tableEnv . registerTableSource ( \"MyTableRow\" , source ); String sqlQuery = \"SELECT user, \" + \"TUMBLE_END(accessTime, INTERVAL '5' MINUTE) AS accessTime, \" + \"COUNT(uri) AS cnt \" + \"from MyTableRow GROUP BY \" + \"user, TUMBLE(accessTime, INTERVAL '5' MINUTE)\" ; Table result = tableEnv . sqlQuery ( sqlQuery ); ...","title":"Example"},{"location":"table-api/#parameters","text":"A builder API is provided to construct an instance of FlinkPravegaJsonTableSource . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Note that the table source supports both the Flink streaming and batch environments . In the streaming environment, the table source uses a FlinkPravegaReader connector. In the batch environment, the table source uses a FlinkPravegaInputFormat connectors. Please see the documentation of Streaming Connector and Batch Connector to have a better understanding on the below mentioned parameter list. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. uid The uid to identify the checkpoint state of this source. Applies only to streaming API. withReaderGroupScope The scope to store the Reader Group synchronization stream into. Applies only to streaming API. withReaderGroupName The Reader Group name for display purposes. Applies only to streaming API. withReaderGroupRefreshTime The interval for synchronizing the Reader Group state across parallel source instances. Applies only to streaming API. withCheckpointInitiateTimeout The timeout for executing a checkpoint of the Reader Group state. Applies only to streaming API. withSchema The table schema which describes which JSON fields to expect. withProctimeAttribute The name of the processing time attribute in the supplied table schema. withRowTimeAttribute supply the name of the rowtime attribute in the table schema, a TimeStampExtractor instance to extract the rowtime attribute value from the event and a WaterMarkStratergy to generate watermarks for the rowtime attribute. failOnMissingField A flag indicating whether to fail if a JSON field is missing.","title":"Parameters"},{"location":"table-api/#custom-formats","text":"To work with stream events in a format other than JSON, extend FlinkPravegaTableSource . Please see the implementation of FlinkPravegaJsonTableSource for more details.","title":"Custom Formats"},{"location":"table-api/#time-attribute-support","text":"With the use of withProctimeAttribute or withRowTimeAttribute builder method, one could supply the time attribute information of the event. The configured field must be present in the table schema and of type Types.SQL_TIMESTAMP() .","title":"Time Attribute Support"},{"location":"table-api/#table-sink","text":"A Pravega Stream may be used as an append-only table within a Flink table program. The Flink Table API is oriented around Flink's TableSchema classes which describe the table fields. A concrete subclass of FlinkPravegaTableSink is then used to write table rows to a Pravega Stream in a particular format. The connector library provides out-of-box support for JSON-formatted data with FlinkPravegaJsonTableSource , and may be extended to support other formats.","title":"Table Sink"},{"location":"table-api/#example_1","text":"The following example uses the provided table sink to write JSON-formatted events to a Pravega Stream: // Create a Flink Table environment StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); StreamTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); // Load the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( ParameterTool . fromArgs ( args )); // Define a table (see Flink documentation) Table table = ... // Write the table to a Pravega Stream FlinkPravegaJsonTableSink sink = FlinkPravegaJsonTableSink . builder () . forStream ( \"sensor_stream\" ) . withPravegaConfig ( config ) . withRoutingKeyField ( \"sensor_id\" ) . withWriterMode ( EXACTLY_ONCE ) . build (); table . writeToSink ( sink );","title":"Example"},{"location":"table-api/#parameters_1","text":"A builder API is provided to construct an instance of FlinkPravegaJsonTableSink . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Note that the table sink supports both the Flink streaming and batch environments. In the streaming environment, the table sink uses a FlinkPravegaWriter connector. In the batch environment, the table sink uses a FlinkPravegaOutputFormat connector. Please see the documentation of Streaming Connector and Batch Connector to have a better understanding on the below mentioned parameter list. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withWriterMode The writer mode to provide Best-effort , At-least-once , or Exactly-once guarantees. withTxnTimeout The timeout for the Pravega Tansaction that supports the exactly-once writer mode. withSchema The table schema which describes which JSON fields to expect. withRoutingKeyField The table field to use as the Routing Key for written events.","title":"Parameters"},{"location":"table-api/#custom-formats_1","text":"To work with stream events in a format other than JSON, extend FlinkPravegaTableSink . Please see the implementation of FlinkPravegaJsonTableSink for more details.","title":"Custom Formats"}]}